{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEMRush Weekly Wisdom - Competitor SERP Features.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/Real_state_url_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIWnvByQQBB3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re,os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "for dirname, _, filenames in os.walk('/content/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZpKA3gPQNWi"
      },
      "source": [
        "def get_data(url):  \n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    real_estate_data = soup.find_all(\"dl\", class_='dl-horizontal-border')\n",
        "    is_property_found = \"Yes\"\n",
        "    data_dict = {}\n",
        "    data_dict[\"URL\"] = url\n",
        "    if real_estate_data:\n",
        "        for d in real_estate_data:\n",
        "            dt = d.find_all('dt')\n",
        "            dd = d.find_all('dd')\n",
        "            \n",
        "            for i,j in zip(dt, dd):\n",
        "                i = i.contents[0].strip()\n",
        "                j = j.contents[0].strip()\n",
        "                if i == \"Unit Number\":\n",
        "                    if j == \"-\":\n",
        "                        data_dict[\"Unit Number\"] = np.nan\n",
        "                    else:\n",
        "                        data_dict[\"Unit Number\"] = j\n",
        "                elif i == \"Price\":\n",
        "                    data_dict[\"Price(¥)\"] = float(j.replace(\",\",\"\").replace(\"¥\",\"\").strip())\n",
        "                elif i == \"Building Name\":\n",
        "                    data_dict[\"Building Name\"] = j\n",
        "                elif i == \"Available From\":\n",
        "                    if \"Please Inquire\" in j:\n",
        "                        data_dict[\"Available From\"] = np.nan\n",
        "                    else:\n",
        "                        data_dict[\"Available From\"] = datetime.strptime(j, '%b %d, %Y')\n",
        "                elif i == \"Type\":\n",
        "                    data_dict[\"Type\"] = j.replace(\" \", \"\")\n",
        "                elif i == \"Size\":\n",
        "                    data_dict[\"Size(m²)\"] = float(j.replace(\"m²\", \"\").replace(\",\", \"\").strip())\n",
        "                elif i == \"Gross Yield\":\n",
        "                    data_dict[\"Gross Yield(%)\"] = float(j.replace(\"%\", \"\").strip())\n",
        "                elif i == \"Land Rights\":\n",
        "                    data_dict[\"Land Rights\"] = j\n",
        "                elif i == \"Maintenance Fee\":\n",
        "                    data_dict[\"Maintenance Fee(¥/mnt)\"] = float(j.replace(\"¥\", \"\").replace(\" / mth\", \"\").strip().replace(\",\",\"\"))\n",
        "                elif i == \"Location\":\n",
        "                    data_dict[\"Location\"] = j.replace(\",\", \"\")\n",
        "                elif i == \"Occupancy\":\n",
        "                    data_dict[\"Occupancy\"] = j\n",
        "                elif i == \"Floor\":\n",
        "                    data_dict[\"Floor\"] = j.replace(\" \", \"\")\n",
        "                elif i == \"Nearest Station\":\n",
        "                    data_dict[\"Nearest Station\"] = j.split(\"(\")[0].strip()\n",
        "                    if len(j.split(\"(\")) > 1:\n",
        "                        if \"walk\" in j:\n",
        "                            data_dict[\"Way to Nearest Station\"] = \"Walk\"\n",
        "                            data_dict[\"Distance From Station(min)\"] = j.split(\"(\")[1].split(\"min\")[0].strip()\n",
        "                        elif \"bus\" in j:\n",
        "                            data_dict[\"Way to Nearest Station\"] = \"Bus\"\n",
        "                            data_dict[\"Distance From Station(min)\"] = j.split(\"(\")[1].split(\"min\")[0].strip()\n",
        "                elif i == \"Layout\":\n",
        "                    data_dict[\"Layout\"] = j\n",
        "                elif i == \"Year Built\":\n",
        "                    data_dict[\"Year Built\"] = j\n",
        "                elif i == \"Direction Facing\":\n",
        "                    data_dict[\"Direction Facing\"] = j.replace(\",\", \"\")\n",
        "                elif i == \"Transaction Type\":\n",
        "                    data_dict[\"Transaction Type\"] = j\n",
        "                elif i == \"Balcony Size\":\n",
        "                    data_dict[\"Balcony Size(m²)\"] = float(j.replace(\"m²\", \"\").replace(\",\", \"\").strip())\n",
        "                elif i == \"Building Description\":\n",
        "                    data_dict[\"Building Description\"] = j.replace(\",\", \"\")\n",
        "                elif i == \"Other Expenses\":\n",
        "                    j = j.replace(\",\", \"\").replace(\" \", \"\").replace(\"，\", \"\")\n",
        "                    lst = re.findall(r'\\d+', j)\n",
        "                    if len(lst) > 0:\n",
        "                        lst = [int(i) for i in lst] \n",
        "                        data_dict[\"Other Expenses\"] = sum(lst)\n",
        "                elif i == \"Parking\":\n",
        "                    data_dict[\"Parking Available\"] = j.split()[0].replace(\",\", \"\")\n",
        "                    if len(j.split()) > 1:\n",
        "                        if j.split()[0].replace(\",\", \"\") == \"Available\":\n",
        "                            data_dict[\"Parking Fee(¥/mnt)\"] = float(j.split()[1].replace(\",\", \"\").replace(\"¥\", \"\").strip())\n",
        "                elif i == \"Date Updated\":\n",
        "                    if \"Please Inquire\" in j:\n",
        "                        data_dict[\"Date Updated\"] = np.nan\n",
        "                    else:\n",
        "                        data_dict[\"Date Updated\"] = datetime.strptime(j, '%b %d, %Y')\n",
        "                elif i == \"Next Update Schedule\":\n",
        "                    if \"Please Inquire\" in j:\n",
        "                        data_dict[\"Next Update Schedule\"] = np.nan\n",
        "                    else:\n",
        "                        data_dict[\"Next Update Schedule\"] = datetime.strptime(j, '%b %d, %Y')\n",
        "                    \n",
        "    else:\n",
        "        is_property_found = \"No\"\n",
        "    data_dict[\"Is_Prop_Avl\"] = is_property_found\n",
        "    return data_dict"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3msuyXsEQ38L"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awq_ZD9ewSf8",
        "outputId": "6c68c95e-8b1c-496e-a7de-aabaf5b30a9b"
      },
      "source": [
        "!pip install cssselect"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cssselect\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: cssselect\n",
            "Successfully installed cssselect-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK3UUdIawK_J"
      },
      "source": [
        "from urllib.parse import urlencode, urlparse, parse_qs\n",
        "\n",
        "from lxml.html import fromstring\n",
        "from requests import get\n",
        "\n",
        "raw = get(\"https://www.google.com/search?q=StackOverflow\").text\n",
        "page = fromstring(raw)\n",
        "\n",
        "for result in page.cssselect(\".r a\"):\n",
        "    url = result.get(\"href\")\n",
        "    if url.startswith(\"/url?\"):\n",
        "        url = parse_qs(urlparse(url).query)['q']\n",
        "    print(url[0])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ6O_hSOwgd3",
        "outputId": "bb1f512c-a0c6-4256-d1b6-d2fc6a0a0a00"
      },
      "source": [
        "!pip install google-search-results\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.0.tar.gz (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from google-search-results) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (1.24.3)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.0-py3-none-any.whl size=19876 sha256=946b304bfacaa40e2b364f4e925bcba082545180d76fa81e10b3aabb01b490a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/32/5f/313cc6b6c013f9c6386fc0a75d10b6b1aa7fe6867acc704d41\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRVWMjjfyEvJ"
      },
      "source": [
        "!pip install search-engine-parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "kL3MhZDGxY3f",
        "outputId": "73f82c6e-8f0f-40b1-94ba-c558e726eb09"
      },
      "source": [
        "from search_engine_parser import GoogleSearch\n",
        "\n",
        "def google(query):\n",
        "    search_args = (query, 1)\n",
        "    gsearch = GoogleSearch()\n",
        "    gresults = gsearch.search(*search_args)\n",
        "    return gresults['links']\n",
        "\n",
        "google('Is it illegal to scrape google results')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7488ee91144d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgoogle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Is it illegal to scrape google results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7488ee91144d>\u001b[0m in \u001b[0;36mgoogle\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msearch_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msearch_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/search_engine_parser/core/base.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, page, cache, proxy, proxy_auth, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             self.get_soup(url, cache=cache,\n\u001b[1;32m    286\u001b[0m                          \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                          proxy_auth=proxy_auth))\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \"\"\"\n\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_runnung\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ihw8lgIzIwT",
        "outputId": "b480a291-1790-4fdb-a5c5-fb8fcb7f2d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!virtualenv --python python3 env\n",
        "!source env/bin/activate\n",
        "!pip install git+git://github.com/NikolaiT/GoogleScraper/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: virtualenv: command not found\n",
            "/bin/bash: env/bin/activate: No such file or directory\n",
            "Collecting git+git://github.com/NikolaiT/GoogleScraper/\n",
            "  Cloning git://github.com/NikolaiT/GoogleScraper/ to /tmp/pip-req-build-2me4yc3z\n",
            "  Running command git clone -q git://github.com/NikolaiT/GoogleScraper/ /tmp/pip-req-build-2me4yc3z\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (3.4.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (4.6.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (3.141.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (2.23.0)\n",
            "Requirement already satisfied: PyMySql in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (1.0.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (1.4.25)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (3.7.4)\n",
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (0.1.11)\n",
            "Requirement already satisfied: recommonmark in /usr/local/lib/python3.7/dist-packages (from GoogleScraper==0.2.5) (0.7.1)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (3.0.1)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->GoogleScraper==0.2.5) (5.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->GoogleScraper==0.2.5) (2.10)\n",
            "Requirement already satisfied: commonmark>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommonmark->GoogleScraper==0.2.5) (0.9.1)\n",
            "Requirement already satisfied: sphinx>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from recommonmark->GoogleScraper==0.2.5) (1.8.5)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from recommonmark->GoogleScraper==0.2.5) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (1.15.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (1.2.4)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (57.4.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.6.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (21.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.9.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->GoogleScraper==0.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->GoogleScraper==0.2.5) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (2.4.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.3.1->recommonmark->GoogleScraper==0.2.5) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->GoogleScraper==0.2.5) (4.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->GoogleScraper==0.2.5) (1.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->GoogleScraper==0.2.5) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwdtquCNzUPh",
        "outputId": "8071241a-8a8f-4dfd-a695-1228a2b1b0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !GoogleScraper -m http --keyword SearchData/some_words.txt --num-workers 5 --search-engines \"bing,yahoo\" --output-filename threaded-results.json -v debug\n",
        "# !GoogleScraper -m http --keyword \"apple\" -v info\n",
        "!GoogleScraper -h\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sqlalchemy/sql/base.py\", line 1205, in __getattr__\n",
            "    return self._index[key]\n",
            "KeyError: '_data'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/GoogleScraper\", line 5, in <module>\n",
            "    from GoogleScraper.core import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/GoogleScraper/__init__.py\", line 20, in <module>\n",
            "    from GoogleScraper.core import scrape_with_config\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/GoogleScraper/core.py\", line 14, in <module>\n",
            "    from GoogleScraper.caching import CacheManager\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/GoogleScraper/caching.py\", line 12, in <module>\n",
            "    from GoogleScraper.output_converter import store_serp_result\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/GoogleScraper/output_converter.py\", line 19, in <module>\n",
            "    csv_fieldnames = sorted(set(Link.__table__.columns._data.keys() + SERP.__table__.columns._data.keys()) - {'id', 'serp_id'})\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sqlalchemy/sql/base.py\", line 1207, in __getattr__\n",
            "    util.raise_(AttributeError(key), replace_context=err)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sqlalchemy/util/compat.py\", line 207, in raise_\n",
            "    raise exception\n",
            "AttributeError: _data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCLIYUGzz8su"
      },
      "source": [
        "!pip install git+https://github.com/abenassi/Google-Search-API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3Zcjd2z5x2",
        "outputId": "a577d9d2-636f-4e9d-ff33-cac05d786c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from googleapi import google\n",
        "num_page = 1\n",
        "search_results = google.search(\"This \", num_page)\n",
        "print(search_results)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idydeoc1R2RR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a765f8a-2aaf-4833-bbae-89cef0e782d7"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = \"https://www.google.com/search?q=املاک+تهران+اجاره\"\n",
        "r = requests.get(url)\n",
        "page = r.text \n",
        "soup = BeautifulSoup(page, 'lxml') \n",
        "\n",
        "i = 0\n",
        "\n",
        "link_list = []\n",
        "for tag in soup.find_all('a'):\n",
        "    i+=1\n",
        "    href = tag['href']\n",
        "    if re.search('http',href):\n",
        "        try:\n",
        "            link = re.search('https://.+\\.com',href).group(0)\n",
        "            link_list.append(link)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "link_list = list(set(link_list))\n",
        "\n",
        "link_list2 = [] \n",
        "\n",
        "for link in link_list:\n",
        "    if not re.search('google.com',link):\n",
        "        link_list2.append(link)\n",
        "        \n",
        "print(link_list2)\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.DataFrame(link_list2)\n",
        "# df.to_csv('/content/sample_data/bs4_final.csv', index=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyZZ55qMMNly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "12cd36de-795d-46af-e809-ef5ac9a04490"
      },
      "source": [
        "df = pd.read_csv(\"/content/sample_data/bs4_final.csv\" , delim_whitespace=True)#\"/kaggle/input/japanese-property-urls/tokyo_property_urls.csv\")\n",
        "print(df)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-debcdc58e079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/bs4_final.csv\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\"/kaggle/input/japanese-property-urls/tokyo_property_urls.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMTpsHJVRHX1"
      },
      "source": [
        "urls = df.columns[0:]\n",
        "len(urls)\n",
        "print (urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxMqqYQSRVsQ"
      },
      "source": [
        "## Scrape Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1GJ66jRUrf"
      },
      "source": [
        "real_estate_df = pd.DataFrame(columns=[\"URL\", \"مناسب برای\", \"شماره واحد\", \"قیمت\", \"نام ساختمان\", \"طبقه\", \"در دسترس از\", \"نوع\", \"اندازه\", \"بازده ناخالص\",\n",
        "                                      \"حقوق زمین\", \"هزینه نگهداری\", \"موقعیت مکانی\", \"شغل\", \"نزدیکترین ایستگاه\", \"راه تا نزدیکترین ایستگاه\", \"فاصله از ایستگاه (دقیقه\",\n",
        "                                      \"نما\", \"سال ساخت\", \"رو به جهت\", \"نوع معامله\", \"اندازه بالکن (متر مربع)\", \"شرح ساختمان\", \"سایر هزینه ها\",\n",
        "                                      \"پارکینگ موجود\", \"هزینه پارکینگ\", \"برنامه به‌روزرسانی بعدی\", \"برنامه به‌روزرسانی بعدی\"])\n",
        "for url in urls:\n",
        "    print (url)\n",
        "    # res = get_data(url)\n",
        "    # real_estate_df = real_estate_df.append(res, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYR0SB5UNHI0",
        "outputId": "efd7b510-9b0d-4890-9b4f-4ed494c200a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%cd '/kaggle/input/d/sotest/crawler/'\n",
        "import ipykernel_launcher\n",
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import colorama\n",
        "\n",
        "# init the colorama module\n",
        "colorama.init()\n",
        "\n",
        "GREEN = colorama.Fore.GREEN\n",
        "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
        "RESET = colorama.Fore.RESET\n",
        "YELLOW = colorama.Fore.YELLOW\n",
        "\n",
        "# initialize the set of links (unique links)\n",
        "internal_urls = set()\n",
        "external_urls = set()\n",
        "\n",
        "total_urls_visited = 0\n",
        "\n",
        "\n",
        "def is_valid(url):\n",
        "    \"\"\"\n",
        "    Checks whether `url` is a valid URL.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
        "\n",
        "\n",
        "def get_all_website_links(url):\n",
        "    \"\"\"\n",
        "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
        "    \"\"\"\n",
        "    # all URLs of `url`\n",
        "    urls = set()\n",
        "    # domain name of the URL without the protocol\n",
        "    domain_name = urlparse(url).netloc\n",
        "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "        href = a_tag.attrs.get(\"href\")\n",
        "        if href == \"\" or href is None:\n",
        "            # href empty tag\n",
        "            continue\n",
        "        # join the URL if it's relative (not absolute link)\n",
        "        href = urljoin(url, href)\n",
        "        parser = urlparse(href)\n",
        "        # remove URL GET parameters, URL fragments, etc.\n",
        "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
        "        if not is_valid(href):\n",
        "            # not a valid URL\n",
        "            continue\n",
        "        if href in internal_urls:\n",
        "            # already in the set\n",
        "            continue\n",
        "        if domain_name not in href:\n",
        "            # external link\n",
        "            if href not in external_urls:\n",
        "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
        "                external_urls.add(href)\n",
        "            continue\n",
        "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
        "        urls.add(href)\n",
        "        internal_urls.add(href)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def crawl(url, max_urls=30):\n",
        "    \"\"\"\n",
        "    Crawls a web page and extracts all links.\n",
        "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
        "    params:\n",
        "        max_urls (int): number of max urls to crawl, default is 30.\n",
        "    \"\"\"\n",
        "    global total_urls_visited\n",
        "    total_urls_visited += 1\n",
        "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
        "    links = get_all_website_links(url)\n",
        "    for link in links:\n",
        "        if total_urls_visited > max_urls:\n",
        "            break\n",
        "        crawl(link, max_urls=max_urls)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "if True:\n",
        "#     import argparse\n",
        "#     parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
        "#     parser.add_argument(\"url\", help=\"The URL to extract links from.\")\n",
        "#     parser.add_argument(\"-m\", \"--max-urls\", help=\"Number of max URLs to crawl, default is 30.\", default=30, type=int)\n",
        "    \n",
        "#     args = parser.parse_args()\n",
        "    url = \"https://short.gy/d3RXQ9\"#args.url\n",
        "    url = \"https://www.zoomila.com\"\n",
        "    max_urls = '1'#args.max_urls\n",
        "\n",
        "    crawl(url, max_urls=max_urls)\n",
        "\n",
        "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
        "    print(\"[+] Total External links:\", len(external_urls))\n",
        "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
        "    print(\"[+] Total crawled URLs:\", max_urls)\n",
        "\n",
        "    domain_name = urlparse(url).netloc\n",
        "\n",
        "    # save the internal links to a file\n",
        "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
        "        for internal_link in internal_urls:\n",
        "            print(internal_link.strip(), file=f)\n",
        "\n",
        "    # save the external links to a file\n",
        "    with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
        "        for external_link in external_urls:\n",
        "            print(external_link.strip(), file=f)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/kaggle/input/d/sotest/crawler/'\n",
            "/content\n",
            "[*] Crawling: https://www.zoomila.com\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d7a04d2005b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mmax_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;31m#args.max_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_urls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[+] Total Internal links:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d7a04d2005b8>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(url, max_urls)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtotal_urls_visited\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{YELLOW}[*] Crawling: {url}{RESET}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_website_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_urls_visited\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d7a04d2005b8>\u001b[0m in \u001b[0;36mget_all_website_links\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# remove URL GET parameters, URL fragments, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_href\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"://\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparsed_href\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparsed_href\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# not a valid URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parsed_href' is not defined"
          ]
        }
      ]
    }
  ]
}