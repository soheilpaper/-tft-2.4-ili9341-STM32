{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/whisper_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper v3 is here!\n",
        "\n",
        "Whisper v3 is a new model open sourced by OpenAI. The model can do multilingual transcriptions and is quite impressive. For example, you can change from English to Spanish or Chinese in the middle of a sentence and it will work well!\n",
        "\n",
        "The model can be run in a free Google Colab instance and is integrated into `transformers` already, so switching can be a very smooth process if you already use the previous versions."
      ],
      "metadata": {
        "id": "OXaUqiE-eyXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the high level `pipeline` from the `transformers` library to load the model."
      ],
      "metadata": {
        "id": "sZONes21fHTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install openai\n",
        "! pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvNICD3ew2Jr",
        "outputId": "fde53468-5a2c-4eab-f586-97cf8fd7d550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/225.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFQeUT9EcIcK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!git+https://github.com/huggingface/transformers\n",
        "\n",
        "!pip install yt-dlp torch gradio"
      ],
      "metadata": {
        "id": "oi-2FpUjKVY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\",\n",
        "               \"openai/whisper-large-v3\",\n",
        "               torch_dtype=torch.float16,\n",
        "               device=\"cuda:0\")"
      ],
      "metadata": {
        "id": "DvBdwMdPcr-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"https://cdn-media.huggingface.co/speech_samples/sample1.flac\")"
      ],
      "metadata": {
        "id": "GZFkIyhjc0Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a quick Gradio demo where we can play with the model directly using our microphone! You can run this code in a Google Colab instance (or locally!) or just head to the <a href=\"https://huggingface.co/spaces/hf-audio/whisper-large-v3\" target=\"_blank\">Space</a> to play directly with it online."
      ],
      "metadata": {
        "id": "pt3YtM_PfTQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "LzS9P-gLxbva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"large-v3\")"
      ],
      "metadata": {
        "id": "vHJUfV6nw9XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def transcribe(inputs):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file submitted! Please record an audio before submitting your request.\")\n",
        "\n",
        "    text = pipe(inputs, generate_kwargs={\"task\": \"transcribe\"}, return_timestamps=True)[\"text\"]\n",
        "    return text\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Large V3: Transcribe Audio\",\n",
        "    description=(\n",
        "        \"Transcribe long-form microphone or audio inputs with the click of a button! Demo uses the\"\n",
        "        \" checkpoint [openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) and 🤗 Transformers to transcribe audio files\"\n",
        "        \" of arbitrary length.\"\n",
        "    ),\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "K0b2UZLVdIze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "source_path = 'myfile.wav'\n",
        "destination_path = '/content/drive/MyDrive/myfile.wav'\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "shutil.copyfile(source_path, destination_path)\n",
        "\n",
        "# Unmount Google Drive (optional)\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "Edm_NEsfx6KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"/content/drive/MyDrive/myfile.wav\")\n",
        "print() # For visual's sake, in case a warning appears.\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "ZWe7L_Ghw_Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fzs1cx010t8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import gradio as gr\n",
        "import yt_dlp as youtube_dl\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.audio_utils import ffmpeg_read\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "MODEL_NAME = \"openai/whisper-large-v3\"\n",
        "BATCH_SIZE = 8\n",
        "FILE_LIMIT_MB = 1000\n",
        "YT_LENGTH_LIMIT_S = 3600  # limit to 1 hour YouTube files\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=MODEL_NAME,\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "\n",
        "def transcribe(inputs, task):\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\"No audio file submitted! Please upload or record an audio file before submitting your request.\")\n",
        "\n",
        "    text = pipe(inputs, batch_size=BATCH_SIZE, generate_kwargs={\"task\": task}, return_timestamps=True)[\"text\"]\n",
        "    return  text\n",
        "\n",
        "\n",
        "def _return_yt_html_embed(yt_url):\n",
        "    video_id = yt_url.split(\"?v=\")[-1]\n",
        "    HTML_str = (\n",
        "        f'<center> <iframe width=\"500\" height=\"320\" src=\"https://www.youtube.com/embed/{video_id}\"> </iframe>'\n",
        "        \" </center>\"\n",
        "    )\n",
        "    return HTML_str\n",
        "\n",
        "def download_yt_audio(yt_url, filename):\n",
        "    info_loader = youtube_dl.YoutubeDL()\n",
        "\n",
        "    try:\n",
        "        info = info_loader.extract_info(yt_url, download=False)\n",
        "    except youtube_dl.utils.DownloadError as err:\n",
        "        raise gr.Error(str(err))\n",
        "\n",
        "    file_length = info[\"duration_string\"]\n",
        "    file_h_m_s = file_length.split(\":\")\n",
        "    file_h_m_s = [int(sub_length) for sub_length in file_h_m_s]\n",
        "\n",
        "    if len(file_h_m_s) == 1:\n",
        "        file_h_m_s.insert(0, 0)\n",
        "    if len(file_h_m_s) == 2:\n",
        "        file_h_m_s.insert(0, 0)\n",
        "    file_length_s = file_h_m_s[0] * 3600 + file_h_m_s[1] * 60 + file_h_m_s[2]\n",
        "\n",
        "    if file_length_s > YT_LENGTH_LIMIT_S:\n",
        "        yt_length_limit_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(YT_LENGTH_LIMIT_S))\n",
        "        file_length_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(file_length_s))\n",
        "        raise gr.Error(f\"Maximum YouTube length is {yt_length_limit_hms}, got {file_length_hms} YouTube video.\")\n",
        "\n",
        "    ydl_opts = {\"outtmpl\": filename, \"format\": \"worstvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\"}\n",
        "\n",
        "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "        try:\n",
        "            ydl.download([yt_url])\n",
        "        except youtube_dl.utils.ExtractorError as err:\n",
        "            raise gr.Error(str(err))\n",
        "\n",
        "\n",
        "def yt_transcribe(yt_url, task, max_filesize=75.0):\n",
        "    html_embed_str = _return_yt_html_embed(yt_url)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        filepath = os.path.join(tmpdirname, \"video.mp4\")\n",
        "        download_yt_audio(yt_url, filepath)\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            inputs = f.read()\n",
        "\n",
        "    inputs = ffmpeg_read(inputs, pipe.feature_extractor.sampling_rate)\n",
        "    inputs = {\"array\": inputs, \"sampling_rate\": pipe.feature_extractor.sampling_rate}\n",
        "\n",
        "    text = pipe(inputs, batch_size=BATCH_SIZE, generate_kwargs={\"task\": task}, return_timestamps=True)[\"text\"]\n",
        "\n",
        "    return html_embed_str, text\n",
        "\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "mf_transcribe = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n",
        "        gr.inputs.Radio([\"transcribe\", \"translate\"], label=\"Task\", default=\"transcribe\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    layout=\"horizontal\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Whisper Large V3: Transcribe Audio\",\n",
        "    description=(\n",
        "        \"Transcribe long-form microphone or audio inputs with the click of a button! Demo uses the\"\n",
        "        f\" checkpoint [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) and 🤗 Transformers to transcribe audio files\"\n",
        "        \" of arbitrary length.\"\n",
        "    ),\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "file_transcribe = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"upload\", type=\"filepath\", optional=True, label=\"Audio file\"),\n",
        "        gr.inputs.Radio([\"transcribe\", \"translate\"], label=\"Task\", default=\"transcribe\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    layout=\"horizontal\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Whisper Large V3: Transcribe Audio\",\n",
        "    description=(\n",
        "        \"Transcribe long-form microphone or audio inputs with the click of a button! Demo uses the\"\n",
        "        f\" checkpoint [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) and 🤗 Transformers to transcribe audio files\"\n",
        "        \" of arbitrary length.\"\n",
        "    ),\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "yt_transcribe = gr.Interface(\n",
        "    fn=yt_transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Textbox(lines=1, placeholder=\"Paste the URL to a YouTube video here\", label=\"YouTube URL\"),\n",
        "        gr.inputs.Radio([\"transcribe\", \"translate\"], label=\"Task\", default=\"transcribe\")\n",
        "    ],\n",
        "    outputs=[\"html\", \"text\"],\n",
        "    layout=\"horizontal\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Whisper Large V3: Transcribe YouTube\",\n",
        "    description=(\n",
        "        \"Transcribe long-form YouTube videos with the click of a button! Demo uses the checkpoint\"\n",
        "        f\" [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) and 🤗 Transformers to transcribe video files of\"\n",
        "        \" arbitrary length.\"\n",
        "    ),\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "with demo:\n",
        "    gr.TabbedInterface([mf_transcribe, file_transcribe, yt_transcribe], [\"Microphone\", \"Audio file\", \"YouTube\"])\n",
        "\n",
        "demo.launch(enable_queue=True)"
      ],
      "metadata": {
        "id": "SMxTZkS80lkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}