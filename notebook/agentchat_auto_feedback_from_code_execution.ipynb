{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/notebook/agentchat_auto_feedback_from_code_execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zf9QZCog1m2"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VmQG6WRg1nK"
      },
      "source": [
        "# Auto Generated Agent Chat: Task Solving with Code Generation, Execution & Debugging\n",
        "\n",
        "AutoGen offers conversable LLM agents, which can be used to solve various tasks with human or automatic feedback, including tasks that require using tools via code.\n",
        "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
        "\n",
        "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to write code and execute the code. Here `AssistantAgent` is an LLM-based agent that can write Python code (in a Python coding block) for a user to execute for a given task. `UserProxyAgent` is an agent which serves as a proxy for the human user to execute the code written by `AssistantAgent`, or automatically execute the code. Depending on the setting of `human_input_mode` and `max_consecutive_auto_reply`, the `UserProxyAgent` either solicits feedback from the human user or returns auto-feedback based on the result of code execution (success or failure and corresponding outputs) to `AssistantAgent`. `AssistantAgent` will debug the code and suggest new code if the result contains error. The two agents keep communicating to each other until the task is done.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
        "```bash\n",
        "pip install pyautogen\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "global gdrive_fpath\n",
        "drive_mounted = False\n",
        "gdrive_fpath = '.'\n",
        "local_path = '/content/'\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "if mount_gdrive : # and not drive_mounted:\n",
        "    from google.colab import drive\n",
        "\n",
        "    gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "    gdrive_subdirectory = 'MyDrive/ChatGPT_Paper_wrting' #@param{type:\"string\"}\n",
        "    gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "    print (\"gdrive path is :\",gdrive_fpath)\n",
        "   # Mount Google Drive\n",
        "    if not os.path.isdir(gdrive_mountpoint):\n",
        "     # If not, mount the drive\n",
        "       drive.mount(gdrive_mountpoint)\n",
        "       if not os.path.exists(gdrive_fpath):\n",
        "          os.makedirs(gdrive_fpath)\n",
        "          os.chdir(gdrive_fpath)\n",
        "    else:\n",
        "          print(\"Drive is already mounted.\")\n",
        "else:\n",
        "   Folder_fpath ='/content/' #@param{type:\"string\"}\n",
        "   #gdrive_subdirectory = 'MyDrive/ChatGPT_Paper_wrting' #@param{type:\"string\"}\n",
        "   gdrive_fpath = Folder_fpath\n",
        "   os.chdir(gdrive_fpath)\n",
        "folder_path = gdrive_fpath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qruC86we5nFT",
        "outputId": "7b235720-a659-4391-bef4-6b64b7a274ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive path is : /content/drive/MyDrive/ChatGPT_Paper_wrting\n",
            "Drive is already mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
          "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
          "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
          "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
        },
        "id": "RNFFse6Cg1nQ"
      },
      "outputs": [],
      "source": [
        "!pip install pyautogen>=0.2.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/autogen\n",
        "%cd autogen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtS_RX9cnpfQ",
        "outputId": "0dd5759f-94f3-4d87-dc43-02de337e0626"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'autogen' already exists and is not an empty directory.\n",
            "/content/autogen/autogen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlROX2FSg1nZ"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_80qYpJcoi5p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = {\n",
        "    'api_key': 'your_openai_api_key',\n",
        "    'base_url': 'https://api.openai.com',\n",
        "    'api_type': 'azure',  # Optional\n",
        "    'api_version': '2023-12-01-preview',  # Optional\n",
        "    'model': 'gpt-3.5-turbo'  # Optional, depending on your use case\n",
        "}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('OAI_CONFIG_LIST.json', 'w') as file:\n",
        "    json.dump(config, file, indent=2)"
      ],
      "metadata": {
        "id": "xoh2oDS4p5zH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7OP82_hOg1nc"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Union\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import Image\n",
        "\n",
        "import autogen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST.json\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-3.5\"] #, \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
        "    },\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "_JVEY4jU6Bj_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6eVZYopg1ne"
      },
      "source": [
        "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
        "\n",
        "The config list looks like the following:\n",
        "```python\n",
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-4',\n",
        "        'api_key': '<your OpenAI API key here>',\n",
        "    },\n",
        "    {\n",
        "        'model': 'gpt-4',\n",
        "        'api_key': '<your Azure OpenAI API key here>',\n",
        "        'base_url': '<your Azure OpenAI API base here>',\n",
        "        'api_type': 'azure',\n",
        "        'api_version': '2023-06-01-preview',\n",
        "    },\n",
        "    {\n",
        "        'model': 'gpt-4-32k',\n",
        "        'api_key': '<your Azure OpenAI API key here>',\n",
        "        'base_url': '<your Azure OpenAI API base here>',\n",
        "        'api_type': 'azure',\n",
        "        'api_version': '2023-06-01-preview',\n",
        "    },\n",
        "]\n",
        "```\n",
        "\n",
        "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhuNAVbCg1nj"
      },
      "source": [
        "## Example Task: Check Stock Price Change\n",
        "\n",
        "In the example below, let's see how to use the agents in AutoGen to write a python script and execute the script. This process involves constructing a `AssistantAgent` to serve as the assistant, along with a `UserProxyAgent` that acts as a proxy for the human user. In this example demonstrated below, when constructing the `UserProxyAgent`,  we select the `human_input_mode` to \"NEVER\". This means that the `UserProxyAgent` will not solicit feedback from the human user. It stops replying when the limit defined by `max_consecutive_auto_reply` is reached, or when `is_termination_msg()` returns true for the received message."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-3.5-turbo',\n",
        "        'api_key': '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' #'<your OpenAI API key here>',\n",
        "    }]"
      ],
      "metadata": {
        "id": "ncrkLnz7hsfu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "88HH0Ucyg1no"
      },
      "outputs": [],
      "source": [
        "# create an AssistantAgent named \"assistant\"\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": 42,  # seed for caching and reproducibility\n",
        "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
        "        \"temperature\": 0,  # temperature for sampling\n",
        "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
        ")\n",
        "# create a UserProxyAgent instance named \"user_proxy\"\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"coding\",\n",
        "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# the assistant receives a message from the user_proxy, which contains the task description\n",
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "Q-mEDM3H6PcE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5nFc5spxSYy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVuZCaJ4g1nw"
      },
      "source": [
        "The example above involves code execution. In AutoGen, code execution is triggered automatically by the `UserProxyAgent` when it detects an executable code block in a received message and no human user input is provided. This process occurs in a designated working directory, using a Docker container by default. Unless a specific directory is specified, AutoGen defaults to the `autogen/extensions` directory. Users have the option to specify a different working directory by setting the `work_dir` argument when constructing a new instance of the `UserProxyAgent`.\n",
        "\n",
        "The whole chat is auto-generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQhBu46Sg1n0"
      },
      "source": [
        "## Example Task: Plot Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gcrQmACZg1n3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831d67af-f50e-4589-ee69-91399a6f74bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Plot a chart of their stock price change YTD and save to stock_price_ytd.png.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# followup of the previous question\n",
        "user_proxy.send(\n",
        "    recipient=assistant,\n",
        "    message=\"\"\"Plot a chart of their stock price change YTD and save to stock_price_ytd.png.\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZSPZB0ig1n6"
      },
      "source": [
        "Let's display the generated figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SUVUfo3Ig1n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2bd736-65d2-4adb-bde1-27ca4fbada77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found. Please check the file name and modify if necessary.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    Image(filename=\"./coding/stock_price_ytd.png\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Image not found. Please check the file name and modify if necessary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AMMRaqbg1n9"
      },
      "source": [
        "## Use a Different Code Execution Environment\n",
        "\n",
        "The code execution happened in a separate process, so the plot is not directly displayed in the notebook. Is it possible to change the code execution environment into IPython?\n",
        "\n",
        "Yes! In the following we demonstrate how to extend the `UserProxyAgent` to use a different code execution environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from typing import Union, Dict\n",
        "import autogen\n",
        "from IPython import get_ipython\n",
        "\n",
        "class IPythonUserProxyAgent(autogen.UserProxyAgent):\n",
        "    def __init__(self, name: str, **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._ipython = get_ipython()\n",
        "\n",
        "    def generate_init_message(self, *args, **kwargs) -> Union[str, Dict]:\n",
        "        return (\n",
        "            super().generate_init_message(*args, **kwargs)\n",
        "            + \"\"\"\n",
        "If you suggest code, the code will be executed in IPython.\"\"\"\n",
        "        )\n",
        "\n",
        "    def run_code(self, code, **kwargs):\n",
        "        result = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "        log = self._ipython.ev(\"cap.stdout\")\n",
        "        log += self._ipython.ev(\"cap.stderr\")\n",
        "\n",
        "        if result.result is not None:\n",
        "            log += str(result.result)\n",
        "\n",
        "        exitcode = 0 if result.success else 1\n",
        "\n",
        "        if result.error_before_exec is not None:\n",
        "            log += f\"\\n{result.error_before_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "        if result.error_in_exec is not None:\n",
        "            log += f\"\\n{result.error_in_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "            # Check if the error message indicates that a module is not found\n",
        "            if \"No module named\" in str(result.error_in_exec):\n",
        "                missing_module = str(result.error_in_exec).split(\"'\")[1]\n",
        "                try:\n",
        "                    # Use pip search to find the correct package name\n",
        "                    search_result = subprocess.run([\"pip\", \"search\", missing_module], capture_output=True, text=True)\n",
        "\n",
        "                    # Check if the package name is found in the search result\n",
        "                    if missing_module.lower() in search_result.stdout.lower():\n",
        "                        print(f\"Installing {missing_module}...\")\n",
        "                        !pip install {missing_module}\n",
        "\n",
        "                        # Retry running the code after installation\n",
        "                        result_after_install = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "\n",
        "                        # Update log and exit code based on the result after installation\n",
        "                        log = self._ipython.ev(\"cap.stdout\") + self._ipython.ev(\"cap.stderr\")\n",
        "                        log += str(result_after_install.result) if result_after_install.result is not None else \"\"\n",
        "                        exitcode = 0 if result_after_install.success else 1\n",
        "\n",
        "                        if result_after_install.error_before_exec is not None:\n",
        "                            log += f\"\\n{result_after_install.error_before_exec}\"\n",
        "                            exitcode = 1\n",
        "                        if result_after_install.error_in_exec is not None:\n",
        "                            log += f\"\\n{result_after_install.error_in_exec}\"\n",
        "                            exitcode = 1\n",
        "                except Exception as install_error:\n",
        "                    # Handle installation error\n",
        "                    log += f\"\\nError installing {missing_module}: {install_error}\"\n",
        "                    exitcode = 1\n",
        "\n",
        "        return exitcode, log, None"
      ],
      "metadata": {
        "id": "8-g9uJKk5y93"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjYEo01Ag1oB"
      },
      "source": [
        "The implementation overrides three functions in `UserProxyAgent`:\n",
        "* constructor. We get the ipython instance as the code execution environment.\n",
        "* `generate_init_message`. We generate a modified initial message to send to the assistant agent, by adding the info that the execution will be performed in IPython.\n",
        "* `run_code`. We execute the code with the ipython instance.\n",
        "\n",
        "With the new `IPythonUserProxyAgent`, we are able to run the code within the current notebook environment and display the plot directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MUtY_kAGg1oD"
      },
      "outputs": [],
      "source": [
        "ipy_user = IPythonUserProxyAgent(\n",
        "    \"ipython_user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\")\n",
        "    or x.get(\"content\", \"\").rstrip().endswith('\"TERMINATE\".'),\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"coding\",\n",
        "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the assistant receives a message from the user, which contains the task description\n",
        "ipy_user.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\" please use the current dir for and show it and write a story about the child school corporation  in nature saving and creat one bussiness  plan for it and save it as docx file ,also  by ready mail to send this Bussiness  plan and ready LinkedIn post for it . The post would be saved at that bussiness plan DICX file and one presentation of bussiness plan sound by 10 word as sumerized would be created in Persian language and saved at the folder of docx saved ,\"\"\" #,please sumerize  it in gpt3.5 tokenk value then send to it :( https://telegra.ph/Awakening-the-Indifferent-Embracing-Progress-and-Standing-with-the-Thousand-Suns-01-15 ) \"\"\",#\"\"\"Plot a chart of META and TESLA stock price gain YTD\"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "jVVHoFJj8S5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e1c202-ac21-47d5-a36f-f3bd7e436d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython_user_proxy (to assistant):\n",
            "\n",
            " please use the current dir for and show it and write a story about the child school corporation  in nature saving and creat one bussiness  plan for it and save it as docx file ,also  by ready mail to send this Bussiness  plan and ready LinkedIn post for it . The post would be saved at that bussiness plan DICX file and one presentation of bussiness plan sound by 10 word as sumerized would be created in Persian language and saved at the folder of docx saved ,\n",
            "If you suggest code, the code will be executed in IPython.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the assistant receives a message from the user, which contains the task description\n",
        "ipy_user.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\" please use the '/content/autogen/codeing' folder as defailult folder to save .py ocdesa and docx or mp3 files and read this doc file (/content/autogen/business_plan.docx) and make a 10 word, as mp3 sound in Persian to describe it and run it to see what you made \"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZHHxC7R76mye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "from typing import Union, Dict\n",
        "import autogen\n",
        "from IPython import get_ipython\n",
        "\n",
        "class IPythonUserProxyAgent(autogen.UserProxyAgent):\n",
        "    def __init__(self, name: str, **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._ipython = get_ipython()\n",
        "\n",
        "    def generate_init_message(self, *args, **kwargs) -> Union[str, Dict]:\n",
        "        return (\n",
        "            super().generate_init_message(*args, **kwargs)\n",
        "            + \"\"\"\n",
        "If you suggest code, the code will be executed in IPython.\"\"\"\n",
        "        )\n",
        "\n",
        "    def run_code(self, code, **kwargs):\n",
        "        result = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "        log = self._ipython.ev(\"cap.stdout\")\n",
        "        log += self._ipython.ev(\"cap.stderr\")\n",
        "\n",
        "        if result.result is not None:\n",
        "            log += str(result.result)\n",
        "\n",
        "        exitcode = 0 if result.success else 1\n",
        "\n",
        "        if result.error_before_exec is not None:\n",
        "            log += f\"\\n{result.error_before_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "        if result.error_in_exec is not None:\n",
        "            log += f\"\\n{result.error_in_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "            # Check if the error message indicates that a module is not found\n",
        "            if \"No module named\" in str(result.error_in_exec):\n",
        "                missing_module = str(result.error_in_exec).split(\"'\")[1]\n",
        "                try:\n",
        "                    # Use pip install to install the missing module\n",
        "                    subprocess.check_call([\"pip\", \"install\", missing_module])\n",
        "\n",
        "                    # Retry running the code after installation\n",
        "                    result_after_install = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "\n",
        "                    # Update log and exit code based on the result after installation\n",
        "                    log = self._ipython.ev(\"cap.stdout\") + self._ipython.ev(\"cap.stderr\")\n",
        "                    log += str(result_after_install.result) if result_after_install.result is not None else \"\"\n",
        "                    exitcode = 0 if result_after_install.success else 1\n",
        "\n",
        "                    if result_after_install.error_before_exec is not None:\n",
        "                        log += f\"\\n{result_after_install.error_before_exec}\"\n",
        "                        exitcode = 1\n",
        "                    if result_after_install.error_in_exec is not None:\n",
        "                        log += f\"\\n{result_after_install.error_in_exec}\"\n",
        "                        exitcode = 1\n",
        "                except Exception as install_error:\n",
        "                    # Handle installation error\n",
        "                    log += f\"\\nError installing {missing_module}: {install_error}\"\n",
        "                    exitcode = 1\n",
        "\n",
        "        return exitcode, log, None"
      ],
      "metadata": {
        "id": "ibFcVMgmlbRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_list = [\n",
        "    {\n",
        "        'model': 'gpt-3.5-turbo',\n",
        "        'api_key': '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' #'<your OpenAI API key here>',\n",
        "    }]\n",
        "# create an AssistantAgent named \"assistant\"\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config={\n",
        "        \"cache_seed\": 42,  # seed for caching and reproducibility\n",
        "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
        "        \"temperature\": 0,  # temperature for sampling\n",
        "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
        ")\n",
        "# create a UserProxyAgent instance named \"user_proxy\"\n",
        "ipy_user = IPythonUserProxyAgent(\n",
        "    \"ipython_user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\")\n",
        "    or x.get(\"content\", \"\").rstrip().endswith('\"TERMINATE\".'),\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"coding\",\n",
        "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "iEp7be53ANk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "task = \"please  summarize one docx repost content by an upload file option and then and create a 30 second voice for that summarized in Persian and English languages. Please consider i have the openai api key and other module be free if possible\"# @param{type:'string'}\n",
        "ipy_user.initiate_chat(\n",
        "    assistant,\n",
        "    message=f\"{task}\",\n",
        ")"
      ],
      "metadata": {
        "id": "qCptDrI4-YZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "from typing import Union, Dict\n",
        "import autogen\n",
        "from IPython import get_ipython\n",
        "\n",
        "class IPythonUserProxyAgent(autogen.UserProxyAgent):\n",
        "    def __init__(self, name: str, **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._ipython = get_ipython()\n",
        "\n",
        "    def generate_init_message(self, *args, **kwargs) -> Union[str, Dict]:\n",
        "        return (\n",
        "            super().generate_init_message(*args, **kwargs)\n",
        "            + \"\"\"\n",
        "If you suggest code, the code will be executed in IPython.\"\"\"\n",
        "        )\n",
        "\n",
        "    def run_code(self, code, **kwargs):\n",
        "        result = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "        log = self._ipython.ev(\"cap.stdout\")\n",
        "        log += self._ipython.ev(\"cap.stderr\")\n",
        "\n",
        "        if result.result is not None:\n",
        "            log += str(result.result)\n",
        "\n",
        "        exitcode = 0 if result.success else 1\n",
        "\n",
        "        if result.error_before_exec is not None:\n",
        "            log += f\"\\n{result.error_before_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "        if result.error_in_exec is not None:\n",
        "            log += f\"\\n{result.error_in_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "            # Check if the error message indicates that a module is not found\n",
        "            if \"No module named\" in str(result.error_in_exec):\n",
        "                missing_module = str(result.error_in_exec).split(\"'\")[1]\n",
        "                try:\n",
        "                    # Use pip install to install the missing module\n",
        "                    subprocess.check_call([\"pip\", \"install\", missing_module])\n",
        "\n",
        "                    # Retry running the code after installation\n",
        "                    result_after_install = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "\n",
        "                    # Update log and exit code based on the result after installation\n",
        "                    log = self._ipython.ev(\"cap.stdout\") + self._ipython.ev(\"cap.stderr\")\n",
        "                    log += str(result_after_install.result) if result_after_install.result is not None else \"\"\n",
        "                    exitcode = 0 if result_after_install.success else 1\n",
        "\n",
        "                    if result_after_install.error_before_exec is not None:\n",
        "                        log += f\"\\n{result_after_install.error_before_exec}\"\n",
        "                        exitcode = 1\n",
        "                    if result_after_install.error_in_exec is not None:\n",
        "                        log += f\"\\n{result_after_install.error_in_exec}\"\n",
        "                        exitcode = 1\n",
        "                except Exception as install_error:\n",
        "                    # Handle installation error\n",
        "                    log += f\"\\nError installing {missing_module}: {install_error}\"\n",
        "                    exitcode = 1\n",
        "\n",
        "        return exitcode, log, None\n",
        "\n",
        "# Create an AssistantAgent named \"assistant\"\n",
        "assistant = autogen.AssistantAgent(\n",
        "   name=\"assistant\",\n",
        "   llm_config={\n",
        "       \"cache_seed\": 42, # seed for caching and reproducibility\n",
        "       \"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": \"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\"}], # a list of OpenAI API configurations\n",
        "       \"temperature\": 0, # temperature for sampling\n",
        "   }, # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
        ")\n",
        "\n",
        "# Create a UserProxyAgent instance named \"user_proxy\"\n",
        "ipy_user = IPythonUserProxyAgent(\n",
        "   \"ipython_user_proxy\",\n",
        "   human_input_mode=\"NEVER\",\n",
        "   max_consecutive_auto_reply=10,\n",
        "   is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\")\n",
        "   or x.get(\"content\", \"\").rstrip().endswith('\"TERMINATE\".'),\n",
        "   code_execution_config={\n",
        "       \"work_dir\": \"coding\",\n",
        "       \"use_docker\": False, # set to True or image name like \"python:3\" to use docker\n",
        "   },\n",
        ")\n",
        "\n",
        "task = \"Please summarize one docx report content by an upload file option and then create a 30-second voice for that summarized in Persian and English languages. Please consider I have the OpenAI API key and other modules be free if possible\"\n",
        "ipy_user.initiate_chat(\n",
        "   assistant,\n",
        "   message=f\"{task}\",\n",
        ")"
      ],
      "metadata": {
        "id": "UOAJD7sHncb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from typing import Union, Dict\n",
        "import autogen\n",
        "from IPython import get_ipython\n",
        "\n",
        "class IPythonUserProxyAgent(autogen.UserProxyAgent):\n",
        "    def __init__(self, name: str, **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._ipython = get_ipython()\n",
        "\n",
        "    def generate_init_message(self, *args, **kwargs) -> Union[str, Dict]:\n",
        "        return (\n",
        "            super().generate_init_message(*args, **kwargs)\n",
        "            + \"\"\"\n",
        "If you suggest code, the code will be executed in IPython.\"\"\"\n",
        "        )\n",
        "\n",
        "    def run_code(self, code, **kwargs):\n",
        "        result = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "        log = self._ipython.ev(\"cap.stdout\")\n",
        "        log += self._ipython.ev(\"cap.stderr\")\n",
        "\n",
        "        if result.result is not None:\n",
        "            log += str(result.result)\n",
        "\n",
        "        exitcode = 0 if result.success else 1\n",
        "\n",
        "        if result.error_before_exec is not None:\n",
        "            log += f\"\\n{result.error_before_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "        if result.error_in_exec is not None:\n",
        "            log += f\"\\n{result.error_in_exec}\"\n",
        "            exitcode = 1\n",
        "\n",
        "            # Check if the error message indicates that a module is not found\n",
        "            if \"No module named\" in str(result.error_in_exec):\n",
        "                missing_module = str(result.error_in_exec).split(\"'\")[1]\n",
        "                try:\n",
        "                    # Use pip search to find the correct package name\n",
        "                    search_result = subprocess.run([\"pip\", \"search\", missing_module], capture_output=True, text=True)\n",
        "\n",
        "                    # Check if the package name is found in the search result\n",
        "                    if missing_module.lower() in search_result.stdout.lower():\n",
        "                        print(f\"Installing {missing_module}...\")\n",
        "                        !pip install {missing_module}\n",
        "\n",
        "                        # Retry running the code after installation\n",
        "                        result_after_install = self._ipython.run_cell(\"%%capture --no-display cap\\n\" + code)\n",
        "\n",
        "                        # Update log and exit code based on the result after installation\n",
        "                        log = self._ipython.ev(\"cap.stdout\") + self._ipython.ev(\"cap.stderr\")\n",
        "                        log += str(result_after_install.result) if result_after_install.result is not None else \"\"\n",
        "                        exitcode = 0 if result_after_install.success else 1\n",
        "\n",
        "                        if result_after_install.error_before_exec is not None:\n",
        "                            log += f\"\\n{result_after_install.error_before_exec}\"\n",
        "                            exitcode = 1\n",
        "                        if result_after_install.error_in_exec is not None:\n",
        "                            log += f\"\\n{result_after_install.error_in_exec}\"\n",
        "                            exitcode = 1\n",
        "                except Exception as install_error:\n",
        "                    # Handle installation error\n",
        "                    log += f\"\\nError installing {missing_module}: {install_error}\"\n",
        "                    exitcode = 1\n",
        "\n",
        "        return exitcode, log, None"
      ],
      "metadata": {
        "id": "pVTVOkEHpd9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ipy_user.initiate_chat(\n",
        "    assistant,\n",
        "    message=\"\"\" please use write a story in 40 word and save it in docx with photo cover, and make the mp3 sound in Persian for describing it \"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "qiBjiiOpg0A5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}