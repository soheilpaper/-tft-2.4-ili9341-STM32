{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/youtube_subtitle/Transcribing_YT_Videos_2_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irvr05hHNYB7"
      },
      "source": [
        "# Transcribing YouTube videos üìπ to text ‚úç\n",
        "\n",
        "Here is the basic idea for this workflow:   \n",
        "\n",
        "1. Get a URL from YT\n",
        "2. Use pytube to download the video locally as .MP4\n",
        "3. Use Whisper by OpenAI to transcribe the audio\n",
        "4. Generate subtitles for YT\n",
        "4. Analyze the extracted text:  \n",
        "  - Summarize it\n",
        "  - Extract questions\n",
        "  - Answer the above questions\n",
        "  - Answer any question\n",
        "\n",
        "---\n",
        "If you are interested in [SEO automation](https://wordlift.io/blog/en/seo-automation/) you might want to read this blog post.\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "  <a href=\"https://wordlift.io\">\n",
        "    <img width=130px src=\"https://wordlift.io/wp-content/uploads/2018/07/logo-assets-510x287.png\" />\n",
        "    </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      by\n",
        "      <a href=\"https://wordlift.io/blog/en/entity/andrea-volpini\">\n",
        "        Andrea Volpini\n",
        "      </a>\n",
        "      MIT License\n",
        "      <br/>\n",
        "      <br/>\n",
        "      <i>Last updated: <b>January 19, 2023</b></i>\n",
        "  </td>\n",
        "</table>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "global gdrive_fpath\n",
        "drive_mounted = False\n",
        "gdrive_fpath = '.'\n",
        "local_path = '/content/'\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "if mount_gdrive : # and not drive_mounted:\n",
        "    from google.colab import drive\n",
        "\n",
        "    gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "    gdrive_subdirectory = 'MyDrive/YouTube_Auto_Subtitle' #@param{type:\"string\"}\n",
        "    gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "    print (\"gdrive path is :\",gdrive_fpath)\n",
        "   # Mount Google Drive\n",
        "    if not os.path.isdir(gdrive_mountpoint):\n",
        "     # If not, mount the drive\n",
        "       drive.mount(gdrive_mountpoint)\n",
        "       if not os.path.exists(gdrive_fpath):\n",
        "          os.makedirs(gdrive_fpath)\n",
        "          os.chdir(gdrive_fpath)\n",
        "    else:\n",
        "          print(\"Drive is already mounted.\")\n",
        "else:\n",
        "   Folder_fpath ='/content/' #@param{type:\"string\"}\n",
        "   #gdrive_subdirectory = 'MyDrive/ChatGPT_Paper_wrting' #@param{type:\"string\"}\n",
        "   gdrive_fpath = Folder_fpath\n",
        "   os.chdir(gdrive_fpath)\n",
        "folder_path = gdrive_fpath"
      ],
      "metadata": {
        "id": "O2jD0o0vYdDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksVebkXDPjNe"
      },
      "source": [
        "## 0. Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRB4prIxFQvD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pytube\n",
        "!pip install -U mock\n",
        "#!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install sentencepiece\n",
        "\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "import os\n",
        "from typing import Iterator, TextIO\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SrdXQHgdxDO"
      },
      "source": [
        "## 1. Get the URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxVAiYThFGqE"
      },
      "outputs": [],
      "source": [
        "#@title Add here the YT URL\n",
        "\n",
        "link = 'https://m.youtube.com/watch?v=LI-_HpIDyoI' #@param {type:\"string\"}\n",
        "\n",
        "file_path = f'{folder_path}/WL_video.mp4'\n",
        "# @ param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "titbebjKq_8D"
      },
      "source": [
        "## 2. Download the video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import mock\n",
        "\n",
        "from pytube.cipher import get_throttling_function_code\n",
        "\n",
        "def patched_throttling_plan(js: str):\n",
        "    \"\"\"Patch throttling plan, from https://github.com/pytube/pytube/issues/1498\"\"\"\n",
        "    raw_code = get_throttling_function_code(js)\n",
        "\n",
        "    transform_start = r\"try{\"\n",
        "    plan_regex = re.compile(transform_start)\n",
        "    match = plan_regex.search(raw_code)\n",
        "\n",
        "    #transform_plan_raw = find_object_from_startpoint(raw_code, match.span()[1] - 1)\n",
        "    transform_plan_raw = js\n",
        "\n",
        "    # Steps are either c[x](c[y]) or c[x](c[y],c[z])\n",
        "    step_start = r\"c\\[(\\d+)\\]\\(c\\[(\\d+)\\](,c(\\[(\\d+)\\]))?\\)\"\n",
        "    step_regex = re.compile(step_start)\n",
        "    matches = step_regex.findall(transform_plan_raw)\n",
        "    transform_steps = []\n",
        "    for match in matches:\n",
        "        if match[4] != '':\n",
        "            transform_steps.append((match[0],match[1],match[4]))\n",
        "        else:\n",
        "            transform_steps.append((match[0],match[1]))\n",
        "\n",
        "    return transform_steps\n",
        "\n",
        "\n",
        "with mock.patch('pytube.cipher.get_throttling_plan', patched_throttling_plan):\n",
        "    from pytube import YouTube\n",
        "    url = link\n",
        "\n",
        "    video = YouTube(url)\n",
        "    audio = video.streams.filter(only_audio=True, file_extension='mp4')[0]\n",
        "    audio.download()"
      ],
      "metadata": {
        "id": "yFMgKjqrKzM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgzjYcmBFGqE"
      },
      "outputs": [],
      "source": [
        "# here is the function to get the video\n",
        "def get_video(_link, _path):\n",
        "  try:\n",
        "      yt = YouTube(_link)\n",
        "  except:\n",
        "      print(\"Connection Error\")\n",
        "\n",
        "  yt.streams.filter(file_extension='mp4')\n",
        "  stream = yt.streams.get_by_itag(139)\n",
        "  stream.download('',_path)\n",
        "  title = yt.title\n",
        "  print(\"video downloaded\")\n",
        "  return title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibA2bGLuFGqF"
      },
      "outputs": [],
      "source": [
        "title = get_video(link, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1hXccSNrGQv"
      },
      "source": [
        "## 3. Transcribe the audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJwhMsOhJgQm"
      },
      "outputs": [],
      "source": [
        "# transcribe the audio track using Whisper by OpenAI set _translate=False to transcribe it in the original language\n",
        "\n",
        "def transcribe_me(_video, _translate=False):\n",
        "  # setting options to define if translating or transcribing\n",
        "  if _translate:\n",
        "    _options = dict(task=\"translate\", beam_size=5, best_of=5)\n",
        "  else:\n",
        "    _options = dict(task=\"transcribe\", beam_size=5, best_of=5)\n",
        "  model = whisper.load_model(\"large-v2\")  # for english the .en tend to perform better otherwise just use \"base\"\n",
        "  result = model.transcribe(_video, **_options)\n",
        "\n",
        "  return result, result[\"text\"], result[\"segments\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwDvWhip6zxk"
      },
      "outputs": [],
      "source": [
        "# store the the entire result (output), the extracted text (extracted_text) and the segments (res)\n",
        "\n",
        "output, extracted_text, res  = transcribe_me(file_path)\n",
        "extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWZsGWpEc7qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Create the subtitles in English for YouTube (optional)\n",
        "\n",
        "By running the cell below you will be able to generate automatically  the subtitle file for any youtube video. Whisper will be translating any language to English (unless you set to `_translate=False` in  `transcribe_me()` above)."
      ],
      "metadata": {
        "id": "6xde1CCApjV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYS47pJCK729"
      },
      "outputs": [],
      "source": [
        "def format_timestamp(seconds: float):\n",
        "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
        "    milliseconds = round(seconds * 1000.0)\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    return (f\"{hours}:\" if hours > 0 else \"\") + f\"{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
        "\n",
        "\n",
        "def write_vtt(format,transcript: Iterator[dict], file: TextIO):\n",
        "    print(f\"WEB{format}\\n\", file=file)\n",
        "    for segment in transcript:\n",
        "        print(\n",
        "            f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\\n\"\n",
        "            f\"{segment['text'].replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def slugify(title):\n",
        "    return \"\".join(c if c.isalnum() else \"_\" for c in title).rstrip(\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vtt_path = os.path.join(f\"{folder_path}\", f\"{slugify(title)}.vtt\")\n",
        "with open(vtt_path, 'w', encoding=\"utf-8\") as vtt:\n",
        "  write_vtt('vvt',output[\"segments\"], file=vtt)\n",
        "  # print saved message with absolute path\n",
        "  print(\"Saved VTT to\", os.path.abspath(vtt_path))"
      ],
      "metadata": {
        "id": "dxq32jK2e-Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download subtitles\n",
        "# @markdown You can download:\n",
        "# @markdown 1. The plaintext transcript (txt)\n",
        "# @markdown 2. Multiple subtitles formats (srt, vtt, tsv)\n",
        "# @markdown 3. Whisper format? (JSON)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "format = \"srt\" # @param [\"txt\", \"srt\", \"vtt\", \"tsv\", \"json\"]\n",
        "vtt_path = os.path.join(\".\", f\"{slugify(title)}.{format}\")\n",
        "with open(vtt_path, 'w', encoding=\"utf-8\") as format:\n",
        "  write_vtt(format,output[\"segments\"], file=format)\n",
        "  # print saved message with absolute path\n",
        "  print(\"Saved VTT to\", os.path.abspath(vtt_path))\n",
        "files.download(f\"{vtt_path}\")"
      ],
      "metadata": {
        "id": "TdQ1193qdWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6dbISA155vw"
      },
      "source": [
        "## 4. Analyze the extracted text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxisgPzg77_2"
      },
      "source": [
        "### 4.1 Run the summarizier\n",
        "\n",
        "We will use GPT-3 to simplify the summary we previously generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af2N3AcrLHon",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Add here your GPT-3 key\n",
        "\n",
        "key = '' #@param {type:\"string\"}\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install langchain and other libraries"
      ],
      "metadata": {
        "id": "sectmckhdNew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuZxu8sZJwav"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install --upgrade faiss-gpu==1.7.1\n",
        "\n",
        "import re\n",
        "import openai\n",
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.chains import VectorDBQAWithSourcesChain\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "import faiss\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Summarization"
      ],
      "metadata": {
        "id": "s0fRtvlk7UTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gp3_summarize_new(_text, _chunk_size):\n",
        "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "    #text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=_chunk_size, chunk_overlap=0)\n",
        "\n",
        "    #text_splitter = SpacyTextSplitter(chunk_size=_chunk_size)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = _chunk_size,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,)\n",
        "\n",
        "    texts = text_splitter.split_text(_text)\n",
        "    docs = [Document(page_content=t) for t in texts[:15]]\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "    out = chain.run(docs)\n",
        "    return out.strip()"
      ],
      "metadata": {
        "id": "XQEORm3D8lB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = gp3_summarize_new(extracted_text, 1600)\n",
        "summary"
      ],
      "metadata": {
        "id": "9-LRCPNn9nUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyNreYW6MAoy"
      },
      "source": [
        "### 4.2 Extract the list of Questions and Answer them\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the questions\n",
        "def gp3_extract_questions(_text_chunks):\n",
        "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "    texts = _text_chunks\n",
        "    docs = [Document(page_content=t) for t in texts[:3]]\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    docsearch = FAISS.from_texts(texts, embeddings)\n",
        "    query = \"Extract the questions from this document\"\n",
        "\n",
        "    try:\n",
        "      docs = docsearch.similarity_search(query)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    chain_refine = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\n",
        "    out = chain_refine({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
        "\n",
        "    questions = out[\"output_text\"].splitlines()\n",
        "    questions = [q for q in questions if re.match(r\"^\\d+\\.\", q)]\n",
        "\n",
        "    return questions\n",
        "\n",
        "# split the transcription into segments and return text and timecodes\n",
        "def store_segments(segments):\n",
        "  texts = []\n",
        "  start_times = []\n",
        "\n",
        "  for segment in segments:\n",
        "    text = segment['text']\n",
        "    start = segment['start']\n",
        "\n",
        "    # Convert the starting time to a datetime object\n",
        "    start_datetime = datetime.fromtimestamp(start)\n",
        "\n",
        "    # Format the starting time as a string in the format \"00:00:00\"\n",
        "    formatted_start_time = start_datetime.strftime('%H:%M:%S')\n",
        "\n",
        "    texts.append(\"\".join(text))\n",
        "    start_times.append(formatted_start_time)\n",
        "\n",
        "  return texts, start_times\n",
        "\n",
        "# find the segment that answer best the question\n",
        "def answer_question(_texts, _start_times, _question):\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=1500)\n",
        "  docs = []\n",
        "  metadatas = []\n",
        "  for i, d in enumerate(_texts):\n",
        "\n",
        "    splits = text_splitter.split_text(d)\n",
        "    docs.extend(splits)\n",
        "    metadatas.extend([{\"source\": _start_times[i]}] * len(splits))\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "  store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "  faiss.write_index(store.index, \"docs.index\")\n",
        "\n",
        "  chain = VectorDBQAWithSourcesChain.from_llm(llm=OpenAI(temperature=0), vectorstore=store)\n",
        "  result = chain({\"question\": _question})\n",
        "\n",
        "  answer = result['answer']\n",
        "  sources = result['sources']\n",
        "\n",
        "  return answer, sources\n",
        "\n",
        "# create a link that directs the user to the provided timecode\n",
        "def add_timecode_to_url(url, timecode):\n",
        "    # convert timecode to seconds\n",
        "    time_parts = timecode.split(':')\n",
        "    seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])\n",
        "    # append the timecode to the URL\n",
        "    return url + \"&t=\" + str(seconds) + \"s\""
      ],
      "metadata": {
        "id": "p8cpxOncuM6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 Here are the questions\n"
      ],
      "metadata": {
        "id": "DXnHWi7E805I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we store text and timecodes\n",
        "texts, start_times = store_segments(res)\n",
        "\n",
        "# here is the list of questions\n",
        "questions = gp3_extract_questions(texts)\n",
        "questions"
      ],
      "metadata": {
        "id": "7YU7xozfy8Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2 Here are the answers\n"
      ],
      "metadata": {
        "id": "rt5hrLVS85uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the answers for the generted questions and provide the links to the video\n",
        "\n",
        "def get_output_df(questions, texts, start_times):\n",
        "    data = []\n",
        "    for q in questions:\n",
        "        try:\n",
        "            r = answer_question(texts, start_times, q)\n",
        "            timecodes = r[1].split(\",\")\n",
        "            urls = []\n",
        "            for t in timecodes:\n",
        "                urls.append(add_timecode_to_url(link, t))\n",
        "            data.append([q, r[0], r[1], urls])\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    df = pd.DataFrame(data, columns=['Question', 'Answer', 'Timecodes', 'URLs'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "Rj2QVa38dBTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7aa9YdsMfNR"
      },
      "outputs": [],
      "source": [
        "#@title 5.1.1 Ask any question\n",
        "\n",
        "question = 'What are the most interesting change?' #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pQu8R89MvjK"
      },
      "outputs": [],
      "source": [
        "r = answer_question(texts, start_times, question)\n",
        "print(question)\n",
        "print(r[0])\n",
        "timecodes = r[1].split(\",\")\n",
        "print(r[1])\n",
        "\n",
        "for t in timecodes:\n",
        "  print(add_timecode_to_url(link, t))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('tjupXBKObXs', start=965)"
      ],
      "metadata": {
        "id": "5f0zoRUDGIWN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "ksVebkXDPjNe",
        "titbebjKq_8D",
        "X1hXccSNrGQv",
        "6xde1CCApjV8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}