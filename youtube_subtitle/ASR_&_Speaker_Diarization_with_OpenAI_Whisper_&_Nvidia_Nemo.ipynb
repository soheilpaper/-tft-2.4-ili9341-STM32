{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/youtube_subtitle/ASR_%26_Speaker_Diarization_with_OpenAI_Whisper_%26_Nvidia_Nemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ASR (OpenAI Whisper)\n",
        "\n",
        "In this section, we'll use OpenAI newly release Whisper to convert a sample youtube podcast to words <> timestamps mapping."
      ],
      "metadata": {
        "id": "lzGju9CtkBlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg\n",
        "!pip install pydub\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SxFC0FSXXVs",
        "outputId": "a100da81-2171-4846-eb57-52303dd2ca1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BXDAFYsUdLF",
        "outputId": "5def4af9-86c5-4768-84ab-fa427d6eca1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.7.25-py3-none-any.whl.metadata (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.1/170.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.7.4)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting requests<3,>=2.32.2 (from yt-dlp)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive_mount_path = Path(\"/content/drive\")\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"MyDrive\""
      ],
      "metadata": {
        "id": "BZ5Ilg83NogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Enter the URL of mp3/WAV/and ..file or the YouTube video, or the path to the video/audio file you want to transcribe, give the output path, and run the cell. HTML file is generated only for YouTube videos\n",
        "# Example usage\n",
        "\n",
        "#Source = 'File (Google Drive)' #@param ['Youtube', 'File (Google Drive)']\n",
        "\n",
        "Source = 'Youtube' #@param ['Youtube', 'File (Google Drive)','Download from the URL']\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **Download Podcast from URL**\n",
        "URL = \"https://fileiran.net/Download/File/Ac1YgzeIzI/20636_Episode%2065%20-%20Radio%20Marz.mp3\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Youtube video**\n",
        "video_url = \"https://youtu.be/NSp2fEQ6wyA\" #@param {type:\"string\"}\n",
        "#store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video or audio path (mp4, wav, mp3)**\n",
        "#@markdown ##### **NOTE:** _This will extract audio only for video files_\n",
        "video_path = \"/content/drive/MyDrive/Customer_Service.mp3\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "output_path = \"/content/drive/MyDrive/transcript/\" #@param {type:\"string\"}\n",
        "output_path = str(Path(output_path))\n",
        "#@markdown ---\n",
        "#@markdown #### **Huggingface API key**\n",
        "access_token = \"YOUR_HUGGINGFACE_API_KEY\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Title to use for transcription**\n",
        "audio_title = \"Transcription of Conversation\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**"
      ],
      "metadata": {
        "id": "sqXNAhoQLxfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "%cd {output_path}\n",
        "video_title = \"\"\n",
        "video_id = \"\""
      ],
      "metadata": {
        "id": "wyPRDchdN7U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Add here the YT URL\n",
        "\n",
        "# link = 'https://youtu.be/A-mLVVJkH7I?si=qtWgD2BAOA2R219c' #@param {type:\"string\"}\n",
        "if Source == 'Youtube' :\n",
        "  link = video_url\n",
        "  #file_path = f'{folder_path}/WL_video.mp4' #@param {type:\"string\"}\n",
        "\n",
        "  # A 20mins podcast from YC official youtube channel.\n",
        "  !rm ./audio.wav\n",
        "  !yt-dlp -xv --audio-format wav  -o audio.wav -- {link} #https://www.youtube.com/watch?v=bp_kMA-eTsE"
      ],
      "metadata": {
        "id": "nBRKOgXFWAqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if Source == 'Youtube' :\n",
        "  !rm ./audio_16k.wav\n",
        "  !ffmpeg -i audio.wav -ac 1 -ar 16000 audio_16k.wav # Converting audio.wav to mono channel & 16K audio_16k.wav\n",
        "  filepath = output_path+'/audio.wav'"
      ],
      "metadata": {
        "id": "FJTiNR8iXzd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from pydub import AudioSegment\n",
        "from tqdm import tqdm # Import tqdm for progress bars\n",
        "\n",
        "if Source == 'Download from the URL':\n",
        "  os.chdir('/content')\n",
        "# Function to download a file from a URL with a progress bar\n",
        "def download_file_from_url(url: str, output_filename: str = None):\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        if not output_filename:\n",
        "            output_filename = url.split(\"/\")[-1]\n",
        "        with open(output_filename, 'wb') as file:\n",
        "            # Use tqdm to create a progress bar\n",
        "            for chunk in tqdm(response.iter_content(chunk_size=1024),\n",
        "                              unit='KB', unit_scale=True, desc=f\"Downloading {output_filename}\"):\n",
        "                if chunk:\n",
        "                    file.write(chunk)\n",
        "        return output_filename\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def convert_audio_to_wav(input_file: str):\n",
        "    output_file = os.path.splitext(input_file)[0] + \".wav\"\n",
        "    if input_file.lower().endswith(\".mp3\"):\n",
        "        audio = AudioSegment.from_mp3(input_file)\n",
        "    elif input_file.lower().endswith(\".m4a\"):\n",
        "        audio = AudioSegment.from_file(input_file, \"m4a\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported audio format. Please provide an MP3 or M4A file.\")\n",
        "\n",
        "    # Export the audio without using a progress_hook\n",
        "    audio.export(output_file, format=\"wav\", codec=\"pcm_s16le\", parameters=[\"-q:a\", \"0\"])\n",
        "    return output_file\n",
        "\n",
        "if Source == 'Download from the URL':\n",
        "\n",
        "  result = download_file_from_url(URL)\n",
        "  if not result:\n",
        "    print(\"Error: Unable to download file.\")\n",
        "  else:\n",
        "    print(f\"Downloaded file to '{result}'\")\n",
        "    wav_file = convert_audio_to_wav(result)\n",
        "    print(f\"Converted file to WAV format and saved as '{wav_file}'\")\n",
        "  (title, filepath) = 'Title','/content/'+wav_file\n",
        "  print(title, filepath)"
      ],
      "metadata": {
        "id": "xy_JtdesMFki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Whisper and WhisperX\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/m-bain/whisperX.git"
      ],
      "metadata": {
        "id": "HIGpaI-kYSbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from whisper import load_model\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Large models result in considerably better and more aligned (words, timestamps) mapping.\n",
        "model = load_model(\"large-v1\")\n",
        "# Set the PYTORCH_CUDA_ALLOC_CONF environment variable\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Beam size if None by default (Greedy Decoding). You can also set the\n",
        "# beam_size to some number like 5. This will increase in better transcription\n",
        "# quality but it'll increase runtime considerabley.\n",
        "results = model.transcribe(filepath,beam_size=None) # './audio_16k.wav', beam_size=None)"
      ],
      "metadata": {
        "id": "c_-9eR5aYvNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WhisperX results in better word timestamps by using wav2vec based forced alignment.\n",
        "import whisperx\n",
        "\n",
        "device = 'cuda'\n",
        "#alignment_model, metadata = whisperx.load_align_model(language_code=results[\"language\"], device=device)\n",
        "#result_aligned = whisperx.align(results[\"segments\"], alignment_model, metadata, './audio_16k.wav', device)"
      ],
      "metadata": {
        "id": "ZTXC4YWagyks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You WhisperX alignment may fail. If this happens then it's most probably because Whisper just `hallucinated` i.e. whisper came up with extra/weird output at the end.\n",
        "\n",
        "This usually happens with long audio files. If this happens, I'd suggest splitting big audio files in small files."
      ],
      "metadata": {
        "id": "lleAULPd7TYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe the audio track using Whisper by OpenAI set _translate=False to transcribe it in the original language\n",
        "import whisper\n",
        "def transcribe_me(_video, _translate=False):\n",
        "  # setting options to define if translating or transcribing\n",
        "  if _translate:\n",
        "    _options = dict(task=\"translate\", beam_size=5, best_of=5)\n",
        "  else:\n",
        "    _options = dict(task=\"transcribe\", beam_size=5, best_of=5)\n",
        "  model = whisper.load_model(\"large-v2\")  # for english the .en tend to perform better otherwise just use \"base\"\n",
        "  result = model.transcribe(_video, **_options)\n",
        "\n",
        "  return result, result[\"text\"], result[\"segments\"]"
      ],
      "metadata": {
        "id": "ePIbCkpMhi3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# store the the entire result (output), the extracted text (extracted_text) and the segments (res)\n",
        "file_path= './audio_16k.wav'\n",
        "output, extracted_text, res  = transcribe_me(file_path)\n",
        "extracted_text"
      ],
      "metadata": {
        "id": "TzecKlvXhk4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing words <> timestamps mapping in a file.\n",
        "import json\n",
        "\n",
        "with open('./word_ts.text', 'w+') as f:\n",
        "    for line in extracted_text : #result_aligned['word_segments']:\n",
        "        line_temp = line.copy()\n",
        "        # WhisperX don't put a space after word but just to make sure.\n",
        "        line_temp['text'] = line_temp['text'].strip()\n",
        "        f.write(f'{json.dumps(line_temp)}\\n')"
      ],
      "metadata": {
        "id": "si9d95bmi4dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Diarization (Nvidian NEMO)\n",
        "\n",
        "In this section, we'll find timestamps <> Speaker Labels mapping using Nvidia NEMO."
      ],
      "metadata": {
        "id": "g-1TunKPjq9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install sox libsndfile1"
      ],
      "metadata": {
        "id": "eeLMbPKdj-Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade hydra-core llvmlite omegaconf --ignore-installed"
      ],
      "metadata": {
        "id": "dbHyqDywl7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@main"
      ],
      "metadata": {
        "id": "0E7NXp_Gn2uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade Cython jiwer braceexpand webdataset librosa sentencepiece\n",
        "!pip install --upgrade youtokentome pyannote-audio transformers pandas inflect editdistance"
      ],
      "metadata": {
        "id": "OdvOv3hsqSjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pytorch-lightning"
      ],
      "metadata": {
        "id": "osqoA5OsuYM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have to restart the runtime at this stage for pytorch-lightning to take effect. Otherwise you may see errors like `Couldn't find lightning.core or lightining.logging`."
      ],
      "metadata": {
        "id": "jCm-vAlt63i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "diarize_manifest = {\n",
        "  'audio_filepath': f'./audio_16k.wav',\n",
        "  'offset': 0,\n",
        "  'duration':  None,\n",
        "  'label': \"infer\",\n",
        "  'text': \"-\",\n",
        "  'num_speakers': None,\n",
        "  'rttm_filepath': f'./diarized/pred_rttms/audio_16k.rttm',\n",
        "  'uniq_id': \"\"\n",
        "}\n",
        "\n",
        "if not os.path.exists('./manifest.json'):\n",
        "  with open('./manifest.json', 'w') as f:\n",
        "    f.write(json.dumps(diarize_manifest))"
      ],
      "metadata": {
        "id": "S9hkkI_Qq4lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "MODEL_CONFIG = os.path.join('./','diar_infer_meeting.yaml')\n",
        "if not os.path.exists(MODEL_CONFIG):\n",
        "    config_url = \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_meeting.yaml\"\n",
        "    MODEL_CONFIG = wget.download(config_url, './')\n",
        "\n",
        "config = OmegaConf.load(MODEL_CONFIG)"
      ],
      "metadata": {
        "id": "9sfEVBiTq73a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_workers = 4\n",
        "config.batch_size = 32\n",
        "\n",
        "config.diarizer.manifest_filepath = './manifest.json'\n",
        "config.diarizer.out_dir = os.path.join('./', 'diarized')\n",
        "config.diarizer.speaker_embeddings.model_path = 'titanet_large'\n",
        "config.diarizer.speaker_embeddings.parameters.window_length_in_sec = [1.5, 1.0, 0.5]\n",
        "config.diarizer.speaker_embeddings.parameters.shift_length_in_sec = [0.75, 0.5, 0.25]\n",
        "config.diarizer.speaker_embeddings.parameters.multiscale_weights = [0.33, 0.33, 0.33]\n",
        "config.diarizer.speaker_embeddings.parameters.save_embeddings = False\n",
        "\n",
        "config.diarizer.ignore_overlap = False\n",
        "config.diarizer.oracle_vad = False\n",
        "config.diarizer.collar = 0.25\n",
        "\n",
        "\n",
        "config.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
        "config.diarizer.oracle_vad = False # ----> Not using oracle VAD"
      ],
      "metadata": {
        "id": "8jrvJBSlrEVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.asr.models.msdd_models import ClusteringDiarizer\n",
        "\n",
        "model = ClusteringDiarizer(cfg=config)"
      ],
      "metadata": {
        "id": "w5d10A9irbAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.diarize()"
      ],
      "metadata": {
        "id": "tGLe96HUu_hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "speaker_ts = []\n",
        "with open('./diarized/pred_rttms/audio_16k.rttm', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(' ')\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split('_')[-1])])\n"
      ],
      "metadata": {
        "id": "-PFVb4R_v0Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-processing\n",
        "\n",
        "From previous cells, we have got `word_ts` (words <> timestamps mapping) and `speaker_ts` (speaker label <> timestamps mapping). In this cell, we are going\n",
        "to merge them together to get a list of (word, speaker label, timestamps) tuples."
      ],
      "metadata": {
        "id": "uLMK1APzK3bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading words <> timestamps mapping, which we saved earlier\n",
        "import json\n",
        "\n",
        "word_ts = []\n",
        "with open('./word_ts.text', 'r+') as f:\n",
        "    for line in f:\n",
        "        line_temp = json.loads(line)\n",
        "        word_ts.append(line_temp)"
      ],
      "metadata": {
        "id": "B8Q-hBtHwkT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_ts_anchor(s, e, option='start'):\n",
        "  if option == 'end':\n",
        "    return e\n",
        "  elif option == 'mid':\n",
        "    return (s + e) / 2\n",
        "  return s\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option='start'):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = int(wrd_dict['start'] * 1000), int(wrd_dict['end'] * 1000), wrd_dict['text']\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "        wrd_spk_mapping.append({'word': wrd, 'start_time': ws, 'end_time': we, 'speaker': sp})\n",
        "    return wrd_spk_mapping"
      ],
      "metadata": {
        "id": "Dv3mD3mlx7LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsm = get_words_speaker_mapping(word_ts, speaker_ts, 'start')"
      ],
      "metadata": {
        "id": "fa25lR13zR6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get accurate diarization results, we rely on correct sentence-ending punctuation i.e. period(.), question mark(?) and, exclamation(!). In all the test-cases, which I have performed using this diarization technique, Whisper was able to come up with correct punctuation markings. But there are chances for whisper to get stuck in no-punctuation loop.\n",
        "\n",
        "One way to fix this is to use a neutral initial prompt with punctuations e.g. `model.transcribe(FILE_NAME, initial_promot='How are you doing? I hope everything is OK.')`. This approach has couple of disadvantages:\n",
        "\n",
        "\n",
        "\n",
        "*   It could affect the transcription quality e.g. for one of my test cases, using `initial_promot` gave me `founders week real quick`, whereas the ground truth was `founders we'd work with.`\n",
        "*   Transcription timestamps could be out-of-sync. For one of the examples, I observed a lag of 2 seconds which could be fixed only if we knew beforehand how much that'd be.\n",
        "\n",
        "A better approach is to use a transformer based model to restore punctuation of the transcript.\n",
        "\n"
      ],
      "metadata": {
        "id": "97aYteYnxtKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepmultilingualpunctuation"
      ],
      "metadata": {
        "id": "KNIcYqwB0Som"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "\n",
        "punct_model = PunctuationModel()\n",
        "words_list = list(map(lambda x: x['word'], wsm))\n",
        "\n",
        "labled_words = punct_model.predict(words_list)"
      ],
      "metadata": {
        "id": "86jaQUEa0r7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper already punctuates the text in most of the case, so we'll give priority\n",
        "# to its puntuation marks over PunctuationModel results.\n",
        "import re\n",
        "\n",
        "ending_puncts = '.?!'\n",
        "model_puncts = '.,;:!?'\n",
        "\n",
        "# We don't want to punctuate U.S.A. with a period. Right?\n",
        "is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "    word = word_dict['word']\n",
        "    if word and labeled_tuple[1] in ending_puncts and (word[-1] not in model_puncts or is_acronym(word)):\n",
        "        word += labeled_tuple[1]\n",
        "        if word.endswith('..'): word = word.rstrip('.')\n",
        "        word_dict['word'] = word"
      ],
      "metadata": {
        "id": "M_i_C1dVFp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realignment using N-Gram/Masked Language Models\n",
        "\n",
        "In previous cells, we were able to get (word, timestamps, speaker_label) tuples which are also properly punctuated. Now we are in position to build a diarized transcription. Something like following:\n",
        "\n",
        "```\n",
        "Speaker A: Hey What's up?\n",
        "Speaker B: Yeah everything is good. How about you?\n",
        ".....\n",
        "```\n",
        "\n",
        "Wihtout doing any pre-processing, we'll see following results more than often.\n",
        "\n",
        "```\n",
        "Speaker A: It's got to come from somewhere else. Yeah, that one's also fun because you know the lows are\n",
        "Speaker B: going to suck, right? So it's actually it hits you on both sides.\n",
        ".....\n",
        "```\n",
        "\n",
        "Or something like:\n",
        "\n",
        "```\n",
        "Speaker A: That was an amazon podcast. Oh\n",
        "Speaker B: yeah, I really enjoyed it.\n",
        ".....\n",
        "```\n",
        "\n",
        "So you will see a splitted sentence on change of speaker. This could happen due to multiple reasons e.g. Overlapped speech, background noise or faulty timestamps (either from ASR or Diarization model)\n",
        "\n",
        "There are couple of ways to fix that. One simple way is to use an n-gram LM (KenLM) or a masked LM to calculate sentence perplexity (Log probability). To do that, we'll get last few words of Speaker A utterance and first few words of Speaker B utterences, add `\\n` at different places and calculate the sentence perplexity. Optimal position of `\\n` would be where probability is maximum. Something like following:\n",
        "\n",
        "```\n",
        "Speaker A: That was an amazon\n",
        "Speaker B: podcast. Oh yeah, I really enjoyed it.\n",
        "perplexity_score: 1\n",
        "\n",
        "Speaker A: That was an amazon podcast.\n",
        "Speaker B: Oh yeah, I really enjoyed it.\n",
        "perplexity_score: 5\n",
        "\n",
        "Speaker A: That was an amazon podcast. Oh\n",
        "Speaker B: yeah, I really enjoyed it.\n",
        "perplexity_score: 3\n",
        "\n",
        "Speaker A: That was an amazon podcast. Oh yeah.\n",
        "Speaker B: I really enjoyed it.\n",
        "perplexity_score: 4\n",
        "\n",
        "Speaker A: That was an amazon podcast. Oh yeah. I\n",
        "Speaker B: really enjoyed it.\n",
        "perplexity_score: 1\n",
        ".....\n",
        "```\n",
        "\n",
        "I tried this approach on multiple examples but results were not consistent.\n"
      ],
      "metadata": {
        "id": "dL0fbRl-Gkfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realignment using Punctuations\n",
        "\n",
        "Another simple method (You can call it a hack) is to using puctuation markings. If a sentence is splitted between two different speakers, it can either be part of Speaker A utterance or Speaker B utterance. We cans simply take mode of speaker labels and use that speaker label for whole sentence. Something like following:\n",
        "\n",
        "```\n",
        "Speaker A: It's got to come from somewhere else. Yeah, that one's also fun because you know the lows are\n",
        "Speaker B: going to suck, right? So it's actually it hits you on both sides.\n",
        "```\n",
        "\n",
        "Speaker A has more part of the sentence `Yeah, that one's also ..... right?` so this whole sentence should be part of Speaker A utterance.\n",
        "\n",
        "This can gets interesting when one speaker is saying some sort of monologue and other is doing `Hmm`, `Yeah`, `Exactly` in background. Whisper will tend to ignore that but Diarization Model (Nemo) will pick it up. So you may get inconsistent outputs like following:\n",
        "\n",
        "```\n",
        "Speaker A: It's got to come from somewhere\n",
        "Speaker B: else. Yeah,\n",
        "Speaker A: that one's also fun because you know the\n",
        "Speaker C: lows are\n",
        "Speaker A: going to suck, right?\n",
        "```\n",
        "\n",
        "So only Speaker A was speaking. Other's `hmm`, `yeah` were ignored by Whisper but because Diarization model was able to give us timestamps for them, we saw this inconsistent output.\n",
        "\n",
        "Fortunately, our punctuation based realignment will fix that.\n",
        "\n",
        "There are some special cases which are hard to fix i.e. when we have equal part of both sentences splitted between two Speakers. Something like\n",
        "\n",
        "```\n",
        "Speaker A: Oh\n",
        "Speaker B: Yeah.\n",
        "Speaker A: Yeah\n",
        "Speaker B: Exactly.\n",
        "```\n",
        "\n",
        "Unfortunately these can't be fixed and we have to accept them as part of diarization error. Our realignment code will assign them to either of the speaker."
      ],
      "metadata": {
        "id": "seaql-exMRsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_ending_punctuations = '.?!'\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "  is_word_sentence_end = lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "  left_idx = word_idx\n",
        "  while (left_idx > 0 and word_idx - left_idx < max_words and\n",
        "          speaker_list[left_idx - 1] == speaker_list[left_idx] and\n",
        "          not is_word_sentence_end(left_idx - 1)):\n",
        "      left_idx -= 1\n",
        "\n",
        "  return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "  is_word_sentence_end = lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "  right_idx = word_idx\n",
        "  while (right_idx < len(word_list) and right_idx - word_idx < max_words and\n",
        "          not is_word_sentence_end(right_idx)):\n",
        "      right_idx += 1\n",
        "\n",
        "  return right_idx if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx) else -1\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(word_speaker_mapping, max_words_in_sentence = 50):\n",
        "  is_word_sentence_end = lambda x: x >= 0 and word_speaker_mapping[x]['word'][-1] in sentence_ending_punctuations\n",
        "  wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "  words_list, speaker_list = [], []\n",
        "  for k, line_dict in enumerate(word_speaker_mapping):\n",
        "      word, speaker = line_dict['word'], line_dict['speaker']\n",
        "      words_list.append(word)\n",
        "      speaker_list.append(speaker)\n",
        "\n",
        "  k = 0\n",
        "  while k < len(word_speaker_mapping):\n",
        "      line_dict = word_speaker_mapping[k]\n",
        "      if k < wsp_len - 1 and speaker_list[k] != speaker_list[k + 1] and not is_word_sentence_end(k):\n",
        "          left_idx = get_first_word_idx_of_sentence(k, words_list, speaker_list, max_words_in_sentence)\n",
        "          right_idx = get_last_word_idx_of_sentence(k, words_list, max_words_in_sentence - k + left_idx - 1) if left_idx > -1 else -1\n",
        "          if min(left_idx, right_idx) == -1:\n",
        "              k += 1\n",
        "              continue\n",
        "\n",
        "          spk_labels = speaker_list[left_idx: right_idx + 1]\n",
        "          mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "          if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "              k += 1\n",
        "              continue\n",
        "\n",
        "          speaker_list[left_idx: right_idx + 1] = [mod_speaker] * (right_idx - left_idx + 1)\n",
        "          k = right_idx\n",
        "\n",
        "      k += 1\n",
        "\n",
        "  k, realigned_list = 0, []\n",
        "  while k < len(word_speaker_mapping):\n",
        "      line_dict = word_speaker_mapping[k].copy()\n",
        "      line_dict['speaker'] = speaker_list[k]\n",
        "      realigned_list.append(line_dict)\n",
        "      k += 1\n",
        "\n",
        "\n",
        "  return realigned_list"
      ],
      "metadata": {
        "id": "E_J0PR1QQHfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "  s, e, spk = spk_ts[0]\n",
        "  prev_spk = spk\n",
        "\n",
        "  snts = []\n",
        "  snt = {'speaker': f'Speaker {spk}', 'start_time': s, 'end_time': e, 'text': ''}\n",
        "\n",
        "  for wrd_dict in word_speaker_mapping:\n",
        "      wrd, spk = wrd_dict['word'], wrd_dict['speaker']\n",
        "      s, e = wrd_dict['start_time'], wrd_dict['end_time']\n",
        "      if spk != prev_spk:\n",
        "          snts.append(snt)\n",
        "          snt = {'speaker': f'Speaker {spk}', 'start_time': s, 'end_time': e, 'text': ''}\n",
        "      else:\n",
        "          snt['end_time'] = e\n",
        "      snt['text'] += wrd + ' '\n",
        "      prev_spk = spk\n",
        "\n",
        "  snts.append(snt)\n",
        "  return snts\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping):\n",
        "  with open('diarization.txt', 'w') as f:\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        sp = sentence_dict['speaker']\n",
        "        text = sentence_dict['text']\n",
        "        f.write(f'\\n\\n{sp}: {text}')"
      ],
      "metadata": {
        "id": "Qv3IJ1FRzhHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
        "get_speaker_aware_transcript(ssm)"
      ],
      "metadata": {
        "id": "h3mk2fN8z9c-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}