{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/Auto_Making/API_13_1-HUG-free-llm-GameTheory_All_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AcaGPT MVP\n",
        "\n",
        "The gosl of this project is Creating something like AutoGPT but in the field of Academic Papers wrting.\n",
        "\n",
        "\n",
        "For example the MVP Lage has been made by this AI free website maker :\n",
        "\n",
        "https://acagpt.site.live/editv2/"
      ],
      "metadata": {
        "id": "qnaAEEnpFyLs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHgZHeIxi31Q"
      },
      "source": [
        "## Aromatically wrrie apaper by the ChatGPT and this github prompt instruction:\n",
        "\n",
        "https://github.com/ahmetbersoz/chatgpt-prompts-for-academic-writing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "global gdrive_fpath\n",
        "drive_mounted = False\n",
        "gdrive_fpath = '.'\n",
        "local_path = '/content/'\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "if mount_gdrive : # and not drive_mounted:\n",
        "    from google.colab import drive\n",
        "\n",
        "    gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "    gdrive_subdirectory = 'MyDrive/ChatGPT_Paper_wrting' #@param{type:\"string\"}\n",
        "    gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "    print (\"gdrive path is :\",gdrive_fpath)\n",
        "   # Mount Google Drive\n",
        "    if not os.path.isdir(gdrive_mountpoint):\n",
        "     # If not, mount the drive\n",
        "       drive.mount(gdrive_mountpoint)\n",
        "       if not os.path.exists(gdrive_fpath):\n",
        "          os.makedirs(gdrive_fpath)\n",
        "          os.chdir(gdrive_fpath)\n",
        "    else:\n",
        "          print(\"Drive is already mounted.\")\n",
        "else:\n",
        "   Folder_fpath ='/content/' #@param{type:\"string\"}\n",
        "   #gdrive_subdirectory = 'MyDrive/ChatGPT_Paper_wrting' #@param{type:\"string\"}\n",
        "   gdrive_fpath = Folder_fpath\n",
        "   os.chdir(gdrive_fpath)\n",
        "folder_path = gdrive_fpath"
      ],
      "metadata": {
        "id": "6GjtEabAJCXo",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff05ce9f-f255-4ec5-8e1f-c207bcb11284"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive path is : /content/drive/MyDrive/ChatGPT_Paper_wrting\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall openai -y\n",
        "#!pip install openai # ==0.28"
      ],
      "metadata": {
        "id": "DIUEYU4EJW6w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vt7VN_fmGT3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd281c61-aee2-4e61-9114-a6482971f5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Downloading openai-1.45.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.45.0\n",
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n",
            "Collecting docx2pdf\n",
            "  Downloading docx2pdf-0.1.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from docx2pdf) (4.66.5)\n",
            "Downloading docx2pdf-0.1.8-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: docx2pdf\n",
            "Successfully installed docx2pdf-0.1.8\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Collecting django\n",
            "  Downloading Django-5.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting asgiref<4,>=3.8.1 (from django)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from django) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from asgiref<4,>=3.8.1->django) (4.12.2)\n",
            "Downloading Django-5.1.1-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: asgiref, django\n",
            "Successfully installed asgiref-3.8.1 django-5.1.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, ffmpy, aiofiles, starlette, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.114.2 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.5 semantic-version-2.10.0 starlette-0.38.5 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n",
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n",
            "35.236.181.115\n",
            "Collecting mega.py\n",
            "  Downloading mega.py-1.0.8-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: requests>=0.10 in /usr/local/lib/python3.10/dist-packages (from mega.py) (2.32.3)\n",
            "Collecting pycryptodome<4.0.0,>=3.9.6 (from mega.py)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.10/dist-packages (from mega.py) (1.0.1)\n",
            "Collecting tenacity<6.0.0,>=5.1.5 (from mega.py)\n",
            "  Downloading tenacity-5.1.5-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from tenacity<6.0.0,>=5.1.5->mega.py) (1.16.0)\n",
            "Downloading mega.py-1.0.8-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-5.1.5-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: tenacity, pycryptodome, mega.py\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotly 5.15.0 requires tenacity>=6.2.0, but you have tenacity 5.1.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mega.py-1.0.8 pycryptodome-3.20.0 tenacity-5.1.5\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Collecting httpcore==0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h11<0.13,>=0.11 (from httpcore==0.15.0)\n",
            "  Downloading h11-0.12.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: sniffio==1.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (1.3.1)\n",
            "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (2024.8.30)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio==3.*->httpcore==0.15.0) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio==3.*->httpcore==0.15.0) (1.2.2)\n",
            "INFO: pip is looking at multiple versions of httpx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting hstspreload (from httpx)\n",
            "  Downloading hstspreload-2024.9.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting chardet==3.* (from httpx)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna>=2.8 (from anyio==3.*->httpcore==0.15.0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-2.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from googletrans) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->googletrans) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->googletrans) (2.0.7)\n",
            "Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-py3-none-any.whl size=15761 sha256=03d8de3951ab615962029541f0920e5130d6df3b3564da7b9a636037c664fcc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/5f/60/c4738a8b36085696062052befbbfb65fc94d2286fb17015856\n",
            "Successfully built googletrans\n",
            "Installing collected packages: h11, dnspython, pymongo, httpcore, googletrans, httpx\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.5\n",
            "    Uninstalling httpcore-1.0.5:\n",
            "      Successfully uninstalled httpcore-1.0.5\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "Successfully installed dnspython-2.6.1 googletrans-2.4.0 h11-0.12.0 httpcore-0.15.0 httpx-0.25.1 pymongo-4.8.0\n",
            "Collecting httpx==0.24.1\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.1) (2024.8.30)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.1) (0.15.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.1) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.24.1) (1.3.1)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1) (0.12.0)\n",
            "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio==3.*->httpcore<0.18.0,>=0.15.0->httpx==0.24.1) (1.2.2)\n",
            "Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.25.1\n",
            "    Uninstalling httpx-0.25.1:\n",
            "      Successfully uninstalled httpx-0.25.1\n",
            "Successfully installed httpx-0.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai #==0.28\n",
        "!pip install backoff\n",
        "!pip install docx2pdf\n",
        "!pip install python-docx\n",
        "!pip install django\n",
        "!pip install tensorflow\n",
        "\n",
        "!pip install tenacity\n",
        "!pip install tiktoken\n",
        "!pip install gradio\n",
        "# SETUP COLAB for run Streamlit\n",
        "!npm install localtunnel\n",
        "\n",
        "!pip install -q dl-translate\n",
        "!curl ipv4.icanhazip.com\n",
        "\n",
        "#!pip install youtube-dl\n",
        "#!apt install python3-distutils\n",
        "!pip install mega.py\n",
        "!apt install python3-distutils  -y\n",
        "\n",
        "#!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "!pip install httpcore==0.15.0 httpx pymongo googletrans\n",
        "!pip install httpx==0.24.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install libreoffice\n",
        "#!sudo apt-get install abiword\n",
        "!sudo apt install libreoffice-writer"
      ],
      "metadata": {
        "id": "AUeIF57v21gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0168b9-ffa8-461b-ccf7-54b4a0329be8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,030 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,576 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,267 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,150 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,108 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,311 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,439 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,541 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
            "Fetched 24.8 MB in 5s (4,698 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor default-jre default-jre-headless dictionaries-common\n",
            "  firebird3.0-common firebird3.0-common-doc firebird3.0-server-core\n",
            "  firebird3.0-utils fonts-crosextra-caladea fonts-crosextra-carlito\n",
            "  fonts-dejavu fonts-dejavu-core fonts-dejavu-extra fonts-liberation2\n",
            "  fonts-linuxlibertine fonts-noto-core fonts-noto-extra fonts-noto-mono\n",
            "  fonts-noto-ui-core fonts-opensymbol fonts-sil-gentium\n",
            "  fonts-sil-gentium-basic gstreamer1.0-gl gstreamer1.0-gtk3 hunspell-en-us\n",
            "  libabsl20210324 libabw-0.1-1 libatk-wrapper-java libatk-wrapper-java-jni\n",
            "  libbsh-java libcdr-0.1-1 libclucene-contribs1v5 libclucene-core1v5\n",
            "  libcolamd2 libe-book-0.1-1 libel-api-java libeot0 libepubgen-0.1-1\n",
            "  libetonyek-0.1-1 libexttextcat-2.0-0 libexttextcat-data libfbclient2\n",
            "  libfontenc1 libfreehand-0.1-1 libgpgme11 libgpgmepp6 libgraphene-1.0-0\n",
            "  libgstreamer-gl1.0-0 libgudev-1.0-0 libharfbuzz-icu0 libhsqldb1.8.0-java\n",
            "  libhunspell-1.7-0 libhyphen0 libib-util libjsp-api-java liblangtag-common\n",
            "  liblangtag1 liblibreoffice-java libmhash2 libmspub-0.1-1 libmwaw-0.3-3\n",
            "  libmythes-1.2-0 libodfgen-0.1-1 liborcus-0.17-0 liborcus-parser-0.17-0\n",
            "  libpagemaker-0.0-0 libraptor2-0 librasqal3 librdf0 libreoffice-base\n",
            "  libreoffice-base-core libreoffice-base-drivers libreoffice-calc\n",
            "  libreoffice-common libreoffice-core libreoffice-draw libreoffice-gnome\n",
            "  libreoffice-gtk3 libreoffice-impress libreoffice-java-common\n",
            "  libreoffice-math libreoffice-nlpsolver libreoffice-report-builder\n",
            "  libreoffice-report-builder-bin libreoffice-script-provider-bsh\n",
            "  libreoffice-script-provider-js libreoffice-script-provider-python\n",
            "  libreoffice-sdbc-firebird libreoffice-sdbc-hsqldb libreoffice-sdbc-mysql\n",
            "  libreoffice-sdbc-postgresql libreoffice-style-colibre\n",
            "  libreoffice-style-elementary libreoffice-style-yaru\n",
            "  libreoffice-wiki-publisher libreoffice-writer librevenge-0.0-0\n",
            "  libservlet-api-java libservlet3.1-java libsuitesparseconfig5\n",
            "  libtext-iconv-perl libtommath1 libuno-cppu3 libuno-cppuhelpergcc3-3\n",
            "  libuno-purpenvhelpergcc3-3 libuno-sal3 libuno-salhelpergcc3-3\n",
            "  libunoloader-java libvisio-0.1-1 libwebsocket-api-java libwpd-0.10-10\n",
            "  libwpg-0.3-3 libwps-0.4-4 libxkbfile1 libxmlsec1 libxmlsec1-nss libxtst6\n",
            "  libxxf86dga1 libyajl2 lp-solve openjdk-11-jre poppler-data python3-uno\n",
            "  uno-libs-private ure ure-java x11-utils\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils ispell | aspell | hunspell wordlist\n",
            "  firebird3.0-server firebird3.0-doc hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core java-virtual-machine libhsqldb1.8.0-java-gcj\n",
            "  raptor2-utils rasqal-utils librdf-storage-postgresql librdf-storage-mysql\n",
            "  librdf-storage-sqlite librdf-storage-virtuoso redland-utils cups-bsd firefox\n",
            "  | firefox-esr | thunderbird ghostscript gpa hyphen-hyphenation-patterns\n",
            "  imagemagick | graphicsmagick-imagemagick-compat libreoffice-grammarcheck\n",
            "  libreoffice-help libreoffice-l10n libreoffice-librelogo myspell-dictionary\n",
            "  mythes-thesaurus openclipart-libreoffice pstoedit unixodbc\n",
            "  gstreamer1.0-plugins-base gstreamer1.0-plugins-good\n",
            "  gstreamer1.0-plugins-ugly gstreamer1.0-plugins-bad gstreamer1.0-libav\n",
            "  libsane1 libofficebean-java libjtds-java libsqliteodbc | tdsodbc\n",
            "  | odbc-mdbtools libreoffice-evolution seahorse libreofficekit-data bluez\n",
            "  default-mysql-server | virtual-mysql-server postgresql mediawiki\n",
            "  poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  apparmor default-jre default-jre-headless dictionaries-common\n",
            "  firebird3.0-common firebird3.0-common-doc firebird3.0-server-core\n",
            "  firebird3.0-utils fonts-crosextra-caladea fonts-crosextra-carlito\n",
            "  fonts-dejavu fonts-dejavu-core fonts-dejavu-extra fonts-liberation2\n",
            "  fonts-linuxlibertine fonts-noto-core fonts-noto-extra fonts-noto-mono\n",
            "  fonts-noto-ui-core fonts-opensymbol fonts-sil-gentium\n",
            "  fonts-sil-gentium-basic gstreamer1.0-gl gstreamer1.0-gtk3 hunspell-en-us\n",
            "  libabsl20210324 libabw-0.1-1 libatk-wrapper-java libatk-wrapper-java-jni\n",
            "  libbsh-java libcdr-0.1-1 libclucene-contribs1v5 libclucene-core1v5\n",
            "  libcolamd2 libe-book-0.1-1 libel-api-java libeot0 libepubgen-0.1-1\n",
            "  libetonyek-0.1-1 libexttextcat-2.0-0 libexttextcat-data libfbclient2\n",
            "  libfontenc1 libfreehand-0.1-1 libgpgme11 libgpgmepp6 libgraphene-1.0-0\n",
            "  libgstreamer-gl1.0-0 libgudev-1.0-0 libharfbuzz-icu0 libhsqldb1.8.0-java\n",
            "  libhunspell-1.7-0 libhyphen0 libib-util libjsp-api-java liblangtag-common\n",
            "  liblangtag1 liblibreoffice-java libmhash2 libmspub-0.1-1 libmwaw-0.3-3\n",
            "  libmythes-1.2-0 libodfgen-0.1-1 liborcus-0.17-0 liborcus-parser-0.17-0\n",
            "  libpagemaker-0.0-0 libraptor2-0 librasqal3 librdf0 libreoffice\n",
            "  libreoffice-base libreoffice-base-core libreoffice-base-drivers\n",
            "  libreoffice-calc libreoffice-common libreoffice-core libreoffice-draw\n",
            "  libreoffice-gnome libreoffice-gtk3 libreoffice-impress\n",
            "  libreoffice-java-common libreoffice-math libreoffice-nlpsolver\n",
            "  libreoffice-report-builder libreoffice-report-builder-bin\n",
            "  libreoffice-script-provider-bsh libreoffice-script-provider-js\n",
            "  libreoffice-script-provider-python libreoffice-sdbc-firebird\n",
            "  libreoffice-sdbc-hsqldb libreoffice-sdbc-mysql libreoffice-sdbc-postgresql\n",
            "  libreoffice-style-colibre libreoffice-style-elementary\n",
            "  libreoffice-style-yaru libreoffice-wiki-publisher libreoffice-writer\n",
            "  librevenge-0.0-0 libservlet-api-java libservlet3.1-java\n",
            "  libsuitesparseconfig5 libtext-iconv-perl libtommath1 libuno-cppu3\n",
            "  libuno-cppuhelpergcc3-3 libuno-purpenvhelpergcc3-3 libuno-sal3\n",
            "  libuno-salhelpergcc3-3 libunoloader-java libvisio-0.1-1\n",
            "  libwebsocket-api-java libwpd-0.10-10 libwpg-0.3-3 libwps-0.4-4 libxkbfile1\n",
            "  libxmlsec1 libxmlsec1-nss libxtst6 libxxf86dga1 libyajl2 lp-solve\n",
            "  openjdk-11-jre poppler-data python3-uno uno-libs-private ure ure-java\n",
            "  x11-utils\n",
            "0 upgraded, 128 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 227 MB of archives.\n",
            "After this operation, 855 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-opensymbol all 2:102.12+LibO7.3.7-0ubuntu0.22.04.6 [103 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-style-colibre all 1:7.3.7-0ubuntu0.22.04.6 [1,294 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuno-sal3 amd64 1:7.3.7-0ubuntu0.22.04.6 [196 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuno-salhelpergcc3-3 amd64 1:7.3.7-0ubuntu0.22.04.6 [17.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuno-cppu3 amd64 1:7.3.7-0ubuntu0.22.04.6 [87.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 uno-libs-private amd64 1:7.3.7-0ubuntu0.22.04.6 [232 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblangtag-common all 0.6.3-2ubuntu1 [193 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblangtag1 amd64 0.6.3-2ubuntu1 [53.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuno-cppuhelpergcc3-3 amd64 1:7.3.7-0ubuntu0.22.04.6 [343 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuno-purpenvhelpergcc3-3 amd64 1:7.3.7-0ubuntu0.22.04.6 [15.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ure amd64 1:7.3.7-0ubuntu0.22.04.6 [1,312 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-common all 1:7.3.7-0ubuntu0.22.04.6 [23.4 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libabsl20210324 amd64 0~20210324.2-2 [387 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libclucene-core1v5 amd64 2.3.3.4+dfsg-1ubuntu5 [530 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libclucene-contribs1v5 amd64 2.3.3.4+dfsg-1ubuntu5 [96.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libeot0 amd64 0.01-5build2 [28.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libexttextcat-data all 3.4.5-1build2 [179 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libexttextcat-2.0-0 amd64 3.4.5-1build2 [13.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgpgme11 amd64 1.16.0-1.2ubuntu4.2 [136 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgpgmepp6 amd64 1.16.0-1.2ubuntu4.2 [109 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz-icu0 amd64 2.7.4-1ubuntu3.1 [5,886 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhyphen0 amd64 2.8.8-7build2 [28.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmythes-1.2-0 amd64 2:1.2.4-4build1 [9,352 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 liborcus-parser-0.17-0 amd64 0.17.2-2 [107 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 liborcus-0.17-0 amd64 0.17.2-2 [393 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libyajl2 amd64 2.1.0-3ubuntu0.22.04.1 [21.0 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraptor2-0 amd64 2.0.15-0ubuntu4 [172 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmhash2 amd64 0.9.9.9-9build2 [95.9 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 librasqal3 amd64 0.9.33-0.2ubuntu1 [193 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 librdf0 amd64 1.0.17-1.1ubuntu3 [106 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 librevenge-0.0-0 amd64 0.0.4-6ubuntu7 [209 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmlsec1 amd64 1.2.33-1build2 [139 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmlsec1-nss amd64 1.2.33-1build2 [67.7 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-core amd64 1:7.3.7-0ubuntu0.22.04.6 [40.4 MB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-base-core amd64 1:7.3.7-0ubuntu0.22.04.6 [974 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-base-drivers amd64 1:7.3.7-0ubuntu0.22.04.6 [602 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-base amd64 1:7.3.7-0ubuntu0.22.04.6 [1,725 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3build2 [595 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.24+8-1ubuntu3~22.04 [214 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 firebird3.0-common-doc all 3.0.8.33535.ds4-1ubuntu2 [26.8 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 firebird3.0-common all 3.0.8.33535.ds4-1ubuntu2 [15.5 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtommath1 amd64 1.2.0-6ubuntu0.22.04.1 [56.5 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfbclient2 amd64 3.0.8.33535.ds4-1ubuntu2 [512 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libib-util amd64 3.0.8.33535.ds4-1ubuntu2 [3,378 B]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 firebird3.0-server-core amd64 3.0.8.33535.ds4-1ubuntu2 [2,533 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 firebird3.0-utils amd64 3.0.8.33535.ds4-1ubuntu2 [872 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-crosextra-caladea all 20130214-2.1 [82.4 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-crosextra-carlito all 20130920-1.1 [743 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-dejavu all 2.37-2build1 [3,192 B]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-liberation2 all 2.1.5-1 [1,614 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-linuxlibertine all 5.3.0-6 [1,627 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-core all 20201225-1build1 [12.2 MB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-noto-extra all 20201225-1build1 [72.4 MB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-ui-core all 20201225-1build1 [1,420 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentium all 20081126:1.03-4 [245 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-sil-gentium-basic all 1.102-1.1 [384 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgraphene-1.0-0 amd64 1.10.8-1 [48.2 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgstreamer-gl1.0-0 amd64 1.20.1-1ubuntu0.2 [204 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gstreamer1.0-gl amd64 1.20.1-1ubuntu0.2 [125 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gstreamer1.0-gtk3 amd64 1.20.3-0ubuntu1.1 [33.2 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/main amd64 libabw-0.1-1 amd64 0.1.3-1build3 [102 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbsh-java all 2.0b4-20 [289 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdr-0.1-1 amd64 0.1.6-2build2 [392 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsuitesparseconfig5 amd64 1:5.10.1+dfsg-4build1 [10.4 kB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcolamd2 amd64 1:5.10.1+dfsg-4build1 [18.0 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/main amd64 libe-book-0.1-1 amd64 0.1.3-2build2 [148 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libel-api-java all 3.0.0-3 [64.9 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 libepubgen-0.1-1 amd64 0.1.1-1ubuntu5 [120 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/main amd64 libetonyek-0.1-1 amd64 0.1.10-3build1 [637 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfreehand-0.1-1 amd64 0.1.2-3build2 [272 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libservlet-api-java all 4.0.1-2 [81.0 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjsp-api-java all 2.3.4-3 [53.7 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwebsocket-api-java all 1.1-2 [40.1 kB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libservlet3.1-java all 1:4.0.1-2 [9,276 B]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhsqldb1.8.0-java all 1.8.0.10+dfsg-11 [765 kB]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libunoloader-java all 1:7.3.7-0ubuntu0.22.04.6 [12.9 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ure-java amd64 1:7.3.7-0ubuntu0.22.04.6 [83.5 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 liblibreoffice-java all 1:7.3.7-0ubuntu0.22.04.6 [1,604 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmspub-0.1-1 amd64 0.1.4-3build3 [144 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmwaw-0.3-3 amd64 0.3.21-1build1 [2,375 kB]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu jammy/main amd64 libodfgen-0.1-1 amd64 0.1.8-2build2 [245 kB]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpagemaker-0.0-0 amd64 0.0.4-1build3 [55.9 kB]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu jammy/main amd64 lp-solve amd64 5.5.2.5-2build2 [315 kB]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwps-0.4-4 amd64 0.4.12-2build1 [812 kB]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-calc amd64 1:7.3.7-0ubuntu0.22.04.6 [8,426 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvisio-0.1-1 amd64 0.1.7-1build5 [238 kB]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwpd-0.10-10 amd64 0.10.3-2build1 [209 kB]\n",
            "Get:106 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwpg-0.3-3 amd64 0.3.3-1build3 [49.9 kB]\n",
            "Get:107 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-draw amd64 1:7.3.7-0ubuntu0.22.04.6 [3,228 kB]\n",
            "Get:108 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-impress amd64 1:7.3.7-0ubuntu0.22.04.6 [1,359 kB]\n",
            "Get:109 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-math amd64 1:7.3.7-0ubuntu0.22.04.6 [597 kB]\n",
            "Get:110 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-report-builder-bin amd64 1:7.3.7-0ubuntu0.22.04.6 [971 kB]\n",
            "Get:111 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-writer amd64 1:7.3.7-0ubuntu0.22.04.6 [10.3 MB]\n",
            "Get:112 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-uno amd64 1:7.3.7-0ubuntu0.22.04.6 [148 kB]\n",
            "Get:113 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice amd64 1:7.3.7-0ubuntu0.22.04.6 [12.9 kB]\n",
            "Get:114 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-gnome amd64 1:7.3.7-0ubuntu0.22.04.6 [72.9 kB]\n",
            "Get:115 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-gtk3 amd64 1:7.3.7-0ubuntu0.22.04.6 [509 kB]\n",
            "Get:116 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-java-common all 1:7.3.7-0ubuntu0.22.04.6 [621 kB]\n",
            "Get:117 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-report-builder all 1:7.3.7-0ubuntu0.22.04.6 [2,112 kB]\n",
            "Get:118 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-script-provider-bsh all 1:7.3.7-0ubuntu0.22.04.6 [42.3 kB]\n",
            "Get:119 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-script-provider-js all 1:7.3.7-0ubuntu0.22.04.6 [645 kB]\n",
            "Get:120 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-script-provider-python all 1:7.3.7-0ubuntu0.22.04.6 [16.5 kB]\n",
            "Get:121 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-sdbc-firebird amd64 1:7.3.7-0ubuntu0.22.04.6 [176 kB]\n",
            "Get:122 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-sdbc-hsqldb amd64 1:7.3.7-0ubuntu0.22.04.6 [135 kB]\n",
            "Get:123 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-sdbc-mysql amd64 1:7.3.7-0ubuntu0.22.04.6 [123 kB]\n",
            "Get:124 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-sdbc-postgresql amd64 1:7.3.7-0ubuntu0.22.04.6 [279 kB]\n",
            "Get:125 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-style-elementary all 1:7.3.7-0ubuntu0.22.04.6 [8,020 kB]\n",
            "Get:126 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-style-yaru all 1:7.3.7-0ubuntu0.22.04.6 [3,761 kB]\n",
            "Get:127 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libreoffice-wiki-publisher all 1.2.0+LibO7.3.7-0ubuntu0.22.04.6 [7,924 B]\n",
            "Get:128 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libreoffice-nlpsolver all 0.9+LibO7.3.7-0ubuntu0.22.04.6 [7,982 B]\n",
            "Fetched 227 MB in 40s (5,715 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 128.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-opensymbol.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../000-fonts-opensymbol_2%3a102.12+LibO7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking fonts-opensymbol (2:102.12+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-style-colibre.\n",
            "Preparing to unpack .../001-libreoffice-style-colibre_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-style-colibre (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libuno-sal3.\n",
            "Preparing to unpack .../002-libuno-sal3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libuno-sal3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libuno-salhelpergcc3-3.\n",
            "Preparing to unpack .../003-libuno-salhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libuno-salhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libuno-cppu3.\n",
            "Preparing to unpack .../004-libuno-cppu3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libuno-cppu3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package uno-libs-private.\n",
            "Preparing to unpack .../005-uno-libs-private_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking uno-libs-private (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package liblangtag-common.\n",
            "Preparing to unpack .../006-liblangtag-common_0.6.3-2ubuntu1_all.deb ...\n",
            "Unpacking liblangtag-common (0.6.3-2ubuntu1) ...\n",
            "Selecting previously unselected package liblangtag1:amd64.\n",
            "Preparing to unpack .../007-liblangtag1_0.6.3-2ubuntu1_amd64.deb ...\n",
            "Unpacking liblangtag1:amd64 (0.6.3-2ubuntu1) ...\n",
            "Selecting previously unselected package libuno-cppuhelpergcc3-3.\n",
            "Preparing to unpack .../008-libuno-cppuhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libuno-cppuhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libuno-purpenvhelpergcc3-3.\n",
            "Preparing to unpack .../009-libuno-purpenvhelpergcc3-3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libuno-purpenvhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package ure.\n",
            "Preparing to unpack .../010-ure_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking ure (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-common.\n",
            "Preparing to unpack .../011-libreoffice-common_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-common (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libabsl20210324:amd64.\n",
            "Preparing to unpack .../012-libabsl20210324_0~20210324.2-2_amd64.deb ...\n",
            "Unpacking libabsl20210324:amd64 (0~20210324.2-2) ...\n",
            "Selecting previously unselected package libclucene-core1v5:amd64.\n",
            "Preparing to unpack .../013-libclucene-core1v5_2.3.3.4+dfsg-1ubuntu5_amd64.deb ...\n",
            "Unpacking libclucene-core1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Selecting previously unselected package libclucene-contribs1v5:amd64.\n",
            "Preparing to unpack .../014-libclucene-contribs1v5_2.3.3.4+dfsg-1ubuntu5_amd64.deb ...\n",
            "Unpacking libclucene-contribs1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Selecting previously unselected package libeot0:amd64.\n",
            "Preparing to unpack .../015-libeot0_0.01-5build2_amd64.deb ...\n",
            "Unpacking libeot0:amd64 (0.01-5build2) ...\n",
            "Selecting previously unselected package libexttextcat-data.\n",
            "Preparing to unpack .../016-libexttextcat-data_3.4.5-1build2_all.deb ...\n",
            "Unpacking libexttextcat-data (3.4.5-1build2) ...\n",
            "Selecting previously unselected package libexttextcat-2.0-0:amd64.\n",
            "Preparing to unpack .../017-libexttextcat-2.0-0_3.4.5-1build2_amd64.deb ...\n",
            "Unpacking libexttextcat-2.0-0:amd64 (3.4.5-1build2) ...\n",
            "Selecting previously unselected package libgpgme11:amd64.\n",
            "Preparing to unpack .../018-libgpgme11_1.16.0-1.2ubuntu4.2_amd64.deb ...\n",
            "Unpacking libgpgme11:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Selecting previously unselected package libgpgmepp6:amd64.\n",
            "Preparing to unpack .../019-libgpgmepp6_1.16.0-1.2ubuntu4.2_amd64.deb ...\n",
            "Unpacking libgpgmepp6:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Selecting previously unselected package libharfbuzz-icu0:amd64.\n",
            "Preparing to unpack .../020-libharfbuzz-icu0_2.7.4-1ubuntu3.1_amd64.deb ...\n",
            "Unpacking libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.1) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../021-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libhyphen0:amd64.\n",
            "Preparing to unpack .../022-libhyphen0_2.8.8-7build2_amd64.deb ...\n",
            "Unpacking libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Selecting previously unselected package libmythes-1.2-0:amd64.\n",
            "Preparing to unpack .../023-libmythes-1.2-0_2%3a1.2.4-4build1_amd64.deb ...\n",
            "Unpacking libmythes-1.2-0:amd64 (2:1.2.4-4build1) ...\n",
            "Selecting previously unselected package liborcus-parser-0.17-0:amd64.\n",
            "Preparing to unpack .../024-liborcus-parser-0.17-0_0.17.2-2_amd64.deb ...\n",
            "Unpacking liborcus-parser-0.17-0:amd64 (0.17.2-2) ...\n",
            "Selecting previously unselected package liborcus-0.17-0:amd64.\n",
            "Preparing to unpack .../025-liborcus-0.17-0_0.17.2-2_amd64.deb ...\n",
            "Unpacking liborcus-0.17-0:amd64 (0.17.2-2) ...\n",
            "Selecting previously unselected package libyajl2:amd64.\n",
            "Preparing to unpack .../026-libyajl2_2.1.0-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libraptor2-0:amd64.\n",
            "Preparing to unpack .../027-libraptor2-0_2.0.15-0ubuntu4_amd64.deb ...\n",
            "Unpacking libraptor2-0:amd64 (2.0.15-0ubuntu4) ...\n",
            "Selecting previously unselected package libmhash2:amd64.\n",
            "Preparing to unpack .../028-libmhash2_0.9.9.9-9build2_amd64.deb ...\n",
            "Unpacking libmhash2:amd64 (0.9.9.9-9build2) ...\n",
            "Selecting previously unselected package librasqal3:amd64.\n",
            "Preparing to unpack .../029-librasqal3_0.9.33-0.2ubuntu1_amd64.deb ...\n",
            "Unpacking librasqal3:amd64 (0.9.33-0.2ubuntu1) ...\n",
            "Selecting previously unselected package librdf0:amd64.\n",
            "Preparing to unpack .../030-librdf0_1.0.17-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking librdf0:amd64 (1.0.17-1.1ubuntu3) ...\n",
            "Selecting previously unselected package librevenge-0.0-0:amd64.\n",
            "Preparing to unpack .../031-librevenge-0.0-0_0.0.4-6ubuntu7_amd64.deb ...\n",
            "Unpacking librevenge-0.0-0:amd64 (0.0.4-6ubuntu7) ...\n",
            "Selecting previously unselected package libxmlsec1:amd64.\n",
            "Preparing to unpack .../032-libxmlsec1_1.2.33-1build2_amd64.deb ...\n",
            "Unpacking libxmlsec1:amd64 (1.2.33-1build2) ...\n",
            "Selecting previously unselected package libxmlsec1-nss:amd64.\n",
            "Preparing to unpack .../033-libxmlsec1-nss_1.2.33-1build2_amd64.deb ...\n",
            "Unpacking libxmlsec1-nss:amd64 (1.2.33-1build2) ...\n",
            "Selecting previously unselected package libreoffice-core.\n",
            "Preparing to unpack .../034-libreoffice-core_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-core (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-base-core.\n",
            "Preparing to unpack .../035-libreoffice-base-core_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-base-core (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-base-drivers.\n",
            "Preparing to unpack .../036-libreoffice-base-drivers_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-base-drivers (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-base.\n",
            "Preparing to unpack .../037-libreoffice-base_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "No diversion 'diversion of /usr/lib/libreoffice/share/basic/dialog.xlc to /usr/lib/libreoffice/share/basic/dialog.xlc.noaccess by libreoffice-base', none removed.\n",
            "No diversion 'diversion of /usr/lib/libreoffice/share/basic/script.xlc to /usr/lib/libreoffice/share/basic/script.xlc.noaccess by libreoffice-base', none removed.\n",
            "Unpacking libreoffice-base (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../038-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "Preparing to unpack .../039-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package apparmor.\n",
            "Preparing to unpack .../040-apparmor_3.0.4-2ubuntu2.3build2_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3build2) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../041-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../042-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../043-openjdk-11-jre_11.0.24+8-1ubuntu3~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../044-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../045-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package firebird3.0-common-doc.\n",
            "Preparing to unpack .../046-firebird3.0-common-doc_3.0.8.33535.ds4-1ubuntu2_all.deb ...\n",
            "Unpacking firebird3.0-common-doc (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-common.\n",
            "Preparing to unpack .../047-firebird3.0-common_3.0.8.33535.ds4-1ubuntu2_all.deb ...\n",
            "Unpacking firebird3.0-common (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package libtommath1:amd64.\n",
            "Preparing to unpack .../048-libtommath1_1.2.0-6ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libtommath1:amd64 (1.2.0-6ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libfbclient2:amd64.\n",
            "Preparing to unpack .../049-libfbclient2_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking libfbclient2:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package libib-util:amd64.\n",
            "Preparing to unpack .../050-libib-util_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking libib-util:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-server-core:amd64.\n",
            "Preparing to unpack .../051-firebird3.0-server-core_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking firebird3.0-server-core:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package firebird3.0-utils.\n",
            "Preparing to unpack .../052-firebird3.0-utils_3.0.8.33535.ds4-1ubuntu2_amd64.deb ...\n",
            "Unpacking firebird3.0-utils (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Selecting previously unselected package fonts-crosextra-caladea.\n",
            "Preparing to unpack .../053-fonts-crosextra-caladea_20130214-2.1_all.deb ...\n",
            "Unpacking fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Selecting previously unselected package fonts-crosextra-carlito.\n",
            "Preparing to unpack .../054-fonts-crosextra-carlito_20130920-1.1_all.deb ...\n",
            "Unpacking fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../055-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../056-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu.\n",
            "Preparing to unpack .../057-fonts-dejavu_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-liberation2.\n",
            "Preparing to unpack .../058-fonts-liberation2_2.1.5-1_all.deb ...\n",
            "Unpacking fonts-liberation2 (2.1.5-1) ...\n",
            "Selecting previously unselected package fonts-linuxlibertine.\n",
            "Preparing to unpack .../059-fonts-linuxlibertine_5.3.0-6_all.deb ...\n",
            "Unpacking fonts-linuxlibertine (5.3.0-6) ...\n",
            "Selecting previously unselected package fonts-noto-core.\n",
            "Preparing to unpack .../060-fonts-noto-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-extra.\n",
            "Preparing to unpack .../061-fonts-noto-extra_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-extra (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../062-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-ui-core.\n",
            "Preparing to unpack .../063-fonts-noto-ui-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-ui-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-sil-gentium.\n",
            "Preparing to unpack .../064-fonts-sil-gentium_20081126%3a1.03-4_all.deb ...\n",
            "Unpacking fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Selecting previously unselected package fonts-sil-gentium-basic.\n",
            "Preparing to unpack .../065-fonts-sil-gentium-basic_1.102-1.1_all.deb ...\n",
            "Unpacking fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Selecting previously unselected package libgraphene-1.0-0:amd64.\n",
            "Preparing to unpack .../066-libgraphene-1.0-0_1.10.8-1_amd64.deb ...\n",
            "Unpacking libgraphene-1.0-0:amd64 (1.10.8-1) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../067-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libgstreamer-gl1.0-0:amd64.\n",
            "Preparing to unpack .../068-libgstreamer-gl1.0-0_1.20.1-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.2) ...\n",
            "Selecting previously unselected package gstreamer1.0-gl:amd64.\n",
            "Preparing to unpack .../069-gstreamer1.0-gl_1.20.1-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking gstreamer1.0-gl:amd64 (1.20.1-1ubuntu0.2) ...\n",
            "Selecting previously unselected package gstreamer1.0-gtk3:amd64.\n",
            "Preparing to unpack .../070-gstreamer1.0-gtk3_1.20.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking gstreamer1.0-gtk3:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../071-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libabw-0.1-1:amd64.\n",
            "Preparing to unpack .../072-libabw-0.1-1_0.1.3-1build3_amd64.deb ...\n",
            "Unpacking libabw-0.1-1:amd64 (0.1.3-1build3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../073-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../074-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../075-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../076-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../077-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../078-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libbsh-java.\n",
            "Preparing to unpack .../079-libbsh-java_2.0b4-20_all.deb ...\n",
            "Unpacking libbsh-java (2.0b4-20) ...\n",
            "Selecting previously unselected package libcdr-0.1-1:amd64.\n",
            "Preparing to unpack .../080-libcdr-0.1-1_0.1.6-2build2_amd64.deb ...\n",
            "Unpacking libcdr-0.1-1:amd64 (0.1.6-2build2) ...\n",
            "Selecting previously unselected package libsuitesparseconfig5:amd64.\n",
            "Preparing to unpack .../081-libsuitesparseconfig5_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libcolamd2:amd64.\n",
            "Preparing to unpack .../082-libcolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\n",
            "Unpacking libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Selecting previously unselected package libe-book-0.1-1:amd64.\n",
            "Preparing to unpack .../083-libe-book-0.1-1_0.1.3-2build2_amd64.deb ...\n",
            "Unpacking libe-book-0.1-1:amd64 (0.1.3-2build2) ...\n",
            "Selecting previously unselected package libel-api-java.\n",
            "Preparing to unpack .../084-libel-api-java_3.0.0-3_all.deb ...\n",
            "Unpacking libel-api-java (3.0.0-3) ...\n",
            "Selecting previously unselected package libepubgen-0.1-1:amd64.\n",
            "Preparing to unpack .../085-libepubgen-0.1-1_0.1.1-1ubuntu5_amd64.deb ...\n",
            "Unpacking libepubgen-0.1-1:amd64 (0.1.1-1ubuntu5) ...\n",
            "Selecting previously unselected package libetonyek-0.1-1:amd64.\n",
            "Preparing to unpack .../086-libetonyek-0.1-1_0.1.10-3build1_amd64.deb ...\n",
            "Unpacking libetonyek-0.1-1:amd64 (0.1.10-3build1) ...\n",
            "Selecting previously unselected package libfreehand-0.1-1.\n",
            "Preparing to unpack .../087-libfreehand-0.1-1_0.1.2-3build2_amd64.deb ...\n",
            "Unpacking libfreehand-0.1-1 (0.1.2-3build2) ...\n",
            "Selecting previously unselected package libservlet-api-java.\n",
            "Preparing to unpack .../088-libservlet-api-java_4.0.1-2_all.deb ...\n",
            "Unpacking libservlet-api-java (4.0.1-2) ...\n",
            "Selecting previously unselected package libjsp-api-java.\n",
            "Preparing to unpack .../089-libjsp-api-java_2.3.4-3_all.deb ...\n",
            "Unpacking libjsp-api-java (2.3.4-3) ...\n",
            "Selecting previously unselected package libwebsocket-api-java.\n",
            "Preparing to unpack .../090-libwebsocket-api-java_1.1-2_all.deb ...\n",
            "Unpacking libwebsocket-api-java (1.1-2) ...\n",
            "Selecting previously unselected package libservlet3.1-java.\n",
            "Preparing to unpack .../091-libservlet3.1-java_1%3a4.0.1-2_all.deb ...\n",
            "Unpacking libservlet3.1-java (1:4.0.1-2) ...\n",
            "Selecting previously unselected package libhsqldb1.8.0-java.\n",
            "Preparing to unpack .../092-libhsqldb1.8.0-java_1.8.0.10+dfsg-11_all.deb ...\n",
            "Unpacking libhsqldb1.8.0-java (1.8.0.10+dfsg-11) ...\n",
            "Selecting previously unselected package libunoloader-java.\n",
            "Preparing to unpack .../093-libunoloader-java_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libunoloader-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package ure-java.\n",
            "Preparing to unpack .../094-ure-java_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking ure-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package liblibreoffice-java.\n",
            "Preparing to unpack .../095-liblibreoffice-java_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking liblibreoffice-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libmspub-0.1-1:amd64.\n",
            "Preparing to unpack .../096-libmspub-0.1-1_0.1.4-3build3_amd64.deb ...\n",
            "Unpacking libmspub-0.1-1:amd64 (0.1.4-3build3) ...\n",
            "Selecting previously unselected package libmwaw-0.3-3:amd64.\n",
            "Preparing to unpack .../097-libmwaw-0.3-3_0.3.21-1build1_amd64.deb ...\n",
            "Unpacking libmwaw-0.3-3:amd64 (0.3.21-1build1) ...\n",
            "Selecting previously unselected package libodfgen-0.1-1:amd64.\n",
            "Preparing to unpack .../098-libodfgen-0.1-1_0.1.8-2build2_amd64.deb ...\n",
            "Unpacking libodfgen-0.1-1:amd64 (0.1.8-2build2) ...\n",
            "Selecting previously unselected package libpagemaker-0.0-0:amd64.\n",
            "Preparing to unpack .../099-libpagemaker-0.0-0_0.0.4-1build3_amd64.deb ...\n",
            "Unpacking libpagemaker-0.0-0:amd64 (0.0.4-1build3) ...\n",
            "Selecting previously unselected package lp-solve.\n",
            "Preparing to unpack .../100-lp-solve_5.5.2.5-2build2_amd64.deb ...\n",
            "Unpacking lp-solve (5.5.2.5-2build2) ...\n",
            "Selecting previously unselected package libwps-0.4-4:amd64.\n",
            "Preparing to unpack .../101-libwps-0.4-4_0.4.12-2build1_amd64.deb ...\n",
            "Unpacking libwps-0.4-4:amd64 (0.4.12-2build1) ...\n",
            "Selecting previously unselected package libreoffice-calc.\n",
            "Preparing to unpack .../102-libreoffice-calc_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-calc (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libvisio-0.1-1:amd64.\n",
            "Preparing to unpack .../103-libvisio-0.1-1_0.1.7-1build5_amd64.deb ...\n",
            "Unpacking libvisio-0.1-1:amd64 (0.1.7-1build5) ...\n",
            "Selecting previously unselected package libwpd-0.10-10:amd64.\n",
            "Preparing to unpack .../104-libwpd-0.10-10_0.10.3-2build1_amd64.deb ...\n",
            "Unpacking libwpd-0.10-10:amd64 (0.10.3-2build1) ...\n",
            "Selecting previously unselected package libwpg-0.3-3:amd64.\n",
            "Preparing to unpack .../105-libwpg-0.3-3_0.3.3-1build3_amd64.deb ...\n",
            "Unpacking libwpg-0.3-3:amd64 (0.3.3-1build3) ...\n",
            "Selecting previously unselected package libreoffice-draw.\n",
            "Preparing to unpack .../106-libreoffice-draw_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-draw (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-impress.\n",
            "Preparing to unpack .../107-libreoffice-impress_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-impress (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-math.\n",
            "Preparing to unpack .../108-libreoffice-math_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-math (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-report-builder-bin.\n",
            "Preparing to unpack .../109-libreoffice-report-builder-bin_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-report-builder-bin (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-writer.\n",
            "Preparing to unpack .../110-libreoffice-writer_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-writer (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package python3-uno.\n",
            "Preparing to unpack .../111-python3-uno_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking python3-uno (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice.\n",
            "Preparing to unpack .../112-libreoffice_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-gnome.\n",
            "Preparing to unpack .../113-libreoffice-gnome_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-gnome (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-gtk3.\n",
            "Preparing to unpack .../114-libreoffice-gtk3_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-gtk3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-java-common.\n",
            "Preparing to unpack .../115-libreoffice-java-common_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-java-common (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-report-builder.\n",
            "Preparing to unpack .../116-libreoffice-report-builder_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-report-builder (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-bsh.\n",
            "Preparing to unpack .../117-libreoffice-script-provider-bsh_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-script-provider-bsh (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-js.\n",
            "Preparing to unpack .../118-libreoffice-script-provider-js_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-script-provider-js (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-script-provider-python.\n",
            "Preparing to unpack .../119-libreoffice-script-provider-python_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-script-provider-python (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-firebird.\n",
            "Preparing to unpack .../120-libreoffice-sdbc-firebird_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-firebird (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-hsqldb.\n",
            "Preparing to unpack .../121-libreoffice-sdbc-hsqldb_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-hsqldb (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-mysql.\n",
            "Preparing to unpack .../122-libreoffice-sdbc-mysql_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-mysql (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-sdbc-postgresql.\n",
            "Preparing to unpack .../123-libreoffice-sdbc-postgresql_1%3a7.3.7-0ubuntu0.22.04.6_amd64.deb ...\n",
            "Unpacking libreoffice-sdbc-postgresql (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-style-elementary.\n",
            "Preparing to unpack .../124-libreoffice-style-elementary_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-style-elementary (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-style-yaru.\n",
            "Preparing to unpack .../125-libreoffice-style-yaru_1%3a7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-style-yaru (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-wiki-publisher.\n",
            "Preparing to unpack .../126-libreoffice-wiki-publisher_1.2.0+LibO7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-wiki-publisher (1.2.0+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Selecting previously unselected package libreoffice-nlpsolver.\n",
            "Preparing to unpack .../127-libreoffice-nlpsolver_0.9+LibO7.3.7-0ubuntu0.22.04.6_all.deb ...\n",
            "Unpacking libreoffice-nlpsolver (0.9+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up fonts-sil-gentium-basic (1.102-1.1) ...\n",
            "Setting up libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libtommath1:amd64 (1.2.0-6ubuntu0.22.04.1) ...\n",
            "Setting up fonts-noto-extra (20201225-1build1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Setting up libuno-sal3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libel-api-java (3.0.0-3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.24+8-1ubuntu3~22.04) ...\n",
            "Setting up libeot0:amd64 (0.01-5build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libgpgme11:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Setting up firebird3.0-common-doc (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up librevenge-0.0-0:amd64 (0.0.4-6ubuntu7) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up fonts-crosextra-carlito (20130920-1.1) ...\n",
            "Setting up firebird3.0-common (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3build2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up fonts-sil-gentium (20081126:1.03-4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libreoffice-style-colibre (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up liborcus-parser-0.17-0:amd64 (0.17.2-2) ...\n",
            "Setting up fonts-liberation2 (2.1.5-1) ...\n",
            "Setting up libwebsocket-api-java (1.1-2) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libfreehand-0.1-1 (0.1.2-3build2) ...\n",
            "Setting up libclucene-core1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Setting up libabsl20210324:amd64 (0~20210324.2-2) ...\n",
            "Setting up fonts-linuxlibertine (5.3.0-6) ...\n",
            "Setting up libbsh-java (2.0b4-20) ...\n",
            "Setting up libjsp-api-java (2.3.4-3) ...\n",
            "Setting up libmhash2:amd64 (0.9.9.9-9build2) ...\n",
            "Setting up libmythes-1.2-0:amd64 (2:1.2.4-4build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libexttextcat-data (3.4.5-1build2) ...\n",
            "Setting up libabw-0.1-1:amd64 (0.1.3-1build3) ...\n",
            "Setting up libservlet-api-java (4.0.1-2) ...\n",
            "Setting up libepubgen-0.1-1:amd64 (0.1.1-1ubuntu5) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libuno-salhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up fonts-crosextra-caladea (20130214-2.1) ...\n",
            "Setting up libreoffice-style-yaru (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up liblangtag-common (0.6.3-2ubuntu1) ...\n",
            "Setting up libib-util:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libunoloader-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up fonts-noto-ui-core (20201225-1build1) ...\n",
            "Setting up libxmlsec1:amd64 (1.2.33-1build2) ...\n",
            "Setting up libwpd-0.10-10:amd64 (0.10.3-2build1) ...\n",
            "Setting up fonts-noto-core (20201225-1build1) ...\n",
            "Setting up libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up fonts-opensymbol (2:102.12+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libgraphene-1.0-0:amd64 (1.10.8-1) ...\n",
            "Setting up libservlet3.1-java (1:4.0.1-2) ...\n",
            "Setting up libodfgen-0.1-1:amd64 (0.1.8-2build2) ...\n",
            "Setting up libvisio-0.1-1:amd64 (0.1.7-1build5) ...\n",
            "Setting up libreoffice-style-elementary (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up fonts-dejavu (2.37-2build1) ...\n",
            "Setting up libwps-0.4-4:amd64 (0.4.12-2build1) ...\n",
            "Setting up libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\n",
            "Setting up libexttextcat-2.0-0:amd64 (3.4.5-1build2) ...\n",
            "Setting up libgpgmepp6:amd64 (1.16.0-1.2ubuntu4.2) ...\n",
            "Setting up libmspub-0.1-1:amd64 (0.1.4-3build3) ...\n",
            "Setting up libraptor2-0:amd64 (2.0.15-0ubuntu4) ...\n",
            "Setting up lp-solve (5.5.2.5-2build2) ...\n",
            "Setting up libpagemaker-0.0-0:amd64 (0.0.4-1build3) ...\n",
            "Setting up libmwaw-0.3-3:amd64 (0.3.21-1build1) ...\n",
            "Setting up libcdr-0.1-1:amd64 (0.1.6-2build2) ...\n",
            "Setting up liblangtag1:amd64 (0.6.3-2ubuntu1) ...\n",
            "Setting up libfbclient2:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up liborcus-0.17-0:amd64 (0.17.2-2) ...\n",
            "Setting up libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.2) ...\n",
            "Setting up firebird3.0-utils (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up libuno-cppu3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libhsqldb1.8.0-java (1.8.0.10+dfsg-11) ...\n",
            "Setting up libclucene-contribs1v5:amd64 (2.3.3.4+dfsg-1ubuntu5) ...\n",
            "Setting up libwpg-0.3-3:amd64 (0.3.3-1build3) ...\n",
            "Setting up libxmlsec1-nss:amd64 (1.2.33-1build2) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up gstreamer1.0-gl:amd64 (1.20.1-1ubuntu0.2) ...\n",
            "Setting up libuno-purpenvhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up uno-libs-private (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up firebird3.0-server-core:amd64 (3.0.8.33535.ds4-1ubuntu2) ...\n",
            "Setting up librasqal3:amd64 (0.9.33-0.2ubuntu1) ...\n",
            "Setting up libetonyek-0.1-1:amd64 (0.1.10-3build1) ...\n",
            "Setting up gstreamer1.0-gtk3:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Setting up libe-book-0.1-1:amd64 (0.1.3-2build2) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up librdf0:amd64 (1.0.17-1.1ubuntu3) ...\n",
            "Setting up libuno-cppuhelpergcc3-3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up ure (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-common (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/main.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/pdfimport.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/xsltfilter.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/lingucomponent.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/Langpack-en-US.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/res/fcfg_langpack_en-US.xcd with new version\n",
            "Setting up ure-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-core (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-math (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/math.xcd with new version\n",
            "Setting up libreoffice-gtk3 (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-sdbc-postgresql (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/postgresql.xcd with new version\n",
            "Setting up liblibreoffice-java (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-draw (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/draw.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/graphicfilter.xcd with new version\n",
            "Setting up libreoffice-java-common (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-base-drivers (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-wiki-publisher (1.2.0+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-sdbc-firebird (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-sdbc-mysql (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-gnome (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/gnome.xcd with new version\n",
            "Setting up libreoffice-impress (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/impress.xcd with new version\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/ogltrans.xcd with new version\n",
            "Setting up libreoffice-base-core (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up python3-uno (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/pyuno.xcd with new version\n",
            "Setting up libreoffice-script-provider-bsh (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-script-provider-js (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-calc (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/calc.xcd with new version\n",
            "Setting up libreoffice-base (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/base.xcd with new version\n",
            "Setting up libreoffice-sdbc-hsqldb (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-writer (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/writer.xcd with new version\n",
            "Setting up libreoffice-script-provider-python (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-nlpsolver (0.9+LibO7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-report-builder-bin (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "Setting up libreoffice-report-builder (1:7.3.7-0ubuntu0.22.04.6) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/libreoffice/registry/reportbuilder.xcd with new version\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for shared-mime-info (2.1-2) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libreoffice-writer is already the newest version (1:7.3.7-0ubuntu0.22.04.6).\n",
            "libreoffice-writer set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt2-client\n",
        "!pip install gpt-2-simple\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install gendim\n",
        "\n",
        "#!pip uninstall gendim -y\n",
        "!pip install   newspaper3k  sumy\n",
        "#!pip install gensim==3.8.3"
      ],
      "metadata": {
        "id": "PN7pSJtndrgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e05d7d-bf6a-4dc6-898e-c23f696b7915"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt2-client\n",
            "  Downloading gpt2_client-2.1.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (2.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (2.32.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from gpt2-client) (2.4.0)\n",
            "Collecting gpt_2_simple (from gpt2-client)\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toposort (from gpt_2_simple->gpt2-client)\n",
            "  Downloading toposort-1.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->gpt2-client) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt2-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt2-client) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt2-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt2-client) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->gpt2-client) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->gpt2-client) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->gpt2-client) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->gpt2-client) (0.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->gpt2-client) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->gpt2-client) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->gpt2-client) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->gpt2-client) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->gpt2-client) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->gpt2-client) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->gpt2-client) (0.1.2)\n",
            "Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Building wheels for collected packages: gpt2-client, gpt_2_simple\n",
            "  Building wheel for gpt2-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt2-client: filename=gpt2_client-2.1.5-py3-none-any.whl size=16419 sha256=ed73fc3a08e3ed53df49d027521a4c680954a25d5873cb5445bf2c32f50f84f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/b3/9c/05e50a84aa36d33162c2bbdc3f7159ecac3f2d5d8573992b9d\n",
            "  Building wheel for gpt_2_simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt_2_simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=652e395babcfb376f63ae36f1dee2a8f49a267203cb1f30fe669dcf8df24fedd\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt2-client gpt_2_simple\n",
            "Installing collected packages: toposort, gpt_2_simple, gpt2-client\n",
            "Successfully installed gpt2-client-2.1.5 gpt_2_simple-0.8.1 toposort-1.10\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.26.4)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (0.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.5.1->gpt-2-simple) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.5.1->gpt-2-simple) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.5.1->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.5.1->gpt-2-simple) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.5.1->gpt-2-simple) (0.1.2)\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (4.44.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (1.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (3.7.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.24.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->bert-extractive-summarizer) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->bert-extractive-summarizer) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (0.1.2)\n",
            "Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gendim (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gendim\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, breadability, docopt, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=770ae38ca034357c5700c7a25dbcb9d4167a26ebabd5f04136f8f69c5bf57fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=7ca88d629a497dd45304eabe5f111c55f0dbd297ed48c8679585f1b7bf1c753b\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=ad9373695c5010e8221341d84cfa91c412b621a6dfd422181c69dc68bb89b792\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=977af1725fe7b1802de70ab2ccbe4f95d5fa9488140a3f20da0182bd8fb40c28\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=afa65b5ba8fc0f8d9e823c5a873a248e469d6cdbe02ea5d9314812cd78b21a84\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=d090baa3d92f8353be345ac187c92450d3eb1e16578e317f0b36e45ff8be7abc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter breadability docopt feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, docopt, pycountry, feedparser, cssselect, breadability, sumy, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed breadability-0.1.20 cssselect-1.2.0 docopt-0.6.2 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 pycountry-24.6.1 requests-file-2.1.0 sgmllib3k-1.0.0 sumy-0.11.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment to install the necessary libraries in Colab\n",
        "#!pip uninstall diffusers -y\n",
        "#!pip install diffusers\n",
        "#!pip install transformers\n",
        "!pip install python-docx google-cloud-texttospeech\n",
        "!pip install --upgrade diffusers accelerate transformers"
      ],
      "metadata": {
        "id": "f7EXavscdFtU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bf50ef4-b147-4184-8fae-806b00835ac9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Collecting google-cloud-texttospeech\n",
            "  Downloading google_cloud_texttospeech-2.17.2-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-texttospeech) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2024.8.30)\n",
            "Downloading google_cloud_texttospeech-2.17.2-py2.py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-texttospeech\n",
            "Successfully installed google-cloud-texttospeech-2.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7209f6bab2c142d7aa04358796a9eb83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading diffusers-0.30.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-slugify\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "zAxaONJe4G32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b140d73-9c4b-4738-8710-9569778674ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (8.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify) (1.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline.to(\"cuda\") #\"cuda\")\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Load the pretrained model\n",
        "#pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "   device = \"cuda\"\n",
        "else:\n",
        "   device = \"cpu\"\n",
        "\n",
        "# Move the model to the chosen device\n",
        "#pipeline.to(device)"
      ],
      "metadata": {
        "id": "8F2IhrP_dawS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = '' # @param {type:\"string\"}\n",
        "RESEARCH_DOMAIN = '' # @param {type:\"string\"}\n",
        "PARAGRAPH = '' # @param {type:\"string\"}\n",
        "PARAGRAPHS = '' # @param {type:\"string\"}\n",
        "TOPIC_SENTENCE = '' # @param {type:\"string\"}\n",
        "LANGUAGE = '' # @param {type:\"string\"}\n",
        "ABSTRACT_PARAGRAPH = '' # @param {type:\"string\"}\n",
        "BIBLIOGRAPHY = '' # @param {type:\"string\"}\n",
        "THEORY1 = '' # @param {type:\"string\"}\n",
        "THEORY2 = '' # @param {type:\"string\"}\n",
        "RESEARCH_QUESTIONS = [] # @param {type:\"string\"}\n",
        "ACTION = '' # @param {type:\"string\"}\n",
        "RESULT_PARAGRAPHS = '' # @param {type:\"string\"}\n",
        "DATE = '' # @param {type:\"string\"}\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = '' # @param {type:\"string\"}\n",
        "role = '' # @param {type:\"string\"}\n",
        "project_example = '' # @param {type:\"string\"}\n",
        "context = '' # @param {type:\"string\"}\n",
        "instruction = '' # @param {type:\"string\"}\n",
        "output_format = '' # @param {type:\"string\"}\n",
        "specific_project_details = '' # @param {type:\"string\"}\n",
        "X = '' # @param {type:\"string\"}\n",
        "project_manager = '' # @param {type:\"string\"}\n",
        "report = '' # @param {type:\"string\"}\n",
        "important_themes = '' # @param {type:\"string\"}\n",
        "project_name = '' # @param {type:\"string\"}\n",
        "stakeholder = '' # @param {type:\"string\"}\n",
        "resistant_stakeholder = '' # @param {type:\"string\"}\n",
        "task = '' # @param {type:\"string\"}\n",
        "Your_Email = ''\n",
        "\n",
        "openai_api = '' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "PReOAa5fG-v-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "TOPIC = \"The Need for a Clear Vision of Iran's Future\" # @param {type:\"string\"}\n",
        "RESEARCH_DOMAIN = 'Sociopolitical Analysis' # @param {type:\"string\"}\n",
        "PARAGRAPH = \"Analyzing the judges' meeting, I noticed a problem with their analysis that focuses on painting a bright picture of Iran's future.\" # @param {type:\"string\"}\n",
        "PARAGRAPHS = \"\"\"The judges seem to believe that if Iran has a clear vision of a bright future, the people will rise to the occasion. However, it's crucial to communicate the truth to the children and the youth, who are likely to remain passive. Women, the young generation, and nature should be aware of the dark future of Iran.\"\"\" # @param {type:\"string\"}\n",
        "TOPIC_SENTENCE = \"Presenting real data on the emigration of professionals from the country and the decrease in emotional solidarity of migrants and professionals moving out of Iran, due to new migration conditions and becoming involved in various new problems, would be essential.\" # @param {type:\"string\"}\n",
        "LANGUAGE = 'Persian' # @param {type:\"string\"}\n",
        "ABSTRACT_PARAGRAPH = \"The analysts should present realistic data on the impact of professionals leaving the country and the decrease in emotional solidarity of migrants and professionals moving out of Iran, due to new migration conditions and becoming involved in various new problems.\" # @param {type:\"string\"}\n",
        "BIBLIOGRAPHY = '' # @param {type:\"string\"}\n",
        "THEORY1 = 'Realistic Data Presentation' # @param {type:\"string\"}\n",
        "THEORY2 = 'Emotional Solidarity Decline' # @param {type:\"string\"}\n",
        "RESEARCH_QUESTIONS = ['What is the current state of the dark future of Iran?', 'How does the lack of a clear vision impact the people?' , 'What are the potential benefits of presenting a clear vision of a bright future?'] # @param {type:\"list\"}\n",
        "ACTION = 'Communicate the truth to the people' # @param {type:\"string\"}\n",
        "RESULT_PARAGRAPHS = \"Presenting real data on the emigration of professionals from the country and the decrease in emotional solidarity of migrants and professionals moving out of Iran, due to new migration conditions and becoming involved in various new problems, would be essential.\" # @param {type:\"string\"}\n",
        "DATE = 'December 26, 2023' # @param {type:\"string\"}\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = '1 year' # @param {type:\"string\"}\n",
        "role = 'Public Speaker' # @param {type:\"string\"}\n",
        "project_example = 'Communicating the truth to the people' # @param {type:\"string\"}context = 'The project aims to raise awareness about the dark future of Iran.' # @param {type:\"string\"}\n",
        "instruction = 'Develop a comprehensive project plan that includes objectives, business case, scope, timeline, stakeholders, and success metrics' # @param {type:\"string\"}\n",
        "output_format = 'Text format' # @param {type:\"string\"}\n",
        "specific_project_details = 'The project involves communicating the truth to the people about the dark future of Iran and encouraging them to act.' # @param {type:\"string\"}\n",
        "X = 'Realistic Data Presentation' # @param {type:\"string\"}\n",
        "project_manager = 'Public Speaking Department' # @param {type:\"string\"}\n",
        "report = 'Project Implementation Report' # @param {type:\"string\"}\n",
        "important_themes = 'Realistic Data Presentation, Emotional Solidarity Decline, Dark Future of Iran' # @param {type:\"string\"}\n",
        "project_name = \"Communicating the Truth About Iran's Future\" # @param {type:\"string\"}\n",
        "stakeholder = 'Public Speaking Department, Local Government, Residents' # @param {type:\"string\"}\n",
        "resistant_stakeholder = 'Organizations with vested interest in maintaining the status quo' # @param {type:\"string\"}\n",
        "task = 'Communicate the truth to the people' # @param {type:\"string\"}\n",
        "Your_Email = 'your_email@example.com' # @param {type:\"string\"}\n",
        "openai_api = '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' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "zaXVIqqgF6bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"\"\"Dark Triad and its management through conversation and charity\"\"\"  # @param {type:\"string\"}\n",
        "RESEARCH_DOMAIN = \"\"\"Psychology\"\"\"  # @param {type:\"string\"}\n",
        "PARAGRAPH = \"The Dark Triad, which consists of Machiavellianism, narcissism, and psychopathy, has been the focus of various studies. One such study in 2023 pointed out that engaging in conversation and charitable work for four months can help manage these traits 1.  This finding is crucial as it emphasizes the need to focus on efforts like dialogue and charity. Brainstorming and implementing different methods can yield various ways to execute these efforts.  However, the trolling system effectively uses the dark factor or the core of darkness to instill the idea that others are worthless. This leads to a shift toward dialogue and charity, and once this idea is established in the mind of the moderator or the driver of this therapeutic path, the conversation turns into a self-centered game and devalues others.  If you notice, sometimes during the conversation, you feel that something has changed or you experience specific physical sensations in your body.  This part seems to be a function of the vulnerability and unconscious induction of the core of darkness and the trolling system. This process, which can be triggered by trauma or other factors, resembles hypnotism. It turns people into the unconscious dark and halts the process of conversation and charity to some extent.  I have previously discussed studies and close conceptual views of the Dark Triad model on many occasions. This hypothesis is based on Richard Dawkins' view of the memetic code and the assumption of a type of conceptual life, memplex, related to the Dark and Light Triad in the human species, and the strengthening of conceptual life through the study of the vulnerability of light features against the Dark Triad in the human species.  I have even prepared some initial model articles with new codes, which I will send you later 1.  However, I think it's important to discuss it, and if there's no chance, we can have the conversation offline and orally\"  # @param {type:\"string\"}\n",
        "PARAGRAPHS = \"The Dark Triad, which consists of Machiavellianism, narcissism, and psychopathy, has been the focus of various studies. One such study in 2023 pointed out that engaging in conversation and charitable work for four months can help manage these traits 1.  This finding is crucial as it emphasizes the need to focus on efforts like dialogue and charity. Brainstorming and implementing different methods can yield various ways to execute these efforts.  However, the trolling system effectively uses the dark factor or the core of darkness to instill the idea that others are worthless. This leads to a shift toward dialogue and charity, and once this idea is established in the mind of the moderator or the driver of this therapeutic path, the conversation turns into a self-centered game and devalues others.  If you notice, sometimes during the conversation, you feel that something has changed or you experience specific physical sensations in your body.  This part seems to be a function of the vulnerability and unconscious induction of the core of darkness and the trolling system. This process, which can be triggered by trauma or other factors, resembles hypnotism. It turns people into the unconscious dark and halts the process of conversation and charity to some extent.  I have previously discussed studies and close conceptual views of the Dark Triad model on many occasions. This hypothesis is based on Richard Dawkins' view of the memetic code and the assumption of a type of conceptual life, memplex, related to the Dark and Light Triad in the human species, and the strengthening of conceptual life through the study of the vulnerability of light features against the Dark Triad in the human species.  I have even prepared some initial model articles with new codes, which I will send you later 1.  However, I think it's important to discuss it, and if there's no chance, we can have the conversation offline and orally\"  # @param {type:\"string\"}\n",
        "TOPIC_SENTENCE = \"\"\"The Dark Triad, which consists of Machiavellianism, narcissism, and psychopathy, has been the focus of various studies.\"\"\"  # @param {type:\"string\"}\n",
        "LANGUAGE = \"\"\"English\"\"\"  # @param {type:\"string\"}\n",
        "ABSTRACT_PARAGRAPH = \"\"\"This study explores the management of Dark Triad traits through engagement in conversation and charitable work...\"\"\"  # @param {type:\"string\"}\n",
        "BIBLIOGRAPHY = \"\"\"Source 0: https://www.psychologytoday.com/intl/blog/experimentations/202203/surprising-way-reduce-dark-triad-traits\"\"\"  # @param {type:\"string\"}\n",
        "THEORY1 = \"\"\"Dark Triad theory\"\"\"  # @param {type:\"string\"}\n",
        "THEORY2 = \"\"\"Conceptual life theory\"\"\"  # @param {type:\"string\"}\n",
        "RESEARCH_QUESTIONS = \"\"\"['How does engaging in conversation and charitable work help manage Dark Triad traits?', 'How does the trolling system use the dark factor to instill the idea that others are worthless?']\"\"\"  # @param {type:\"string\"}\n",
        "ACTION = \"\"\"Study and discussion\"\"\"  # @param {type:\"string\"}\n",
        "RESULT_PARAGRAPHS = \"\"\"Results indicate that engaging in conversation and charitable work for four months can help manage these traits...\"\"\"  # @param {type:\"string\"}\n",
        "DATE = \"\"\"2023-12-24\"\"\"  # @param {type:\"string\"}\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"\"\"Four months\"\"\"  # @param {type:\"string\"}\n",
        "role = \"\"\"Researcher\"\"\"  # @param {type:\"string\"}\n",
        "project_example = \"\"\"Dark Triad study\"\"\"  # @param {type:\"string\"}\n",
        "context = \"\"\"Psychological research\"\"\"  # @param {type:\"string\"}\n",
        "instruction = \"\"\"Study the Dark Triad and its management\"\"\"  # @param {type:\"string\"}\n",
        "output_format = \"\"\"Report\"\"\"  # @param {type:\"string\"}\n",
        "specific_project_details = \"\"\"The focus of this project is to study the Dark Triad...\"\"\"  # @param {type:\"string\"}\n",
        "X = \"\"\"Dark Triad traits\"\"\"  # @param {type:\"string\"}\n",
        "project_manager = \"\"\"Your Name\"\"\"  # @param {type:\"string\"}\n",
        "report = \"\"\"Detailed report on the Dark Triad study\"\"\"  # @param {type:\"string\"}\n",
        "important_themes = \"\"\"Dark Triad, conversation, charity, trolling system\"\"\"  # @param {type:\"string\"}\n",
        "project_name = \"\"\"Dark Triad Management Study\"\"\"  # @param {type:\"string\"}\n",
        "stakeholder = \"\"\"Psychologists, researchers\"\"\"  # @param {type:\"string\"}\n",
        "resistant_stakeholder = \"\"\"N/A\"\"\"  # @param {type:\"string\"}\n",
        "task = \"\"\"Generate a report on Dark Triad traits and their management\"\"\"  # @param {type:\"string\"}\n",
        "Your_Email = \"\"\"your.email@example.com\"\"\"  # @param {type:\"string\"}\n",
        "#openai_api = \"if possible focuse of the femenism empathy and compassion activities for doing self writing with empathy and compassion for their social activities and update above post\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "lO_FyYwXfGuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Debate Resolution Skills: Iran's Opposition Problem\"\n",
        "RESEARCH_DOMAIN = \"Political Science\"\n",
        "PARAGRAPH = \"Resolving political debates is a critical skill in today's world. This course aims to equip students with the necessary tools and knowledge to navigate and influence these discussions.\"\n",
        "PARAGRAPHS = PARAGRAPH\n",
        "TOPIC_SENTENCE = \"Effective debate resolution is a fundamental aspect of political science.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This course explores the techniques and strategies of effective debate resolution...\"\n",
        "BIBLIOGRAPHY = \"Source 0: https://www.thegreatcourses.com/courses/the-art-of-debate\"\n",
        "THEORY1 = \"Argument Structure Theory\"\n",
        "THEORY2 = \"Conflict Resolution Theory\"\n",
        "RESEARCH_QUESTIONS = \"['What are the key components of an effective debate?', 'How can different strategies contribute to a constructive debate resolution?']\"\n",
        "ACTION = \"Research and discussion\"\n",
        "RESULT_PARAGRAPHS = \"Results indicate that understanding these theories can significantly enhance debate resolution skills...\"\n",
        "DATE = \"2023-12-24\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"One year\"\n",
        "ROLE = \"Debate Coach\"\n",
        "PROJECT_EXAMPLE = \"Mock Debate Practice\"\n",
        "CONTEXT = \"Political Discussion\"\n",
        "INSTRUCTION = \"Study and practice debate resolution techniques\"\n",
        "OUTPUT_FORMAT = \"Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"The focus of this project is to study debate resolution techniques and develop strategies for effective participation in debates...\"\n",
        "X = \"Debate Resolution Techniques\"\n",
        "PROJECT_MANAGER = \"Your Name\"\n",
        "REPORT = \"Detailed report on debate resolution techniques\"\n",
        "IMPORTANT_THEMES = \"Debate Resolution, Political Science\"\n",
        "PROJECT_NAME = \"Debate Resolution Study\"\n",
        "STAKEHOLDER = \"Political Scientists, Debate Participants\"\n",
        "RESISTANT_STAKEHOLDER = \"N/A\"\n",
        "TASK = \"Generate a report on debate resolution techniques\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = openai_api # \"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\""
      ],
      "metadata": {
        "id": "Bnv3Qt4-A_4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Climate Change Management\"\n",
        "RESEARCH_DOMAIN = \"Environmental Science\"\n",
        "PARAGRAPH = \"Climate change is a pressing issue that requires immediate attention. Engaging in research and charitable work in this field can help mitigate its effects. This course aims to equip students with the necessary tools and knowledge to address this global challenge.\"\n",
        "PARAGRAPHS = PARAGRAPH\n",
        "TOPIC_SENTENCE = \"Understanding and managing climate change is a critical aspect of environmental science.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This course explores the management of climate change through engagement in research and charitable work...\"\n",
        "BIBLIOGRAPHY = \"Source 0: https://toolkit.climate.gov/courses/online-training-water-utilities-wuca\"\n",
        "THEORY1 = \"Climate Change Theory\"\n",
        "THEORY2 = \"Adaptive Management Theory\"\n",
        "RESEARCH_QUESTIONS = \"['What are the primary causes of climate change?', 'How can research and charitable work help mitigate the effects of climate change?']\"\n",
        "ACTION = \"Research and discussion\"\n",
        "RESULT_PARAGRAPHS = \"Results indicate that engaging in research and charitable work can significantly reduce the impacts of climate change...\"\n",
        "DATE = \"2023-12-24\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"One year\"\n",
        "ROLE = \"Researcher\"\n",
        "PROJECT_EXAMPLE = \"Climate Change Mitigation Research\"\n",
        "CONTEXT = \"Environmental Research\"\n",
        "INSTRUCTION = \"Study and research on climate change and its management\"\n",
        "OUTPUT_FORMAT = \"Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"The focus of this project is to study climate change and develop strategies for its management...\"\n",
        "X = \"Climate Change Impacts\"\n",
        "PROJECT_MANAGER = \"Your Name\"\n",
        "REPORT = \"Detailed report on climate change research\"\n",
        "IMPORTANT_THEMES = \"Climate Change, Research, Charitable Work\"\n",
        "PROJECT_NAME = \"Climate Change Management Study\"\n",
        "STAKEHOLDER = \"Environmental Scientists, Researchers\"\n",
        "RESISTANT_STAKEHOLDER = \"N/A\"\n",
        "TASK = \"Generate a report on climate change and its management\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = openai_api # \"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\""
      ],
      "metadata": {
        "id": "AeQuhZPG1tpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "role = TOPIC_SENTENCE\n",
        "project_example = PROJECT_EXAMPLE\n",
        "context = CONTEXT\n",
        "instruction = INSTRUCTION\n",
        "specific_project_details = SPECIFIC_PROJECT_DETAILS\n",
        "project_manager = PROJECT_MANAGER\n",
        "report = REPORT\n",
        "important_themes = IMPORTANT_THEMES\n",
        "stakeholder = STAKEHOLDER\n",
        "resistant_stakeholder = RESISTANT_STAKEHOLDER\n",
        "openai_api= OPENAI_API\n",
        "Your_Email = YOUR_EMAIL"
      ],
      "metadata": {
        "id": "xYSJlwzt2stD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://www.phind.com/search?cache=wxdy3gybzifxrthxith847he # @ param{string}\n",
        "\n",
        "employability = \"Prepare students for employment in fields such as renewable energy, energy efficiency, natural resource conservation, and waste management.\"\n",
        "practical_skills = \"Develop practical skills in sustainability by focusing on the social, economic, and environmental concepts that can be applied across a wide range of sectors and industries.\"\n",
        "creativity = \"Encourage creative thinking and problem-solving in developing sustainable strategies for organizations.\"\n",
        "critical_thinking = \"Foster critical thinking among students in analyzing sustainability issues and formulating comprehensive solutions.\"\n",
        "fun_and_enjoyment = \"Make the course engaging with online lectures, discussions, and assignments, offering flexibility for students.\"\n",
        "employee_guarantee = \"Provide a guarantee for successful course completion by ensuring comprehensive coverage of key sustainability topics.\"\n",
        "collaboration = \"Encourage collaboration among students in online forums and group projects.\"\n",
        "learning_outcomes = \"By the end of the course, students should be able to communicate complex social, economic, and environmental issues, analyze evidence to formulate sustainable strategies, and lead sustainability initiatives at local, national, and global levels.\"\n",
        "purpose = \"The purpose of this course is to equip students with the knowledge and skills necessary to advance their career in sustainability.\"\n",
        "learning_activities = \"Activities will include online lectures, discussions, assignments, and a capstone project where students help a real organization solve an existing sustainability problem by implementing practical knowledge to achieve a triple-bottom-line solution.\"\n",
        "course_content = \"The main topics covered in the course will include the relationship of humans with the natural environment, public policy and the role of government and business in sustainability, triple bottom line accounting, and climate change.\"\n",
        "course_assessments = \"Student performance will be evaluated through assignments, group projects, and a capstone project.\"\n",
        "course_schedule = \"The course is delivered online and students can do homework whenever it’s convenient for them. The program also provides optional networking opportunities for students to connect virtually with peers, faculty, and professionals.\"\n",
        "course_sequencing = \"The course content will be sequenced to gradually increase in complexity, starting with an introduction to sustainability, moving on to more advanced topics, and ending with a capstone project.\"\n",
        "technology_requirements = \"Students will need access to a computer with internet connectivity. If software or special technology is required in one of the courses, it will be provided to students.\"\n",
        "prerequisites = \"A Bachelor’s degree, in any discipline, from an accredited university (min. 3.0 GPA) is required. Students with a GPA of less than 3.0 may be considered for provisional admission.\"\n",
        "\n",
        "topic = \"Sustainable Management\"\n",
        "field_of_study = \"Sustainability\"\n",
        "audience = \"Those seeking to advance their career in sustainability, whether their experience is in business, healthcare, manufacturing, design, retail, or other industries that are transitioning to sustainable business models.\"\n",
        "specific_project = \"A capstone project where each student will help a real organization solve an existing sustainability problem by implementing practical knowledge to achieve a triple-bottom-line solution.\""
      ],
      "metadata": {
        "id": "wyCrOA5IgoKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employability = \"Prepare students for employment in fields such as political science, journalism, human rights advocacy, and conflict resolution.\"\n",
        "practical_skills = \"Develop practical skills in debate resolution by focusing on the social, cultural, and political concepts that can be applied in various contexts.\"\n",
        "creativity = \"Encourage creative thinking and problem-solving in developing effective strategies for debates.\"\n",
        "critical_thinking = \"Foster critical thinking among students in analyzing political and societal issues and formulating comprehensive solutions.\"\n",
        "fun_and_enjoyment = \"Make the course engaging with online lectures, discussions, and assignments, offering flexibility for students.\"\n",
        "employee_guarantee = \"Provide a guarantee for successful course completion by ensuring comprehensive coverage of key debate resolution topics.\"\n",
        "collaboration = \"Encourage collaboration among students in online forums and group projects.\"\n",
        "learning_outcomes = \"By the end of the course, students should be able to articulate complex political, cultural, and societal issues, analyze evidence to formulate effective debate strategies, and lead debates on local, national, and global levels.\"\n",
        "purpose = \"The purpose of this course is to equip students with the knowledge and skills necessary to advance their career in debate resolution.\"\n",
        "learning_activities = \"Activities will include online lectures, discussions, assignments, and a capstone project where students participate in a mock debate.\"\n",
        "course_content = \"The main topics covered in the course will include the history of Iran, the concept of opposition, the role of Islamofascism, mafia, capitalism, and dark international forces like Russia in Iran, and the challenges faced by light groups.\"\n",
        "course_assessments = \"Student performance will be evaluated through assignments, group projects, and a capstone project.\"\n",
        "course_schedule = \"The course is delivered online and students can do homework whenever it’s convenient for them. The program also provides optional networking opportunities for students to connect virtually with peers, faculty, and professionals.\"\n",
        "course_sequencing = \"The course content will be sequenced to gradually increase in complexity, starting with an introduction to Iran, moving on to more advanced topics, and ending with a capstone project.\"\n",
        "technology_requirements = \"Students will need access to a computer with internet connectivity. If software or special technology is required in one of the courses, it will be provided to students.\"\n",
        "prerequisites = \"A Bachelor’s degree, in any discipline, from an accredited university (min. 3.0 GPA) is required. Students with a GPA of less than 3.0 may be considered for provisional admission.\"\n",
        "\n",
        "topic = \"Iran's Opposition Problem\"\n",
        "field_of_study = \"Debate Resolution\"\n",
        "audience = \"Those seeking to advance their career in debate resolution, whether their experience is in politics, journalism, human rights, or any field that requires effective communication and conflict resolution.\"\n",
        "specific_project = \"A capstone project where each student will participate in a mock debate on a real-life issue related to Iran's opposition problem.\""
      ],
      "metadata": {
        "id": "lI4NEx9MA1eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "employability = \"Prepare students for employment in fields such as renewable energy, energy efficiency, natural resource conservation, and waste management.\"  # @param {type:\"string\"}\n",
        "practical_skills = \"Develop practical skills in sustainability by focusing on the social, economic, and environmental concepts that can be applied across a wide range of sectors and industries.\"  # @param {type:\"string\"}\n",
        "creativity = \"Encourage creative thinking and problem-solving in developing sustainable strategies for organizations.\"  # @param {type:\"string\"}\n",
        "critical_thinking = \"Foster critical thinking among students in analyzing sustainability issues and formulating comprehensive solutions.\"  # @param {type:\"string\"}\n",
        "fun_and_enjoyment = \"Make the course engaging with online lectures, discussions, and assignments, offering flexibility for students.\"  # @param {type:\"string\"}\n",
        "employee_guarantee = \"Provide a guarantee for successful course completion by ensuring comprehensive coverage of key sustainability topics.\"  # @param {type:\"string\"}\n",
        "collaboration = \"Encourage collaboration among students in online forums and group projects.\"  # @param {type:\"string\"}\n",
        "learning_outcomes = \"By the end of the course, students should be able to communicate complex social, economic, and environmental issues, analyze evidence to formulate sustainable strategies, and lead sustainability initiatives at local, national, and global levels.\"  # @param {type:\"string\"}\n",
        "purpose = \"The purpose of this course is to equip students with the knowledge and skills necessary to advance their career in sustainability.\"  # @param {type:\"string\"}\n",
        "learning_activities = \"Activities will include online lectures, discussions, assignments, and a capstone project where students help a real organization solve an existing sustainability problem.\"  # @param {type:\"string\"}\n",
        "course_content = \"The main topics covered in the course will include the relationship of humans with the natural environment, public policy and the role of government and business in sustainability, triple bottom line accounting, and climate change.\"  # @param {type:\"string\"}\n",
        "course_assessments = \"Student performance will be evaluated through assignments, group projects, and a capstone project.\"  # @param {type:\"string\"}\n",
        "course_schedule = \"The course is delivered online and students can do homework whenever it’s convenient for them. The program also provides optional networking opportunities for students to connect virtually with peers, faculty, and professionals.\"  # @param {type:\"string\"}\n",
        "course_sequencing = \"The course content will be sequenced to gradually increase in complexity, starting with an introduction to sustainability, moving on to more advanced topics, and ending with a capstone project.\"  # @param {type:\"string\"}\n",
        "technology_requirements = \"Students will need access to a computer with internet connectivity. If software or special technology is required in one of the courses, it will be provided to students.\"  # @param {type:\"string\"}\n",
        "prerequisites = \"A Bachelor’s degree, in any discipline, from an accredited university (min. 3.0 GPA) is required. Students with a GPA of less than 3.0 may be considered for provisional admission.\"  # @param {type:\"string\"}\n",
        "\n",
        "topic = TOPIC\n",
        "field_of_study = \"Sustainability\"  # @param {type:\"string\"}\n",
        "audience = \"Those seeking to advance their career in sustainability, whether their experience is in business, healthcare, manufacturing, design, retail, or other industries that are transitioning to sustainable business models.\"  # @param {type:\"string\"}\n",
        "specific_project = \"A capstone project where each student will help a real organization solve an existing sustainability problem by implementing practical knowledge to achieve a triple-bottom-line solution.\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "0pWuC5pxgKL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Addressing Iran's Crises: Exploring Social Change Methodologies and Creating a Counter Profile\"\n",
        "RESEARCH_DOMAIN = \"Sociology\"\n",
        "PARAGRAPH = \"Iran's societal challenges are complex and require immediate attention. Engaging in research and working on solutions in this field can help address these issues. This course aims to equip students with the necessary tools and knowledge to navigate these challenges.\"\n",
        "PARAGRAPHS = PARAGRAPH\n",
        "TOPIC_SENTENCE = \"Understanding and addressing Iran's crisis is a critical aspect of sociology.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This course explores the management of Iran's crises through engagement in research and charitable work...\"\n",
        "BIBLIOGRAPHY = \"Source 0: https://www.csis.org/analysis/crisis-iran-what-now\"\n",
        "THEORY1 = \"Sociological Theory\"\n",
        "THEORY2 = \"Social Change Theory\"\n",
        "RESEARCH_QUESTIONS = \"['What are the primary causes of Iran's crises?', 'How can research and charitable work help address these issues?']\"\n",
        "ACTION = \"Research and discussion\"\n",
        "RESULT_PARAGRAPHS = \"Results indicate that engaging in research and charitable work can significantly reduce the impacts of Iran's crises...\"\n",
        "DATE = \"2023-12-30\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"One year\"\n",
        "ROLE = \"Researcher\"\n",
        "PROJECT_EXAMPLE = \"Social Change Management Project\"\n",
        "CONTEXT = \"Social Development\"\n",
        "INSTRUCTION = \"Study and research on Iran's crises and their management\"\n",
        "OUTPUT_FORMAT = \"Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"The focus of this project is to study Iran's crises and develop strategies for their management...\"\n",
        "X = \"Iran's Crises Impacts\"\n",
        "PROJECT_MANAGER = \"Your Name\"\n",
        "REPORT = \"Detailed report on Iran's crisis research\"\n",
        "IMPORTANT_THEMES = \"Social Change, Research, Charitable Work\"\n",
        "PROJECT_NAME = \"Social Change Management Study\"\n",
        "STAKEHOLDER = \"Social Scientists, Researchers\"\n",
        "RESISTANT_STAKEHOLDER = \"N/A\"\n",
        "TASK = \"Generate a report on Iran's crisis and its management\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = openai_api # \"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\"\n",
        "\n",
        "employability = \"Prepare students for employment in fields such as social sciences, human rights, and international relations.\" # @param {type:\"string\"}\n",
        "practical_skills = \"Develop practical skills in social analysis by focusing on the social, economic, and political concepts that can be applied across a wide range of sectors and industries.\" # @param {type:\"string\"}\n",
        "creativity = \"Encourage creative thinking and problem-solving in developing social strategies for organizations.\" # @param {type:\"string\"}\n",
        "critical_thinking = \"Foster critical thinking among students in analyzing social issues and formulating comprehensive solutions.\" # @param {type:\"string\"}\n",
        "fun_and_enjoyment = \"Make the course engaging with online lectures, discussions, and assignments, offering flexibility for students.\" # @param {type:\"string\"}\n",
        "employee_guarantee = \"Provide a guarantee for successful course completion by ensuring comprehensive coverage of key social topics.\" # @param {type:\"string\"}\n",
        "collaboration = \"Encourage collaboration among students in online forums and group projects.\" # @param {type:\"string\"}\n",
        "learning_outcomes = \"By the end of the course, students should be able to communicate complex social issues, analyze evidence to formulate social strategies, and lead social initiatives at local, national, and global levels.\" # @param {type:\"string\"}\n",
        "purpose = \"The purpose of this course is to equip students with the knowledge and skills necessary to advance their career in social sciences.\" # @param {type:\"string\"}\n",
        "learning_activities = \"Activities will include online lectures, discussions, assignments, and a capstone project where students help a real organization solve an existing social problem.\" # @param {type:\"string\"}\n",
        "course_content = \"The main topics covered in the course will include the relationship of humans with the natural environment, public policy and the role of government and business in social sciences, triple bottom line accounting, and social change.\" # @param {type:\"string\"}\n",
        "\n",
        "course_assessments = \"Student performance will be evaluated through assignments, group projects, and a capstone project.\" # @param {type:\"string\"}\n",
        "course_schedule = \"The course is delivered online and students can do homework whenever it’s convenient for them. The program also provides optional networking opportunities for students to connect virtually with peers, faculty, and professionals.\" # @param {type:\"string\"}\n",
        "course_sequencing = \"The course content will be sequenced to gradually increase in complexity, starting with an introduction to crisis management, moving on to more advanced topics, and ending with a capstone project.\" # @param {type:\"string\"}\n",
        "technology_requirements = \"Students will need access to a computer with internet connectivity. If software or special technology is required in one of the courses, it will be provided to students.\" # @param {type:\"string\"}\n",
        "prerequisites = \"A Bachelor’s degree, in any discipline, from an accredited university (min. 3.0 GPA) is required. Students with a GPA of less than 3.0 may be considered for provisional admission.\" # @param {type:\"string\"}\n",
        "\n",
        "topic = TOPIC\n",
        "field_of_study = \"Sociology\" # @param {type:\"string\"}\n",
        "audience = \"Those seeking to advance their career in sociology, whether their experience is in business, healthcare, manufacturing, design, retail, or other industries that are transitioning to sustainable business models.\" # @param {type:\"string\"}\n",
        "specific_project = \"A capstone project where each student will help a real organization solve an existing societal problem by implementing practical knowledge to achieve a triple-bottom-line solution.\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "uWv0Uhehdcoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Iranian B2B Electrical Marketplace Business Plan\"\n",
        "RESEARCH_DOMAIN = \"Business and Entrepreneurship\"\n",
        "PARAGRAPH = \"Your comprehensive business plan for an Iranian B2B Electrical Marketplace is well-structured and detailed. It covers key aspects such as market analysis, business model, target audience, and strategies for growth. The emphasis on a user-friendly platform, a wide range of products, and competitive pricing aligns with the current trends in online marketplaces.\\n\\nYour mission to streamline the purchasing process and promote industry growth reflects a customer-centric approach. The commission-based business model and additional premium services offer a balanced revenue strategy. It's positive to see a focus on diverse delivery methods and payment options to cater to customer preferences.\\n\\nYour commitment to regulatory compliance and obtaining necessary licenses demonstrates a thorough understanding of the business environment. The incorporation of contingency plans for potential risks adds a layer of preparedness.\\n\\nOverall, your business plan appears robust and well-thought-out. If you have specific questions or if there's an area you'd like further assistance with, feel free to let me know!\"\n",
        "PARAGRAPHS = PARAGRAPH\n",
        "TOPIC_SENTENCE = \"Revolutionizing the Iranian electrical products market through a comprehensive online B2B platform.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This business plan outlines the vision and strategies for [Company Name], aiming to transform the traditional electrical products industry in Iran. With a focus on a user-friendly interface, a diverse product range, and competitive pricing, our goal is to become the leading online B2B electrical products marketplace in the country.\"\n",
        "BIBLIOGRAPHY = \"Source 0: https://example.com/business-plan-resources\"\n",
        "THEORY1 = \"Market Analysis\"\n",
        "THEORY2 = \"B2B E-commerce Strategies\"\n",
        "RESEARCH_QUESTIONS = \"['What are the key components of a successful B2B marketplace?', 'How can regulatory compliance impact the success of an online business in Iran?']\"\n",
        "ACTION = \"Implementation and Iteration\"\n",
        "RESULT_PARAGRAPHS = \"Results indicate a positive reception from the target market, with steady growth and customer satisfaction. Continuous innovation and adaptation to market trends contribute to [Company Name]'s success.\"\n",
        "DATE = \"2024-01-04\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"Five years\"\n",
        "ROLE = \"Founder and CEO\"\n",
        "PROJECT_EXAMPLE = \"Iranian B2B Electrical Marketplace Launch\"\n",
        "CONTEXT = \"E-commerce in Iran\"\n",
        "INSTRUCTION = \"Develop and implement strategies outlined in the business plan for [Company Name].\"\n",
        "OUTPUT_FORMAT = \"Business Plan\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"This project involves establishing and growing an online B2B electrical products marketplace in Iran. [Company Name] will focus on user experience, product variety, and competitive pricing to achieve market leadership.\"\n",
        "X = \"Market Trends\"\n",
        "PROJECT_MANAGER = \"Your Name\"\n",
        "REPORT = \"Comprehensive report on the success and growth of [Company Name]\"\n",
        "IMPORTANT_THEMES = \"B2B E-commerce, Market Leadership, Regulatory Compliance\"\n",
        "PROJECT_NAME = \"Iranian B2B Electrical Marketplace Business Plan\"\n",
        "STAKEHOLDER = \"Entrepreneurs, Business Owners\"\n",
        "RESISTANT_STAKEHOLDER = \"Traditional Brick-and-Mortar Stores\"\n",
        "TASK = \"Execute the outlined strategies and monitor the growth of [Company Name].\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = \"\" #\"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\"\n",
        "\n",
        "employability = \"Improving awareness and knowledge about environmental issues\"\n",
        "practical_skills = \"Communication skills, Research skills\"\n",
        "creativity = \"Creating engaging content for the radio program\"\n",
        "critical_thinking = \"Analyzing and interpreting complex environmental data\"\n",
        "fun_and_enjoyment = \"Engaging and entertaining the audience\"\n",
        "employee_guarantee = \"A commitment to promoting environmental sustainability\"\n",
        "collaboration = \"Working together with government authorities and communities\"\n",
        "learning_outcomes = \"Understanding the importance of environmental conservation\"\n",
        "purpose = \"To educate and inspire listeners to take action on environmental issues\"\n",
        "learning_activities = \"Listening to the radio program, Participating in environmental cleanup activities\"\n",
        "course_content = \"Content on various environmental issues, Potential solutions, Real-life examples\"\n",
        "course_assessments = \"Quizzes, Listener feedback surveys\"\n",
        "course_schedule = \"Weekly broadcasts\"\n",
        "course_sequencing = \"Introduction to environmental issues, Causes and effects, Solutions and actions\"\n",
        "technology_requirements = \"Radio broadcasting equipment\"\n",
        "prerequisites = \"Basic understanding of environmental issues\"\n",
        "\n",
        "audience = \"Iranian Listeners\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "n8UW9VTWR-GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOPIC = \"Environmental Problems in Iran: A Call to Action\"\n",
        "RESEARCH_DOMAIN = \"Environmental Science\"\n",
        "# Define the variables\n",
        "\n",
        "PARAGRAPH = \"This report explores Iran's environmental crisis, presenting current statistics and proposing actionable solutions. It addresses domestic system irresponsibility, investigates historical approaches, and highlights women's compassionate perspective. Exploring new technologies, emphasizing Iranian activism, and media coverage are crucial. The role of AI technology and adherence to the Good Samaritan law are stressed. This structure aims to provide a comprehensive overview and potential solutions.\"\n",
        "\n",
        "PARAGRAPHS = PARAGRAPH + \"Additionally, Iran's rapid urbanization and demographic changes have exacerbated these issues, placing greater ecological stress on its urban areas. With more than 70% of Iran's population now residing in cities, the environmental crisis has become a matter of urgency.\"\n",
        "TOPIC_SENTENCE = \"Join us today as we delve deeper into Iran's environmental crisis and explore potential solutions.\"\n",
        "LANGUAGE = \"Persian\"\n",
        "ABSTRACT_PARAGRAPH = \"This radio program aims to raise awareness among Iranians about the urgent environmental problems our country is facing. We'll discuss the current situation, propose actionable solutions, and highlight the role of Iranian activists, companies, and citizens in preserving our environment.\"\n",
        "BIBLIOGRAPHY = \"Refer to 'Iran's Environmental Crisis: A Comprehensive Report' for more details.\"\n",
        "THEORY1 = \"Conservation Theory\"\n",
        "THEORY2 = \"Sustainability Theory\"\n",
        "RESEARCH_QUESTIONS = [\"What are the major environmental problems in Iran?\", \"What are the causes of these problems?\", \"What are the potential solutions?\"]\n",
        "ACTION = \"Listen to this program to learn more about Iran's environmental problems and ways to address them.\"\n",
        "RESULT_PARAGRAPHS = \"By the end of this program, listeners will have a better understanding of the environmental issues in Iran and steps they can take to contribute to their solution.\"\n",
        "DATE = \"01/04/2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"1 day\"\n",
        "ROLE = \"climate change and environmental critics management\"\n",
        "PROJECT_EXAMPLE = \"An environmental cleanup project in a local park\"\n",
        "CONTEXT = \"Iranian Society\"\n",
        "INSTRUCTION = \"Listen attentively to understand the importance of addressing environmental issues.\"\n",
        "OUTPUT_FORMAT = \"Audio Broadcast\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"A project to promote recycling in schools across Iran\"\n",
        "X = \"Number of listeners reached\"\n",
        "PROJECT_MANAGER = \"Project Manager's Name\"\n",
        "REPORT = \"Report on the effectiveness of the environmental cleanup project\"\n",
        "IMPORTANT_THEMES = \"Environmental Conservation, Sustainability, Community Engagement\"\n",
        "PROJECT_NAME = \"Environmental Cleanup Project\"\n",
        "STAKEHOLDER = \"Local Schools, Government Authorities, Local Communities\"\n",
        "RESISTANT_STAKEHOLDER = \"Individuals resistant to change\"\n",
        "TASK = \"Raise awareness about environmental issues\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = openai_api #\"OpenAI API Key\"\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Improving awareness and knowledge about environmental issues\"\n",
        "practical_skills = \"Communication skills, Research skills\"\n",
        "creativity = \"Creating engaging content for the radio program\"\n",
        "critical_thinking = \"Analyzing and interpreting complex environmental data\"\n",
        "fun_and_enjoyment = \"Engaging and entertaining the audience\"\n",
        "employee_guarantee = \"A commitment to promoting environmental sustainability\"\n",
        "collaboration = \"Working together with government authorities and communities\"\n",
        "learning_outcomes = \"Understanding the importance of environmental conservation\"\n",
        "purpose = \"To educate and inspire listeners to take action on environmental issues\"\n",
        "learning_activities = \"Listening to the radio program, Participating in environmental cleanup activities\"\n",
        "course_content = \"Content on various environmental issues, Potential solutions, Real-life examples\"\n",
        "course_assessments = \"Quizzes, Listener feedback surveys\"\n",
        "course_schedule = \"Weekly broadcasts\"\n",
        "course_sequencing = \"Introduction to environmental issues, Causes and effects, Solutions and actions\"\n",
        "technology_requirements = \"Radio broadcasting equipment\"\n",
        "prerequisites = \"Basic understanding of environmental issues\"\n",
        "\n",
        "audience = \"Iranian Listeners\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "C7ayzKdKjuDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Embracing the Power of Technology: A $1 Trillion Opportunity for Iran\"\n",
        "RESEARCH_DOMAIN = \"Computer Science\"\n",
        "PARAGRAPH = \"The global waste management market, valued at approximately $1.3 trillion, poses significant opportunities for innovation and development. In particular, Iran has the potential to leverage technology to address waste management challenges and contribute to a more sustainable and equitable economy.\"\n",
        "PARAGRAPHS = \"\"\n",
        "TOPIC_SENTENCE = \"The proposed waste management Android application aims to facilitate resource recovery and promote the circular economy, leveraging technological advancements in the waste management industry.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"The proposed waste management Android application provides a comprehensive platform for waste collection, disposal, and resource recovery. It uses advanced technologies to optimize waste management processes and provide a user-friendly experience for both individuals and waste management companies.\"\n",
        "BIBLIOGRAPHY = \"Refer to 'Embracing the Power of Technology: A $1 Trillion Opportunity for Iran' for more details.\"\n",
        "THEORY1 = \"Computer Science\"\n",
        "THEORY2 = \"Environmental Science\"\n",
        "RESEARCH_QUESTIONS = [\"How does the proposed waste management Android application optimize waste management processes?\", \"What role does technology play in the promotion of the circular economy?\", \"How does the application promote social responsibility among businesses and governments?\"]\n",
        "ACTION = \"Develop the waste management Android application to optimize waste management processes and promote a circular economy.\"\n",
        "RESULT_PARAGRAPHS = \"By the end of this research, we aim to develop a comprehensive platform for waste collection, disposal, and resource recovery that leverages advanced technologies and promotes social responsibility.\"\n",
        "DATE = \"01/15/2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"3 months\"\n",
        "ROLE = \"Researcher\"\n",
        "PROJECT_EXAMPLE = \"Development of a waste management Android application\"\n",
        "CONTEXT = \"Global Waste Management Market\"\n",
        "INSTRUCTION = \"Read carefully to understand the significance of the research topic and the methodology used.\"\n",
        "OUTPUT_FORMAT = \"Research Paper\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"A research project to develop a waste management Android application to optimize waste management processes and promote a circular economy.\"\n",
        "X = \"Number of waste management Android applications developed\"\n",
        "PROJECT_MANAGER = \"Researcher's Name\"\n",
        "REPORT = \"Report on the findings of the research paper\"\n",
        "IMPORTANT_THEMES = \"Waste Management, Circular Economy, Social Responsibility\"\n",
        "PROJECT_NAME = \"Research Project on Waste Management Android Application\"\n",
        "STAKEHOLDER = \"Businesses, Governments\"\n",
        "RESISTANT_STAKEHOLDER = \"Those resistant to change in waste management practices\"\n",
        "TASK = \"Develop a waste management Android application\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Developing innovative solutions for waste management\"\n",
        "practical_skills = \"Mobile app development, Data analysis skills\"\n",
        "creativity = \"Designing user-friendly interfaces, Formulating research questions\"\n",
        "critical_thinking = \"Analyzing and interpreting complex waste management data\"\n",
        "fun_and_enjoyment = \"Engaging in intellectual discourse\"\n",
        "employee_guarantee = \"A commitment to conducting thorough and unbiased research\"\n",
        "collaboration = \"Collaborating with other researchers and scholars\"\n",
        "learning_outcomes = \"Understanding the factors contributing to effective waste management and potential strategies to overcome these barriers\"\n",
        "purpose = \"To investigate the issue of waste management and propose innovative solutions\"\n",
        "learning_activities = \"Reading research papers, Participating in academic discussions\"\n",
        "course_content = \"Content on various computer science theories, Current state of affairs, Historical context\"\n",
        "course_assessments = \"Literature reviews, Research proposals\"\n",
        "course_schedule = \"Monthly online sessions\"\n",
        "course_sequencing = \"Introduction to the research topic, Factors contributing to waste management, Case studies\"\n",
        "technology_requirements = \"Access to research databases, Internet access\"\n",
        "prerequisites = \"Basic understanding of computer science and environmental science\"\n",
        "audience = \"Researchers, Scholars, Policy Makers\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "CzJOAvE-0uVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"OpenAI's Democratic Rules and Light Triad Personality Traits for Iran\"\n",
        "RESEARCH_DOMAIN = \"Artificial Intelligence\"\n",
        "PARAGRAPH = \"OpenAI has been at the forefront of shaping a 'democratic process' for governing AI. This process entails a diverse group of individuals exchanging opinions, participating in deliberative discussions, and collectively deciding outcomes transparently [Source 0](https://openai.com/blog/democratic-inputs-to-ai). The initiative, highlighted by OpenAI's grant program, aims to distribute ten $100,000 grants for experiments establishing a democratic framework within legal boundaries [Source 0](https://openai.com/blog/democratic-inputs-to-ai). Addressing issues undermining democratic processes, such as representation gaps, manipulation, and participation challenges, is central to this program [Source 0](https://openai.com/blog/democratic-inputs-to-ai). Its overarching objective is to propose statements fostering agreement among individuals with differing views, utilizing AI for efficient communication [Source 0](https://openai.com/blog/democratic-inputs-to-ai). While some express concerns about AI's role in policy writing and its opaque application in democratic processes, many participants gain optimism about the public's ability to guide AI responsibly [Source 1](https://openai.com/blog/democratic-inputs-to-ai-grant-program-update). In conclusion, OpenAI endeavors toward a democratic AI governance approach, engaging the public in determining AI behavior aligned with human values.\"\n",
        "PARAGRAPHS = PARAGRAPH\n",
        "TOPIC_SENTENCE = \"The proposed democratic system for Iran would leverage OpenAI's models to gather insights from a broad range of Iranian citizens, taking into account their views and opinions on various issues.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This approach would not only promote a more inclusive and representative form of governance but also foster a society that values empathy, altruism, and compassion.\"\n",
        "BIBLIOGRAPHY = \"Refer to 'OpenAI's Democratic Rules and Light Triad Personality Traits for Iran' for more details.\"\n",
        "THEORY1 = \"Artificial Intelligence\"\n",
        "THEORY2 = \"Psychology\"\n",
        "RESEARCH_QUESTIONS = [\"How does OpenAI's democratic process work?\", \"What role does AI play in decision-making processes?\", \"How do the light triad personality traits influence society?\"]\n",
        "ACTION = \"Develop a democratic system for Iran leveraging OpenAI's models.\"\n",
        "RESULT_PARAGRAPHS = \"By the end of this research, we aim to propose a new version of a democratic system for Iran that leverages OpenAI's models to gather insights from a broad range of Iranian citizens.\"\n",
        "DATE = \"01/19/2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"3 months\"\n",
        "ROLE = \"Researcher\"\n",
        "PROJECT_EXAMPLE = \"Development of a democratic system for Iran\"\n",
        "CONTEXT = \"Current state of AI and Psychology in Iran\"\n",
        "INSTRUCTION = \"Read carefully to understand the significance of the research topic and the methodology used.\"\n",
        "OUTPUT_FORMAT = \"Research Paper\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"A research project to develop a democratic system for Iran leveraging OpenAI's models.\"\n",
        "X = \"Number of democratic systems developed\"\n",
        "PROJECT_MANAGER = \"Researcher's Name\"\n",
        "REPORT = \"Report on the findings of the research paper\"\n",
        "IMPORTANT_THEMES = \"Artificial Intelligence, Democracy, Light Triad Personality Traits\"\n",
        "PROJECT_NAME = \"Research Project on Democracy System for Iran\"\n",
        "STAKEHOLDER = \"Government, Citizens\"\n",
        "RESISTANT_STAKEHOLDER = \"Those resistant to change in governance practices\"\n",
        "TASK = \"Develop a democratic system for Iran\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Developing innovative solutions for governance\"\n",
        "practical_skills = \"AI development, Data analysis skills\"\n",
        "creativity = \"Designing user-friendly interfaces, Formulating research questions\"\n",
        "critical_thinking = \"Analyzing and interpreting complex AI and psychology data\"\n",
        "fun_and_enjoyment = \"Engaging in intellectual discourse\"\n",
        "employee_guarantee = \"A commitment to conducting thorough and unbiased research\"\n",
        "collaboration = \"Collaborating with other researchers and scholars\"\n",
        "learning_outcomes = \"Understanding the factors contributing to effective governance and potential strategies to overcome these barriers\"\n",
        "purpose = \"To investigate the issue of governance and propose innovative solutions\"\n",
        "learning_activities = \"Reading research papers, Participating in academic discussions\"\n",
        "course_content = \"Content on various AI and psychology theories, Current state of affairs, Historical context\"\n",
        "course_assessments = \"Literature reviews, Research proposals\"\n",
        "course_schedule = \"Monthly online sessions\"\n",
        "course_sequencing = \"Introduction to the research topic, Factors contributing to governance, Case studies\"\n",
        "technology_requirements = \"Access to research databases, Internet access\"\n",
        "prerequisites = \"Basic understanding of AI and psychology\"\n",
        "audience = \"Researchers, Scholars, Policy Makers\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "Kxp5PkwEw1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"OpenAI Research Engineer, Collective Alignment Job Opportunity\"\n",
        "RESEARCH_DOMAIN = \"Artificial Intelligence\"\n",
        "PARAGRAPH = \"OpenAI has recently opened a position for a Research Engineer in Collective Alignment.\"\n",
        "PARAGRAPHS = \"This role is part of the Collective Alignment Team, which works on shaping the future of technology by ensuring that Artificial General Intelligence (AGI) is democratically shaped.\"\n",
        "TOPIC_SENTENCE = \"This job opportunity presents a unique opportunity for Iranian women and men.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This job opportunity is a call to action for Iranian women and men interested in making a difference in the field of AI.\"\n",
        "BIBLIOGRAPHY = \"[Source 0](https://openai.com/careers/research-engineer-collective-alignment)\"\n",
        "THEORY1 = \"Artificial General Intelligence (AGI)\"\n",
        "THEORY2 = \"Democratizing AI Governance\"\n",
        "RESEARCH_QUESTIONS = [\"What are the requirements for the Research Engineer in Collective Alignment role?\", \"How can Iranian women and men contribute to the OpenAI GitHub repository?\"]\n",
        "ACTION = \"Join OpenAI's team or contribute to the OpenAI GitHub repository\"\n",
        "RESULT_PARAGRAPHS = \"By joining OpenAI's team or contributing to the OpenAI GitHub repository, Iranian women and men can contribute to the development of a more democratic and fair AI system, and play a significant role in Iran's democratic transformation.\"\n",
        "DATE = \"January 21, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"21 days, 1 month, 1 year\"\n",
        "ROLE = \"Research Engineer in Collective Alignment\"\n",
        "PROJECT_EXAMPLE = \"Developing a system for collecting and encoding public input on model behavior into their systems\"\n",
        "CONTEXT = \"OpenAI's initiative to democratize AI governance\"\n",
        "INSTRUCTION = \"Write a blog post about the job opportunity and the needs of Iranian women and men to work by OpenAI team\"\n",
        "OUTPUT_FORMAT = \"Blog Post\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Collective Alignment Team, Research Engineer in Collective Alignment\"\n",
        "X = \"100,000\"\n",
        "PROJECT_MANAGER = \"OpenAI\"\n",
        "REPORT = \"Report on the progress of the project\"\n",
        "IMPORTANT_THEMES = \"AI, Democracy, Iranian Women and Men, OpenAI\"\n",
        "PROJECT_NAME = \"OpenAI Research Engineer, Collective Alignment Project\"\n",
        "STAKEHOLDER = \"Iranian Women and Men\"\n",
        "RESISTANT_STAKEHOLDER = \"Unnamed\"\n",
        "TASK = \"Write a blog post about the job opportunity and the needs of Iranian women and men to work by OpenAI team\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Strong curiosity about the sociotechnical challenges around aligning and understanding Machine Learning models\"\n",
        "practical_skills = \"Experience implementing Machine Learning algorithms, developing data visualization or data collection interfaces\"\n",
        "creativity = \"Excitement about OpenAI's mission of building safe, universally beneficial AGI\"\n",
        "critical_thinking = \"Ability to use one's career to address sociotechnical challenges\"\n",
        "fun_and_enjoyment = \"Enjoyment of a fast-paced, collaborative, and cutting-edge research environment\"\n",
        "employee_guarantee = \"Total compensation that includes generous equity and benefits\"\n",
        "collaboration = \"Collaboration with the selected teams in the OpenAI GitHub repository\"\n",
        "learning_outcomes = \"Valuable experience, making connections with other professionals, potentially influencing the direction of AI research\"\n",
        "purpose = \"To promote peace and reduce internal conflicts, and enhance peacebuilding efforts and create a more empathetic and compassionate society.\"\n",
        "learning_activities = \"Contributing to the development of a more democratic and fair AI system\"\n",
        "course_content = \"AI and Democracy\"\n",
        "course_assessments = \"Impact on Iran's democratic transformation\"\n",
        "course_schedule = \"Immediate\"\n",
        "course_sequencing = \"Sequential\"\n",
        "technology_requirements = \"Machine Learning Algorithms, Data Visualization or Data Collection Interfaces\"\n",
        "prerequisites = \"Interest in the field of AI\"\n",
        "\n",
        "audience = \"Iranian Women and Men\"\n",
        "topic = \"AI and Democracy\"\n",
        "field_of_study = \"Artificial Intelligence\"\n",
        "specific_project = \"OpenAI Research Engineer, Collective Alignment Project\""
      ],
      "metadata": {
        "id": "rKj5XvASeyrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Sustainable Harmony: A Trillion-Dollar Opportunity for Taylor Swift and Her Influential Community\"\n",
        "RESEARCH_DOMAIN = \"Environmental Sustainability, Waste Management, Artificial Intelligence\"\n",
        "PARAGRAPHS = \"In a world clamoring for change, we're unveiling an extraordinary initiative that beckons the attention of Taylor Swift, her famed peers, and the Swiftie community. 🌍✨ Join us on this transformative journey as we present a visionary waste management project in Iran, merging artificial intelligence, profitability, and environmental consciousness in perfect harmony.\"\n",
        "PARAGRAPH = \"\"\"\n",
        "**Project Highlights:**\n",
        "✅ **AI Revolution:** Unleashing the power of AI for optimized waste management, setting a new standard for environmental stewardship.\n",
        "✅ **Empowerment Through Profit:** A revolutionary fusion of communism, capitalism, and green principles, ensuring profitability while giving back to users.\n",
        "✅ **Endorsement by Academia:** Validated by academic papers, successful reports, and a groundbreaking model founded on empathy and compassion.\n",
        "\n",
        "**Swifties, Your Voice is the Amplifier!**\n",
        "Calling upon Taylor Swift, esteemed signers, and her vast community! Your endorsement can catapult this project, addressing environmental concerns while championing social causes like LGBT rights, women's rights, and fighting racism and police brutality.\n",
        "\n",
        "**Funding for Transformation:**\n",
        "To initiate this vision, we're considering selling a house in Iran, investing approximately $70,000 as a symbol of dedication. This endeavor isn't just a project; it's a movement toward a sustainable future.\n",
        "\n",
        "**Join the Symphony, Seize the Opportunity:**\n",
        "This isn't merely waste management; it's a trillion-dollar opportunity to create a global community-driven company, echoing the success of Uber but in waste management. Imagine a world where daily profits return to users, and first-year revenue is shared with others. Be part of this eco-harmony, shaping a sustainable economy.\n",
        "\n",
        "**Time is of the Essence:**\n",
        "As environmental crises intensify, we implore you to expedite the process. Check the provided information, and if it aligns with your values, help us spread the word. Together, let's turn this opportunity into a reality.\n",
        "\n",
        "**Swifties, Be the Change, Shape Tomorrow! 🌱💚**\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"This job opportunity presents a unique opportunity for Iranian women and men.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This job opportunity is a call to action for Iranian women and men interested in making a difference in the field of AI.\"\n",
        "BIBLIOGRAPHY = \"[Source 0](https://openai.com/careers/research-engineer-collective-alignment)\"\n",
        "THEORY1 = \"Artificial General Intelligence (AGI)\"\n",
        "THEORY2 = \"Democratizing AI Governance\"\n",
        "RESEARCH_QUESTIONS = [\"What are the requirements for the Research Engineer in Collective Alignment role?\", \"How can Iranian women and men contribute to the OpenAI GitHub repository?\"]\n",
        "ACTION = \"Join OpenAI's team or contribute to the OpenAI GitHub repository\"\n",
        "RESULT_PARAGRAPHS = \"By joining OpenAI's team or contributing to the OpenAI GitHub repository, Iranian women and men can contribute to the development of a more democratic and fair AI system, and play a significant role in Iran's democratic transformation.\"\n",
        "DATE = \"January 21, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"21 days, 1 month, 1 year\"\n",
        "ROLE = \"Research Engineer in Collective Alignment\"\n",
        "PROJECT_EXAMPLE = \"Developing a system for collecting and encoding public input on model behavior into their systems\"\n",
        "CONTEXT = \"OpenAI's initiative to democratize AI governance\"\n",
        "INSTRUCTION = \"Write a blog post about the job opportunity and the needs of Iranian women and men to work by OpenAI team\"\n",
        "OUTPUT_FORMAT = \"Blog Post\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Collective Alignment Team, Research Engineer in Collective Alignment\"\n",
        "X = \"100,000\"\n",
        "PROJECT_MANAGER = \"OpenAI\"\n",
        "REPORT = \"Report on the progress of the project\"\n",
        "IMPORTANT_THEMES = \"AI, Democracy, Iranian Women and Men, OpenAI\"\n",
        "PROJECT_NAME = \"OpenAI Research Engineer, Collective Alignment Project\"\n",
        "STAKEHOLDER = \"Iranian Women and Men\"\n",
        "RESISTANT_STAKEHOLDER = \"Unnamed\"\n",
        "TASK = \"Write a blog post about the job opportunity and the needs of Iranian women and men to work by OpenAI team\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Strong curiosity about the sociotechnical challenges around aligning and understanding Machine Learning models\"\n",
        "practical_skills = \"Experience implementing Machine Learning algorithms, developing data visualization or data collection interfaces\"\n",
        "creativity = \"Excitement about OpenAI's mission of building safe, universally beneficial AGI\"\n",
        "critical_thinking = \"Ability to use one's career to address sociotechnical challenges\"\n",
        "fun_and_enjoyment = \"Enjoyment of a fast-paced, collaborative, and cutting-edge research environment\"\n",
        "employee_guarantee = \"Total compensation that includes generous equity and benefits\"\n",
        "collaboration = \"Collaboration with the selected teams in the OpenAI GitHub repository\"\n",
        "learning_outcomes = \"Valuable experience, making connections with other professionals, potentially influencing the direction of AI research\"\n",
        "purpose = \"To promote peace and reduce internal conflicts, and enhance peacebuilding efforts and create a more empathetic and compassionate society.\"\n",
        "learning_activities = \"Contributing to the development of a more democratic and fair AI system\"\n",
        "course_content = \"AI and Democracy\"\n",
        "course_assessments = \"Impact on Iran's democratic transformation\"\n",
        "course_schedule = \"Immediate\"\n",
        "course_sequencing = \"Sequential\"\n",
        "technology_requirements = \"Machine Learning Algorithms, Data Visualization or Data Collection Interfaces\"\n",
        "prerequisites = \"Interest in the field of AI\"\n",
        "\n",
        "audience = \"Iranian Women and Men\"\n",
        "topic = \"AI and Democracy\"\n",
        "field_of_study = \"Artificial Intelligence\"\n",
        "specific_project = \"OpenAI Research Engineer, Collective Alignment Project\""
      ],
      "metadata": {
        "id": "_-MGrr3EmjCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated variables with specific details, political prevention, and anti-fascism emphasis\n",
        "TOPIC = \"Combating Political Bullying, Proposing Online Courts for Accountability, and Resisting Fascist Governments\"\n",
        "RESEARCH_DOMAIN = \"Political Bullying, Online Courts, Anti-Fascism Strategies\"\n",
        "PARAGRAPHS = \"In my research, I've uncovered effective techniques for combating political bullying, resisting fascist governments, and propose the implementation of online courts for accountability in Iran. This initiative aligns with proven anti-bullying strategies, draws inspiration from the use of online courts in history, and emphasizes the resistance against fascist ideologies.\"\n",
        "PARAGRAPH = \"\"\"\n",
        "1. **Fast Action Against Bullying**: Implement clear rules, seek expert approval, and enforce consequences for violators.\n",
        "2. **Changing the Environment**: Promote positive behaviors, encourage peer interactions, and foster a sense of belonging.\n",
        "3. **Accountability for Parents and Government**: Encourage responsibility among parents and government officials, aligning with community involvement.\n",
        "4. **Warning and Educational Programs**: Raise awareness about political bullying, foster respect, and actively engage citizens in educational programs.\n",
        "Mirroring anti-bullying techniques: Ensure a safe environment, establish clear rules, promote accountability, and prioritize political education.\n",
        "\n",
        "Aligned with Multi-Tiered System of Supports (MTSS) framework:\n",
        "- **Clear Rules and Guidelines**: Define bullying, outline consequences.\n",
        "- **Safe Environment**: Prevent bullying, positive behaviors, belonging.\n",
        "- **Accountability**: Encourage responsibility, align with community.\n",
        "- **Educational Programs**: Raise awareness, promote respect and empathy.\n",
        "\n",
        "Implementing these strategies involves creating a fast action through online courts, inspired by post-WWII accountability measures. The plan aligns with Iranian context for online courts as a startup, including reports like project management, business plans, and media scripts. Learning from potential failures is crucial, recognizing the value of time.\n",
        "The first post about virtual courts to stop Iranian Islamofascist government bullying and resisting fascist ideologies is detailed below:\n",
        "\"\"\"\n",
        "\n",
        "TOPIC_SENTENCE = \"Addressing political bullying, resisting fascist ideologies, and proposing online courts for accountability in Iran.\"\n",
        "ABSTRACT_PARAGRAPH = \"Exploring effective anti-political-bullying strategies, advocating against fascist governments, and proposing the implementation of online courts to address government bullying in Iran.\"\n",
        "THEORY1 = \"Anti-Political-Bullying Strategies and Resistance Against Fascism\"\n",
        "THEORY2 = \"Online Courts for Political Accountability\"\n",
        "RESEARCH_QUESTIONS = [\"What are the most effective anti-political-bullying strategies?\", \"How can online courts be implemented to address government bullying in Iran?\", \"What strategies can be employed to resist fascist ideologies in government?\"]\n",
        "ACTION = \"Proposing online courts for accountability in the Iranian government, advocating for effective anti-political-bullying strategies, and resisting fascist ideologies.\"\n",
        "RESULT_PARAGRAPHS = \"The proposed action aims to bring attention to political bullying issues, resist fascist ideologies, and hold the Iranian government accountable through the implementation of online courts.\"\n",
        "PROJECT_EXAMPLE = \"Implementation of online courts for political accountability in Iran and resistance against fascist ideologies\"\n",
        "CONTEXT = \"Addressing political bullying issues, resisting fascist ideologies, and government accountability through strategic initiatives\"\n",
        "INSTRUCTION = \"Write a blog post discussing effective anti-political-bullying strategies, proposing online courts for accountability in the Iranian government, and advocating for resistance against fascist ideologies.\"\n",
        "OUTPUT_FORMAT = \"Blog Post\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Online Courts Proposal, Anti-Political-Bullying Advocacy, Resistance Against Fascism\"\n",
        "PROJECT_NAME = \"Political Bullying Prevention, Government Accountability, and Anti-Fascism Initiative\"\n",
        "STAKEHOLDER = \"Citizens of Iran, Anti-Fascist Advocates\"\n",
        "RESISTANT_STAKEHOLDER = \"Unresponsive Government Officials, Supporters of Fascist Ideologies\"\n",
        "REPORT = \"Blog post on effective anti-political-bullying strategies, resistance against fascist ideologies, and the proposal for online courts in the Iranian political context.\"\n",
        "# Additional variables (filled based on the provided context)\n",
        "employability = \"Strong curiosity about sociopolitical challenges around political bullying, government accountability, and resistance against fascist ideologies\"\n",
        "practical_skills = \"Experience in researching and proposing strategic initiatives in a political context, advocating against fascist ideologies\"\n",
        "creativity = \"Excitement about finding innovative solutions to societal and political challenges, creative strategies for resistance\"\n",
        "critical_thinking = \"Ability to analyze and propose effective anti-political-bullying strategies, government accountability measures, and strategies for resisting fascist ideologies\"\n",
        "fun_and_enjoyment = \"Enjoyment of advocating for positive sociopolitical change, and resistance against oppressive ideologies\"\n",
        "employee_guarantee = \"Commitment to promoting a politically safe, inclusive environment, and actively resisting fascist ideologies through proposed initiatives\"\n",
        "collaboration = \"Collaboration with citizens, advocacy groups, and anti-fascist organizations to implement proposed strategies\"\n",
        "learning_outcomes = \"Valuable experience in addressing sociopolitical issues, making connections with advocacy groups and anti-fascist organizations, and potentially influencing political policies\"\n",
        "purpose = \"To contribute to creating a politically safer, more accountable society, and actively resist oppressive ideologies through innovative initiatives.\"\n",
        "learning_activities = \"Researching effective anti-political-bullying strategies, proposing online courts, advocating against fascist ideologies, and creative strategies for resistance.\"\n",
        "course_content = \"Sociopolitical Challenges, Innovative Solutions, Resistance Against Fascism\"\n",
        "course_assessments = \"Impact on sociopolitical change, government accountability, and resistance against oppressive ideologies\"\n",
        "course_schedule = \"Immediate\"\n",
        "course_sequencing = \"Sequential\"\n",
        "technology_requirements = \"Digital communication tools, online research platforms\"\n",
        "prerequisites = \"Interest in sociopolitical issues, advocacy, proposing strategic initiatives, and actively resisting oppressive ideologies\""
      ],
      "metadata": {
        "id": "uUvE53C7OR_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"The Nurturing Empowerment: A Holistic Approach for the Iranian Mahsa Amini Movement\"\n",
        "RESEARCH_DOMAIN = \"Sociology, Gender Studies, Advocacy\"\n",
        "PARAGRAPHS = \"In the quest for women's rights and empowerment, the Iranian Mahsa Amini Movement, under the leadership of the renowned poet Mahsa Amini, serves as a beacon of change. However, the passionate advocates within this movement often grapple with the challenges of compassion fatigue. To sustain their impactful work, a comprehensive strategy is essential.\"\n",
        "PARAGRAPH = \"\"\"\n",
        "To bolster the impact of the movement, it's crucial to implement actionable steps:\n",
        "\n",
        "1. **Advocacy for Women's Rights:**\n",
        "   - Raise awareness about pressing women's rights issues in Iran.\n",
        "\n",
        "2. **Community Engagement:**\n",
        "   - Foster community involvement and garner support for the movement.\n",
        "\n",
        "3. **Educational Initiatives:**\n",
        "   - Implement educational programs to empower women with knowledge and skills.\n",
        "\n",
        "4. **Legal Support:**\n",
        "   - Provide legal assistance to women facing challenges and injustices.\n",
        "\n",
        "5. **Mental Health Resources:**\n",
        "   - Establish dedicated resources for mental health support within the movement.\n",
        "\n",
        "Embrace the provided checklists as a roadmap to fortify yourself and the Mahsa Amini Movement against compassion fatigue. By prioritizing self-care, taking strategic actions, and implementing workplace strategies, we pave the way for sustained empowerment, resilience, and impactful advocacy for women's rights.\n",
        "\n",
        "Together, let us nurture a culture of well-being and empowerment within the Iranian Mahsa Amini Movement. 🌟 #Empowerment #CompassionFatigue #WomenRights #MahsaAminiMovement\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"In the face of catastrophic situations, both the army and health services have developed coping strategies to address compassion fatigue.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This research explores the strategies for resilience and empowerment within the Iranian Mahsa Amini Movement. It delves into self-care checklists, workplace strategies, positive coping techniques, and actionable steps to nurture a culture of well-being and empowerment.\"\n",
        "BIBLIOGRAPHY = \"[Source 0](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6456805/), [Source 1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4924075/), [Source 2](https://agentgpt.reworkd.ai/agent?id=clrx5uefh00vpjx08u9nnmf82)\"\n",
        "THEORY1 = \"Compassion Fatigue\"\n",
        "THEORY2 = \"Empowerment Strategies\"\n",
        "RESEARCH_QUESTIONS = [\"How effective have these programs been in reducing compassion fatigue among military caregivers?\", \"Are there any potential risks or negative side effects associated with these programs?\"]\n",
        "ACTION = \"Implement strategies for resilience and empowerment within the Iranian Mahsa Amini Movement.\"\n",
        "RESULT_PARAGRAPHS = \"The strategies for resilience and empowerment can significantly reduce compassion fatigue and enhance the effectiveness of advocacy for women's rights. However, it's crucial to monitor and evaluate these programs regularly to ensure they are meeting their intended goals and not exacerbating existing issues.\"\n",
        "DATE = \"January 28, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"0 days, 0 months, 0 years\" # Update with the correct values\n",
        "ROLE = \"Researcher in Sociology and Gender Studies\"\n",
        "PROJECT_EXAMPLE = \"Studying the Role of Compassion Fatigue in the Iranian Mahsa Amini Movement\"\n",
        "CONTEXT = \"Sociological research on gender issues and advocacy\"\n",
        "INSTRUCTION = \"Write a research paper on the strategies for resilience and empowerment within the Iranian Mahsa Amini Movement.\"\n",
        "OUTPUT_FORMAT = \"Research Paper\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Sociology Research, Study on Compassion Fatigue and Empowerment Strategies\"\n",
        "X = \"Not applicable\"\n",
        "PROJECT_MANAGER = \"Lead Researcher\"\n",
        "REPORT = \"Research Paper on the Role of Compassion Fatigue in the Iranian Mahsa Amini Movement\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Understanding sociology and gender studies, analyzing compassion fatigue, strong research skills\"\n",
        "practical_skills = \"Data collection and analysis, conducting sociological experiments\"\n",
        "creativity = \"Developing innovative approaches to study the strategies for resilience and empowerment\"\n",
        "critical_thinking = \"Analyzing complex sociological phenomena and their implications, evaluating the effectiveness of different strategies for resilience and empowerment, synthesizing information from various sources to draw meaningful conclusions.\"\n",
        "\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"0 days, 0 months, 0 years\" # Update with the correct values\n",
        "\n",
        "# Additional variables\n",
        "purpose = \"To explore the role of compassion fatigue in the Iranian Mahsa Amini Movement and develop strategies for resilience and empowerment.\"\n",
        "learning_outcomes = \"Understanding of compassion fatigue, ability to develop and implement resilience and empowerment strategies, enhanced research skills in sociology and gender studies.\"\n",
        "course_content = \"Introduction to sociology and gender studies, Overview of compassion fatigue, Study of resilience and empowerment strategies, Case studies of the Iranian Mahsa Amini Movement.\"\n",
        "course_assessments = \"Research paper on the topic, Presentation of research findings, Group discussions on the topic.\"\n",
        "course_schedule = \"Weekly lectures and discussions, Weekly reading assignments, Monthly assessments.\"\n",
        "course_sequencing = \"Sequential course structure with weekly topics, Regular quizzes and assessments, Final project on the topic.\"\n",
        "technology_requirements = \"Access to academic databases for research, Access to internet for online resources, Personal computer or laptop for writing and presenting.\"\n",
        "prerequisites = \"Basic knowledge in sociology and gender studies, Good command of English, Ability to perform independent research.\"\n",
        "audience = \"Students pursuing courses in sociology, gender studies, or related fields, Professionals interested in women's rights and empowerment, General public interested in social issues.\"\n",
        "topic = \"Compassion Fatigue and Empowerment Strategies in the Iranian Mahsa Amini Movement\"\n",
        "field_of_study = \"Sociology, Gender Studies\"\n",
        "specific_project = \"Study of the Iranian Mahsa Amini Movement and its strategies for resilience and empowerment\""
      ],
      "metadata": {
        "id": "qwvDi0I0aSWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"The Hypothesis of Self-Destructive Behavior as an Attempt to Undermine the Dark Triad Personality Traits\"\n",
        "RESEARCH_DOMAIN = \"Psychology, Personality Traits, Behavioral Interventions\"\n",
        "PARAGRAPHS = \"Self-destructive behavior has intrigued researchers and psychologists for decades, with varying theories attempting to explain its causes and implications. This report discusses the hypothesis that self-destructive behavior may be an attempt to undermine the dark triad personality traits of narcissism, Machiavellianism, and psychopathy. By compiling relevant information, this report sheds light on the prevalence and causes of self-destructive behavior, while also exploring potential interventions to redirect individuals towards positive patterns of behavior such as empathy, friendship, and compassion.\"\n",
        "\n",
        "PARAGRAPH = \"\"\"\n",
        "## Prevalence and Causes of Self-Destructive Behavior:\n",
        "\n",
        "1. **Prevalence:**\n",
        "   - Existing research and statistics indicate that self-destructive behavior is observed among children, adolescents, and even physically ill elderly patients [2] [5].\n",
        "   - Influencing factors include childhood trauma, mental health conditions, and a lack of healthy coping mechanisms [1].\n",
        "\n",
        "2. **Causes:**\n",
        "   - Self-destructive behavior may be an attempt to counteract or undermine the negative characteristics associated with the dark triad traits [1].\n",
        "   - Contributing factors include misusing substances, trauma, social isolation, and exposure to self-destructive behavior in friends or family [1] [4].\n",
        "\n",
        "## The Hypothesis: Self-Destructive Behavior as an Attempt to Undermine the Dark Triad Traits:\n",
        "\n",
        "1. **Explanation of the hypothesis:**\n",
        "   - Individuals with dark triad traits engage in self-destructive behavior as a subconscious effort to undermine their dominant personality traits [1].\n",
        "   - Self-destructive acts may serve as a coping mechanism or a form of self-sabotage to counterbalance the negative impact of these traits.\n",
        "\n",
        "2. **Supporting evidence:**\n",
        "   - Empirical research findings demonstrate a correlation between dark triad personality traits and self-destructive tendencies [1].\n",
        "   - Case studies and psychological experiments showcase individuals exhibiting self-destructive behavior due to narcissistic, Machiavellian, or psychopathic characteristics [1].\n",
        "\n",
        "## Potential Interventions to Redirect Individuals towards Positive Patterns of Behavior:\n",
        "\n",
        "1. **Empathy:**\n",
        "   - Cultivating empathy is crucial for developing healthier relationships and emotional intelligence [4].\n",
        "   - Therapeutic interventions, such as cognitive-behavioral therapy (CBT), can help individuals understand perspectives and foster empathy [4].\n",
        "\n",
        "2. **Friendship:**\n",
        "   - Encouraging individuals to develop and maintain healthy friendships provides social support and positive influences [4].\n",
        "   - Social skills training can improve their ability to engage in friendships, fostering positive patterns of behavior [4].\n",
        "\"\"\"\n",
        "\n",
        "TOPIC_SENTENCE = \"This comprehensive report explores the hypothesis that self-destructive behavior may serve as a subconscious attempt to undermine the dark triad personality traits of narcissism, Machiavellianism, and psychopathy.\"\n",
        "\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This research delves into the hypothesis that self-destructive behavior can be a coping mechanism to counteract the negative impact of dark triad personality traits. It explores prevalence, causes, and potential interventions to redirect individuals towards positive patterns like empathy, friendship, and compassion.\"\n",
        "\n",
        "BIBLIOGRAPHY = \"[Source 0](https://ajnpp.umsha.ac.ir/browse.php?a_id=318&slc_lang=en&sid=1&ftxt=1&html=1), [Source 1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7503275/), [Source 2](https://www.tandfonline.com/doi/full/10.1080/24732850.2023.2291459?src=exp-la), [Source 3](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6617551/), [Source 4](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6456805/), [Source 5](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4924075/)\"\n",
        "\n",
        "THEORY1 = \"Dark Triad Personality Traits\"\n",
        "THEORY2 = \"Self-Destructive Behavior\"\n",
        "\n",
        "RESEARCH_QUESTIONS = [\n",
        "    \"How prevalent is self-destructive behavior among individuals with dark triad personality traits?\",\n",
        "    \"What are the key causes and influencing factors behind self-destructive behavior in this population?\"\n",
        "]\n",
        "\n",
        "ACTION = \"Conduct research on the prevalence and causes of self-destructive behavior among individuals with dark triad personality traits, and explore potential interventions to redirect them towards positive patterns.\"\n",
        "\n",
        "RESULT_PARAGRAPHS += \"These interventions, including empathy, friendship, and compassion, aim to redirect individuals with dark triad traits towards positive patterns of behavior. The hypothesis of self-destructive behavior as an attempt to undermine these traits provides a framework for understanding the complex interplay between personality and maladaptive behaviors. By addressing the root causes and implementing targeted interventions, we can contribute to a more nuanced understanding and effective management of self-destructive tendencies among individuals with dark triad personality traits.\"\n",
        "\n",
        "DATE = \"January 30, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "ROLE = \"Researcher in Psychology and Behavioral Interventions\"\n",
        "PROJECT_EXAMPLE = \"Investigating the Relationship between Dark Triad Traits and Self-Destructive Behavior\"\n",
        "CONTEXT = \"Psychological research on personality traits and behavioral interventions\"\n",
        "INSTRUCTION = \"Conduct an in-depth study on the hypothesis that self-destructive behavior may be an attempt to undermine the dark triad personality traits. Explore prevalence, causes, and potential interventions for redirection towards positive patterns.\"\n",
        "OUTPUT_FORMAT = \"Comprehensive Research Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Psychology Research, Dark Triad Traits, Self-Destructive Behavior, Behavioral Interventions\"\n",
        "X = \"Not applicable\"\n",
        "PROJECT_MANAGER = \"Lead Researcher\"\n",
        "REPORT = \"Comprehensive Research Report on the Relationship between Dark Triad Traits and Self-Destructive Behavior\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Expertise in psychology, strong research skills, proficiency in behavioral interventions\"\n",
        "practical_skills = \"Data analysis, psychological assessments, designing and implementing interventions\"\n",
        "creativity = \"Developing innovative approaches to understand and address self-destructive behavior\"\n",
        "critical_thinking = \"Analyzing complex psychological phenomena, evaluating intervention effectiveness, synthesizing research findings\"\n",
        "\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "\n",
        "# Additional variables\n",
        "purpose = \"To investigate the hypothesis of self-destructive behavior as an attempt to undermine dark triad personality traits and provide insights into potential interventions.\"\n",
        "learning_outcomes = \"Enhanced understanding of dark triad traits and their impact on behavior, proficiency in analyzing psychological research, advanced skills in designing and implementing behavioral interventions.\"\n",
        "course_content = \"Introduction to dark triad personality traits, in-depth study of self-destructive behavior, exploration of behavioral interventions, case studies and real-world applications.\"\n",
        "course_assessments = \"Research paper on the relationship between dark triad traits and self-destructive behavior, presentation of findings, group discussions on behavioral interventions.\"\n",
        "course_schedule = \"Weekly lectures and discussions, assignments on relevant topics, final project on investigating the hypothesis.\"\n",
        "course_sequencing = \"Sequential course structure with weekly topics, regular quizzes and assessments, final project integrated with real-world applications.\"\n",
        "technology_requirements = \"Access to psychological research databases, internet access for online resources, personal computer or laptop for research and report writing.\"\n",
        "prerequisites = \"Basic knowledge in psychology, familiarity with research methodologies, good command of English.\"\n",
        "audience = \"Students pursuing courses in psychology, behavioral sciences, or related fields, Professionals interested in understanding and addressing self-destructive behavior, General public interested in psychological research.\"\n",
        "topic = \"Dark Triad Traits and Self-Destructive Behavior: An In-Depth Investigation\"\n",
        "field_of_study = \"Psychology, Behavioral Interventions\"\n",
        "specific_project = \"Research on the Relationship between Dark Triad Traits and Self-Destructive Behavior\""
      ],
      "metadata": {
        "id": "geFZsMuE_BEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"The Idea of Sharing 100% of Daily Profits with Users\"\n",
        "RESEARCH_DOMAIN = \"App Economy, User Motivation, Financial Empowerment\"\n",
        "PARAGRAPHS = \"\"\"\n",
        "The idea of sharing 100% of daily profits with users can be seen as a powerful motivator for people, especially those from lower income brackets, to engage with the app. Here's why:\n",
        "\n",
        "1. **Immediate Value Perception**: People tend to respond more positively to immediate value. The prospect of receiving a direct benefit from their actions (such as participating in waste management) can significantly increase their interest and willingness to engage with the app [0].\n",
        "\n",
        "2. **Financial Empowerment**: This model can empower users financially, especially those who may not have regular income. It can provide a sense of security and independence, reducing reliance on external sources of income [0].\n",
        "\n",
        "3. **Psychological Appeal**: Research shows that emotional appeals can significantly influence decision-making behavior. By framing the app as a tool for financial empowerment, it can tap into the emotional responses of users, making them more likely to engage with the app [0].\n",
        "\n",
        "4. **Community Engagement**: The app can foster a sense of community among users. By working together to manage waste, users can develop a stronger connection with each other and with the app, increasing their likelihood of continued engagement [4].\n",
        "\n",
        "5. **Risk Mitigation**: The profit-sharing model can also mitigate the risk perception associated with participating in new platforms. Users know they stand to gain from their participation, which can alleviate concerns about potential losses [0].\n",
        "\n",
        "In summary, the unique profit-sharing model of the app can tap into the psychological aspects of human behavior to attract and retain users, especially those from lower income brackets. It provides immediate value, fosters financial empowerment, taps into emotional appeals, promotes community engagement, and mitigates risk perceptions [0][4].\n",
        "\"\"\"\n",
        "\n",
        "PARAGRAPH = PARAGRAPHS\n",
        "TOPIC_SENTENCE = \"The unique profit-sharing model of the app can tap into the psychological aspects of human behavior to attract and retain users, especially those from lower income brackets.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This research explores the idea of sharing 100% of daily profits with users and its implications on user engagement, financial empowerment, and community building.\"\n",
        "BIBLIOGRAPHY = \"[Source 0](https://www.digitalocean.com/community/tutorials/how-to-use-variables-in-python-3), [Source 4](https://www.datacamp.com/tutorial/scope-of-variables-python)\"\n",
        "THEORY1 = \"Immediate Value Perception\"\n",
        "THEORY2 = \"Financial Empowerment\"\n",
        "\n",
        "RESEARCH_QUESTIONS = [\n",
        "    \"How does the immediate value perception influence user engagement with apps offering profit-sharing models?\",\n",
        "    \"What role does financial empowerment play in user retention in apps with profit-sharing models?\"\n",
        "]\n",
        "\n",
        "ACTION = \"Conduct research on the impact of profit-sharing models on user engagement and financial empowerment.\"\n",
        "RESULT_PARAGRAPHS = \"\"\n",
        "DATE = \"February 3, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "ROLE = \"Researcher in App Economy and User Motivation\"\n",
        "PROJECT_EXAMPLE = \"Investigating the Impact of Profit-Sharing Models on User Engagement\"\n",
        "CONTEXT = \"App economy and user motivation research\"\n",
        "INSTRUCTION = \"Conduct an in-depth study on the impact of profit-sharing models on user engagement and financial empowerment.\"\n",
        "OUTPUT_FORMAT = \"Research Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"App Economy Research, User Motivation, Profit-Sharing Models\"\n",
        "X = \"Not applicable\"\n",
        "PROJECT_MANAGER = \"Lead Researcher\"\n",
        "REPORT = \"Research Report on the Impact of Profit-Sharing Models on User Engagement\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = None\n",
        "employability = \"Expertise in app economy and user motivation, strong research skills, proficiency in data analysis\"\n",
        "practical_skills = \"Data analysis, survey design, user testing\"\n",
        "creativity = \"Developing innovative approaches to understand user behavior and motivation\"\n",
        "critical_thinking = \"Analyzing complex economic phenomena, evaluating model effectiveness, synthesizing research findings\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "purpose = \"To investigate the impact of profit-sharing models on user engagement and financial empowerment.\"\n",
        "learning_outcomes = \"Enhanced understanding of app economy and user motivation, proficiency in analyzing economic research, advanced skills in data analysis.\"\n",
        "course_content = \"Introduction to app economy, in-depth study of user motivation, exploration of profit-sharing models, case studies and real-world applications.\"\n",
        "course_assessments = \"Research paper on the impact of profit-sharing models on user engagement, presentation of findings, group discussions on app economy.\"\n",
        "course_schedule = \"Weekly lectures and discussions, assignments on relevant topics, final project on investigating the impact.\"\n",
        "course_sequencing = \"Sequential course structure with weekly topics, regular quizzes and assessments, final project integrated with real-world applications.\"\n",
        "technology_requirements = \"Access to app economy databases, internet access for online resources, personal computer or laptop for research and report writing.\"\n",
        "prerequisites = \"Basic knowledge in economics, familiarity with research methodologies, good command of English.\"\n",
        "audience = \"Students pursuing courses in business, economics, or related fields, Professionals interested in understanding and enhancing app user engagement, General public interested in app economy.\"\n",
        "topic = \"Impact of Profit-Sharing Models on User Engagement\"\n",
        "field_of_study = \"App Economy, User Motivation\"\n",
        "specific_project = \"\"\"\n",
        "The specific project involves investigating the impact of sharing 100% of daily profits with users on user engagement and financial empowerment. The goal is to understand how this unique profit-sharing model can tap into the psychological aspects of human behavior to attract and retain users, especially those from lower income brackets. It provides immediate value, fosters financial empowerment, taps into emotional appeals, promotes community engagement, and mitigates risk perceptions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mW7cDTF8bu-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOPIC = \"Challenges in Movement: Self-centered behavior, Patriarchal attitudes, and Inefficiencies in Diaspora\"\n",
        "RESEARCH_DOMAIN = \"Social Movements, Diaspora Engagement, Team Collaboration, Philanthropy\"\n",
        "PARAGRAPHS = \"\"\"\n",
        "Several issues within the movement have been highlighted, including:\n",
        "\n",
        "1. **Self-centered Behavior and Patriarchal Attitudes**: The movement faces challenges related to self-centered gameplay and a patriarchal approach that hinders effective communication with certain geographical areas like the Kurds. Interestingly, instead of listening to the voices of women and local activists, others tend to speak on their behalf. This recurring mistake needs attention to prevent the hijacking of the movement in favor of dark patterns such as selfishness, Machiavellianism, and psychopathy. Here, Machiavellianism creates bitterness as the dominant gender exploits and inflicts pain on enlightened groups, ultimately leading to psychopathy disrupting solidarity and friendship among them.\n",
        "\n",
        "   My hypothesis is that if the tracking of this hijacking by the dominant gender and patriarchal groups or other geographical entities is recognized as a dark triad pattern, the solution presented by the movement's own intellectuals in the 2023 article is effective. The remedy is a combination of dialogue and philanthropy for a duration of 4 months.\n",
        "\n",
        "   This implies that as an opposition figure, engaging in a structured conversation and philanthropic activities in environments like clubhouses for an explicit discussion of issues for 4 months is necessary. The conversation involves inviting women and minority groups like the Kurds, Sistanis, and political prisoners for dialogue. Additionally, this should not be an empty conversation; the conditions for charitable work during this dialogue should be present, or a transparent mechanism for this charity work should be established. One tested mechanism for this is pricing for political prisoners or inviting officials from international charity organizations.\n",
        "\n",
        "   In my opinion, this approach works, and if necessary, I will emphasize it, akin to the presentation of the dark triad for 1 or 2 years.\n",
        "\n",
        "2. **Diaspora Challenges in Persian Community**: The second problem discussed is related to the ineffective actions of the diaspora, where their focus on self-interest and flashy projects hindered their collaboration with the people and the movement. Although finding partners, acquiring funds, and networking might be reasons for their behavior, actions like pricing for prisoners or preparing for potential deaths proved to be useful.\n",
        "\n",
        "   Regarding the usefulness of the actions of the dominant gender within and outside the country, the statements of Shabnam and Faiza on this issue were intriguing to me. Shabnam, in her discussion on Alley or joining the women's movement, highlighted that those who join should not seek benefits for themselves or place the burden of benefit on others, and instead, they should avoid hijacking the movement or playing the leader.\n",
        "\n",
        "   In my view, this statement aligns with not imposing benefits and not seeking employment with the dark triad behavioral pattern. Furthermore, if we consider Natal Hudson's treatment as a conversation along with philanthropy for 4 months, then avoiding imposition, hijacking the movement, or playing the leader seems to be related to the lack of a culture of philanthropy and cooperation in the Persian community.\n",
        "\n",
        "   In this way, it can be said that teamwork and philanthropy techniques, especially in movements like open-source (open-source software like Linux), will be useful for engaging with the Persian-speaking movement.\n",
        "\n",
        "   Alongside that, new techniques for increasing the efficiency of economic entities, such as horizontal management techniques against traditional methods like vertical management, bring a variety of tested and defined methods from the economic university group into social activities.\n",
        "\n",
        "   These methods can be used for the growth of solidarity, friendship, and empathy in improving a movement towards dialogue, philanthropy, and collaboration on small projects and practicing teamwork with open-source projects.\n",
        "\"\"\"\n",
        "\n",
        "PARAGRAPH = PARAGRAPHS\n",
        "TOPIC_SENTENCE = \"Challenges within the movement include self-centered behavior, patriarchal attitudes, and ineffective diaspora actions.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This research explores challenges in a social movement, addressing self-centered behavior, patriarchal attitudes, and inefficiencies in diaspora actions.\"\n",
        "BIBLIOGRAPHY = \"[Source 0](https://www.digitalocean.com/community/tutorials/how-to-use-variables-in-python-3), [Source 4](https://www.datacamp.com/tutorial/scope-of-variables-python)\"\n",
        "THEORY1 = \"Self-centered Behavior and Patriarchal Attitudes\"\n",
        "THEORY2 = \"Inefficiencies in Diaspora Actions\"\n",
        "\n",
        "RESEARCH_QUESTIONS = [\n",
        "    \"How does self-centered behavior and patriarchal attitudes impact the effectiveness of social movements?\",\n",
        "    \"What role do diaspora inefficiencies play in hindering collaboration with grassroots movements?\"\n",
        "]\n",
        "\n",
        "ACTION = \"Conduct research on addressing self-centered behavior, patriarchal attitudes, and diaspora inefficiencies in social movements.\"\n",
        "RESULT_PARAGRAPHS = \"\"\n",
        "DATE = \"February 3, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "ROLE = \"Researcher in Social Movements and Diaspora Engagement\"\n",
        "PROJECT_EXAMPLE = \"Addressing Challenges in Social Movements\"\n",
        "CONTEXT = \"Social movements and diaspora engagement research\"\n",
        "INSTRUCTION = \"Conduct an in-depth study on challenges in social movements related to self-centered behavior, patriarchal attitudes, and diaspora inefficiencies.\"\n",
        "OUTPUT_FORMAT = \"Research Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Social Movements Research, Diaspora Engagement, Challenges in Movement\"\n",
        "X = \"\"\n",
        "PROJECT_MANAGER = \"Lead Researcher\"\n",
        "REPORT = \"Research Report on Challenges in Social Movements\"\n",
        "YOUR_EMAIL = \"your-email@example.com\"\n",
        "OPENAI_API = \"\"\n",
        "employability = \"Expertise in social movements and diaspora engagement, strong research skills, proficiency in data analysis\"\n",
        "practical_skills = \"Data analysis, survey design, user testing\"\n",
        "creativity = \"Developing innovative approaches to address challenges in social movements\"\n",
        "critical_thinking = \"Analyzing complex social phenomena, evaluating the effectiveness of strategies, synthesizing research findings\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"2 days, 0 months, 0 years\"\n",
        "purpose = \"To investigate and address challenges in social movements related to self-centered behavior, patriarchal attitudes, and diaspora inefficiencies.\"\n",
        "learning_outcomes = \"Enhanced understanding of social movements and diaspora engagement, proficiency in analyzing social research, advanced skills in data analysis.\"\n",
        "course_content = \"Introduction to social movements, in-depth study of diaspora engagement, exploration of challenges in movement, case studies and real-world applications.\"\n",
        "\n",
        "course_assessments = \"Research paper on challenges in social movements, presentation of findings, group discussions on diaspora engagement.\"\n",
        "course_schedule = \"Weekly lectures and discussions, assignments on relevant topics, final project on addressing challenges in social movements.\"\n",
        "course_sequencing = \"Sequential course structure with weekly topics, regular quizzes and assessments, final project integrated with real-world applications.\"\n",
        "technology_requirements = \"Access to social movements databases, internet access for online resources, personal computer or laptop for research and report writing.\"\n",
        "prerequisites = \"Basic knowledge in sociology, familiarity with research methodologies, good command of English.\"\n",
        "audience = \"Students pursuing courses in sociology, diaspora studies, or related fields, Professionals interested in understanding and addressing challenges in social movements, General public interested in diaspora engagement.\"\n",
        "topic = \"Challenges in Social Movements: Addressing Self-centered Behavior, Patriarchal Attitudes, and Diaspora Inefficiencies\"\n",
        "field_of_study = \"Social Movements, Diaspora Engagement\"\n",
        "specific_project = \"\"\"\n",
        "The specific project involves addressing challenges in a social movement, focusing on self-centered behavior, patriarchal attitudes, and inefficiencies in diaspora actions. The goal is to understand the impact of these challenges and propose strategies for improvement, fostering teamwork, philanthropy, and collaboration on small projects within the movement.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LgAm8xbTZKq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOPIC = 'Integrating Compassion in Engineering Ethics Education'\n",
        "RESEARCH_DOMAIN = 'Engineering Education'\n",
        "PARAGRAPHS = '''\n",
        "Engineering is a field that demands high technical proficiency, problem-solving skills, and an understanding of complex systems. However, the lack of compassion in engineering education can lead to a disconnect between the technical prowess of engineers and the emotional intelligence needed to effectively interact with stakeholders and contribute to societal well-being.\n",
        "'''\n",
        "PARAGRAPH = '''\n",
        "In the realm of engineering education, the integration of compassionate principles is a critical component that can significantly enhance the overall quality of education. By teaching empathy, altruism, and compassion, engineering programs can foster a culture of ethical responsibility that extends beyond mere technical proficiency. This is not just about imparting knowledge but also about instilling a deep understanding of the human element that is inherent in engineering projects. The ability to empathize with users, to act altruistically towards societal needs, and to demonstrate compassion in one's work are all essential attributes that can set engineers apart from their peers. Moreover, this focus on the Light Triad traits can serve as a counterbalance to the potential pitfalls associated with the Dark Triad, ensuring that engineers emerge from their education with a robust moral compass. By weaving these ethical threads into the fabric of engineering education, institutions can groom future engineers who are not only technically adept but also emotionally intelligent and socially conscious. This approach to education is not just a noble endeavor but also a strategic move that can position engineering as a force for positive change in our increasingly interconnected and complex world.\n",
        "'''\n",
        "TOPIC_SENTENCE = 'This post highlights the importance of integrating compassion into engineering ethics education to ensure that engineers are not only technically proficient but also morally responsible stewards of society and the environment.'\n",
        "LANGUAGE = 'English'\n",
        "ABSTRACT_PARAGRAPH = '''\n",
        "Engineering education should aim for the holistic development of engineers, equipping them with both technical skills and emotional intelligence. Compassion education can lead to long-term benefits, including improved job satisfaction, less burnout, and a stronger sense of purpose among engineers.\n",
        "'''\n",
        "BIBLIOGRAPHY = '''\n",
        "[2] EDC, Creating Integrated Engineering and Empathy Curriculum for Pre-Kindergarten and Kindergarten Classrooms\n",
        "[4] ResearchGate, Investigating the role of compassion in engineering service-learning\n",
        "'''\n",
        "THEORY1 = 'Light Triad - Empathy, Altruism, Compassion'\n",
        "THEORY2 = 'Dark Triad - Narcissism, Machiavellianism, Psychopathy'\n",
        "RESEARCH_QUESTIONS = ['How does compassion contribute to engineering ethics?', 'What are the implications of integrating compassion into engineering education?']\n",
        "ACTION = 'Integrating compassion into engineering education to bridge the gap between technical expertise and emotional intelligence.'\n",
        "RESULT_PARAGRAPHS = '''\n",
        "By embracing compassion education, engineering programs can ensure that their graduates are not only skilled technicians but also compassionate and thoughtful professionals who are capable of making meaningful contributions to society.\n",
        "'''\n",
        "DATE = '02/12/2024'\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = ''\n",
        "ROLE = 'Educator'\n",
        "PROJECT_EXAMPLE = 'EDC’s Creating Integrated Engineering and Empathy Curriculum for Pre-Kindergarten and Kindergarten Classrooms'\n",
        "CONTEXT = 'Engineering education reform'\n",
        "INSTRUCTION = 'Develop a comprehensive plan for integrating compassion into engineering education.'\n",
        "OUTPUT_FORMAT = 'Academic paper or report'\n",
        "SPECIFIC_PROJECT_DETAILS = 'The project involves designing and testing an integrated engineering and empathy intervention model for pre-K and K classrooms.'\n",
        "X = ''\n",
        "PROJECT_MANAGER = 'Michelle Cerrone'\n",
        "REPORT = 'To be determined upon completion of the project'\n",
        "YOUR_EMAIL = 'example@email.com'\n",
        "OPENAI_API = 'No API usage required for this task'\n",
        "employability = 'High'\n",
        "practical_skills = 'Technical proficiency, empathy, problem-solving'\n",
        "creativity = 'Yes'\n",
        "critical_thinking = 'Yes'\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = 'Duration:  2019–Present'\n",
        "purpose = 'To chart a new course for engineering education by examining the feasibility and promise of using engineering design challenges to engage young children in empathy.'\n",
        "learning_outcomes = 'Developed curriculum module, professional development for teachers, and design principles for integrated engineering and empathy activities.'\n",
        "course_content = 'Includes modules on ethics, human-centered design, and social responsibility.'\n",
        "course_assessments = 'Projects requiring compassionate decision-making and real-world application.'\n",
        "course_schedule = 'To be determined based on project timeline.'\n",
        "course_sequencing = 'Sequential, starting with theoretical understanding followed by experiential learning.'\n",
        "technology_requirements = 'None specified'\n",
        "prerequisites = 'Basic understanding of engineering principles'\n",
        "audience = 'Pre-K and K educators, curriculum specialists, and school staff'\n",
        "topic = 'Engineering and Empathy Education'\n",
        "field_of_study = 'Engineering Education'\n",
        "specific_project = 'Creating Integrated Engineering and Empathy Curriculum for Pre-Kindergarten and Kindergarten Classrooms'"
      ],
      "metadata": {
        "id": "1oydDh-7Acx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"The Evolution of Nonviolent Movements: Transitioning to Guerrilla Tactics\"\n",
        "RESEARCH_DOMAIN = \"Sociology, Political Science, History\"\n",
        "PARAGRAPHS = \"\"\"\n",
        "Nonviolent resistance has been a cornerstone of social change, with movements such as the Civil Rights Movement and the Montgomery Bus Boycott leading to significant political and social transformations. However, the transition from nonviolent resistance to guerrilla tactics is a complex process that can be influenced by various factors.\n",
        "\"\"\"\n",
        "PARAGRAPH = \"\"\"\n",
        "Nonviolent resistance has been a cornerstone of social change, with movements such as the Civil Rights Movement and the Montgomery Bus Boycott leading to significant political and social transformations. However, the transition from nonviolent resistance to guerrilla tactics is a complex process that can be influenced by various factors.\n",
        "\n",
        "Erica Chenoweth, a Harvard Professor, has conducted extensive research on nonviolent resistance and its effectiveness compared to violent campaigns. She found that nonviolent resistance often leads to longer-term reforms and democratization, even when they fail in the short term. Countries with nonviolent campaigns were about   10 times more likely to transition to democracies within a five-year period compared to countries with violent campaigns [2].\n",
        "\n",
        "Chenoweth identifies key elements necessary for a successful nonviolent campaign:\n",
        "\n",
        "- **Large and Diverse Participation**: A movement that can attract a wide range of participants is more likely to succeed.\n",
        "- **Loyalty Shifts Among Security Forces and Elites**: The ability to influence security forces and other elites is crucial for the success of a nonviolent campaign.\n",
        "- **Variation in Methods**: Campaigns need to employ a variety of methods, not just protests, to be effective.\n",
        "- **Resilience Against Repression**: If repressed, the movement should not descend into chaos or resort to violence. Instead, it should adapt and persist [2].\n",
        "\n",
        "The transition from nonviolence to guerrilla tactics can be seen as a failure of these elements. It may occur when a movement faces severe repression that it cannot overcome through nonviolent means. However, such a transition is risky and can have significant downsides, including loss of public support, damage to the movement's reputation, increased repression, loss of legitimacy, risk of radicalization, and backlash against all nonviolent movements.\n",
        "\n",
        "The study of this transition can provide valuable insights into the dynamics of social conflict and the potential outcomes of different resistance strategies. Understanding the factors that influence the decision to transition from nonviolence to guerrilla tactics is crucial for researchers, policymakers, and activists alike. It can help inform strategies that maximize the chances of achieving social change while minimizing the risks associated with more militant tactics.\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"Nonviolent movements have been a cornerstone of social change throughout history, often characterized by peaceful protests and civil disobedience.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"\"\"\n",
        "The transition from nonviolent resistance to guerrilla tactics can be seen as a failure of key elements necessary for a successful nonviolent campaign. It may occur when a movement faces severe repression that it cannot overcome through nonviolent means. However, such a transition is risky and can have significant downsides, including loss of public support, damage to the movement's reputation, increased repression, loss of legitimacy, risk of radicalization, and backlash against all nonviolent movements.\n",
        "\"\"\"\n",
        "BIBLIOGRAPHY = \"\"\"\n",
        "The study of this transition can provide valuable insights into the dynamics of social conflict and the potential outcomes of different resistance strategies. Understanding the factors that influence the decision to transition from nonviolence to guerrilla tactics is crucial for researchers, policymakers, and activists alike. It can help inform strategies that maximize the chances of achieving social change while minimizing the risks associated with more militant tactics.\n",
        "\"\"\"\n",
        "THEORY1 = \"Nonviolent Resistance Theory\"\n",
        "THEORY2 = \"Guerrilla Tactics Theory\"\n",
        "RESEARCH_QUESTIONS = [\"What factors influence the transition from nonviolence to guerrilla tactics?\", \"How can the effectiveness of nonviolent resistance be measured and compared to guerrilla tactics?\"]\n",
        "ACTION = \"Studying the transition from nonviolent to guerrilla tactics to inform strategies for social change.\"\n",
        "RESULT_PARAGRAPHS = \"\"\"\n",
        "The study of this transition can be approached through the lens of game theory, which examines the strategic interactions between different groups or states. Game theory can help us understand the decision-making process of leaders and participants within nonviolent movements, the strategies employed by various groups, and the outcomes of different actions.\n",
        "\"\"\"\n",
        "DATE = \"February  16,  2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"Days, Months, Years\"\n",
        "ROLE = \"Researcher, Historian, Sociologist\"\n",
        "PROJECT_EXAMPLE = \"Analysis of the transition from nonviolent to guerrilla movements in historical context\"\n",
        "CONTEXT = \"Historical and contemporary context of social change movements.\"\n",
        "INSTRUCTION = \"Analyze the transition from nonviolent to guerrilla movements and their impact on social change.\"\n",
        "OUTPUT_FORMAT = \"Research Paper, Policy Report, Data Visualization\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Detailed study of the transition dynamics, including case studies and theoretical analysis.\"\n",
        "X = \"Variable representing the extent of repression faced by nonviolent movements\"\n",
        "PROJECT_MANAGER = \"Dr. Jane Doe\"\n",
        "REPORT = \"Final Research Report on Nonviolent to Guerrilla Transitions\"\n",
        "YOUR_EMAIL = \"yourname@example.edu\"\n",
        "OPENAI_API = \"OpenAI API Key\"\n",
        "employability = \"Ability to conduct in-depth research and present findings in a clear and concise manner\"\n",
        "practical_skills = \"Research methodology, data analysis, and critical thinking\"\n",
        "creativity = \"Capacity to synthesize complex information and present it in a comprehensible format\"\n",
        "critical_thinking = \"Analytical skills to evaluate the effectiveness of nonviolent and guerrilla strategies\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"Metric representing the duration of the research project\"\n",
        "purpose = \"To understand the transition from nonviolent to guerrilla movements and its implications for social change.\"\n",
        "learning_outcomes = \"Gain insights into the dynamics of social conflict and the strategic transition from nonviolence to guerrilla tactics.\"\n",
        "course_content = \"Introduction to Social Change, Nonviolent Movements, Guerrilla Tactics, and Historical Analysis.\"\n",
        "course_assessments = \"Research Papers, Case Studies, Presentations, and Discussions.\"\n",
        "course_schedule = \"Weekly lectures and bi-weekly assessments.\"\n",
        "course_sequencing = \"Sequential learning path from social change to guerrilla tactics.\"\n",
        "technology_requirements = \"Access to academic databases, statistical software, and relevant literature.\"\n",
        "prerequisites = \"Basic understanding of social sciences and research methodology.\"\n",
        "audience = \"Academics, policymakers, and activists interested in social change and resistance strategies.\"\n",
        "topic = \"Transitioning from Nonviolence to Guerrilla Tactics\"\n",
        "field_of_study = \"Sociology, Political Science\"\n",
        "specific_project = \"Comparative Analysis of Nonviolent and Guerrilla Movements\""
      ],
      "metadata": {
        "id": "scV1hRSRfhIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOPIC = \"Cultivating Empathy and Compassion: A Strategy for Light Triad Personality Development in Non-Democratic and Democratic Contexts\"\n",
        "RESEARCH_DOMAIN = \"Sociology, Political Science, Psychology\"\n",
        "PARAGRAPHS = \"\"\"\n",
        "In the face of non-democratic regimes that seem to determine the state of Iran, it is essential to protect the light triad personality traits—empathy, compassion, and altruism—against the dark triad, which can be likened to a psychological virus, such as COVID-19. This post will explore two key strategies to empower democracy and foster the growth of the light triad in both non-democratic and democratic countries.\n",
        "\n",
        "Nonviolent resistance has been a cornerstone of social change, with movements such as the Civil Rights Movement and the Montgomery Bus Boycott leading to significant political and social transformations. However, the transition from nonviolent resistance to guerrilla tactics is a complex process that can be influenced by various factors.\n",
        "\n",
        "Erica Chenoweth, a Harvard Professor, has conducted extensive research on nonviolent resistance and its effectiveness compared to violent campaigns. She found that nonviolent resistance often leads to longer-term reforms and democratization, even when they fail in the short term. Countries with nonviolent campaigns were about   10 times more likely to transition to democracies within a five-year period compared to countries with violent campaigns [2].\n",
        "\n",
        "Chenoweth identifies key elements necessary for a successful nonviolent campaign:\n",
        "\n",
        "- **Large and Diverse Participation**: A movement that can attract a wide range of participants is more likely to succeed.\n",
        "- **Loyalty Shifts Among Security Forces and Elites**: The ability to influence security forces and other elites is crucial for the success of a nonviolent campaign.\n",
        "- **Variation in Methods**: Campaigns need to employ a variety of methods, not just protests, to be effective.\n",
        "- **Resilience Against Repression**: If repressed, the movement should not descend into chaos or resort to violence. Instead, it should adapt and persist [2].\n",
        "\n",
        "The transition from nonviolence to guerrilla tactics can be seen as a failure of these elements. It may occur when a movement faces severe repression that it cannot overcome through nonviolent means. However, such a transition is risky and can have significant downsides, including loss of public support, damage to the movement's reputation, increased repression, loss of legitimacy, risk of radicalization, and backlash against all nonviolent movements.\n",
        "\n",
        "The study of this transition can provide valuable insights into the dynamics of social conflict and the potential outcomes of different resistance strategies. Understanding the factors that influence the decision to transition from nonviolence to guerrilla tactics is crucial for researchers, policymakers, and activists alike. It can help inform strategies that maximize the chances of achieving social change while minimizing the risks associated with more militant tactics.\n",
        "\"\"\"\n",
        "PARAGRAPH = \"\"\"\n",
        "Nonviolent resistance has been a cornerstone of social change, with movements such as the Civil Rights Movement and the Montgomery Bus Boycott leading to significant political and social transformations. However, the transition from nonviolent resistance to guerrilla tactics is a complex process that can be influenced by various factors.\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"Nonviolent movements have been a cornerstone of social change throughout history, often characterized by peaceful protests and civil disobedience.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"\"\"\n",
        "The transition from nonviolent resistance to guerrilla tactics can be seen as a failure of key elements necessary for a successful nonviolent campaign. It may occur when a movement faces severe repression that it cannot overcome through nonviolent means. However, such a transition is risky and can have significant downsides, including loss of public support, damage to the movement's reputation, increased repression, loss of legitimacy, risk of radicalization, and backlash against all nonviolent movements.\n",
        "\"\"\"\n",
        "BIBLIOGRAPHY = \"\"\"\n",
        "The study of this transition can provide valuable insights into the dynamics of social conflict and the potential outcomes of different resistance strategies. Understanding the factors that influence the decision to transition from nonviolence to guerrilla tactics is crucial for researchers, policymakers, and activists alike. It can help inform strategies that maximize the chances of achieving social change while minimizing the risks associated with more militant tactics.\n",
        "\"\"\"\n",
        "THEORY1 = \"Nonviolent Resistance Theory\"\n",
        "THEORY2 = \"Guerrilla Tactics Theory\"\n",
        "RESEARCH_QUESTIONS = [\"What factors influence the transition from nonviolence to guerrilla tactics?\", \"How can the effectiveness of nonviolent resistance be measured and compared to guerrilla tactics?\"]\n",
        "ACTION = \"Studying the transition from nonviolent to guerrilla tactics to inform strategies for social change.\"\n",
        "RESULT_PARAGRAPHS = \"\"\"\n",
        "The study of this transition can be approached through the lens of game theory, which examines the strategic interactions between different groups or states. Game theory can help us understand the decision-making process of leaders and participants within nonviolent movements, the strategies employed by various groups, and the outcomes of different actions.\n",
        "\"\"\"\n",
        "DATE = \"February   21,   2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"Days, Months, Years\"\n",
        "ROLE = \"Researcher, Historian, Sociologist\"\n",
        "PROJECT_EXAMPLE = \"Analysis of the transition from nonviolent to guerrilla movements in historical context\"\n",
        "CONTEXT = \"Historical and contemporary context of social change movements.\"\n",
        "INSTRUCTION = \"Analyze the transition from nonviolent to guerrilla movements and their impact on social change.\"\n",
        "OUTPUT_FORMAT = \"Research Paper, Policy Report, Data Visualization\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Detailed study of the transition dynamics, including case studies and theoretical analysis.\"\n",
        "X = \"Variable representing the extent of repression faced by nonviolent movements\"\n",
        "PROJECT_MANAGER = \"Dr. Jane Doe\"\n",
        "REPORT = \"Final Research Report on Nonviolent to Guerrilla Transitions\"\n",
        "YOUR_EMAIL = \"yourname@example.edu\"\n",
        "OPENAI_API = \"OpenAI API Key\"\n",
        "employability = \"Ability to conduct in-depth research and present findings in a clear and concise manner\"\n",
        "practical_skills = \"Research methodology, data analysis, and critical thinking\"\n",
        "creativity = \"Capacity to synthesize complex information and present it in a comprehensible format\"\n",
        "critical_thinking = \"Analytical skills to evaluate the effectiveness of nonviolent and guerrilla strategies\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"Metric representing the duration of the research project\"\n",
        "purpose = \"To understand the transition from nonviolent to guerrilla movements and its implications for social change.\"\n",
        "learning_outcomes = \"Gain insights into the dynamics of social conflict and the strategic transition from nonviolence to guerrilla tactics.\"\n",
        "course_content = \"Introduction to Social Change, Nonviolent Movements, Guerrilla Tactics, and Historical Analysis.\"\n",
        "course_assessments = \"Research Papers, Case Studies, Presentations, and Discussions.\"\n",
        "course_schedule = \"Weekly lectures and bi-weekly assessments.\"\n",
        "course_sequencing = \"Sequential learning path from social change to guerrilla tactics.\"\n",
        "technology_requirements = \"Access to academic databases, statistical software, and relevant literature.\"\n",
        "prerequisites = \"Basic understanding of social sciences and research methodology.\"\n",
        "audience = \"Academics, policymakers, and activists interested in social change and resistance strategies.\"\n",
        "topic = \"Transitioning from Nonviolence to Guerrilla Tactics\"\n",
        "field_of_study = \"Sociology, Political Science\"\n",
        "specific_project = \"Comparative Analysis of Nonviolent and Guerrilla Movements\""
      ],
      "metadata": {
        "id": "npex5xs7yHdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Cancel Culture\"\n",
        "RESEARCH_DOMAIN = \"Digital Communication and Public Engagement\"\n",
        "PARAGRAPHS = \"Exploring the multifaceted discourse around cancel culture, its implications for accountability, and the well-being of individuals and organizations.\"\n",
        "PARAGRAPH = \"\"\"\n",
        "In the digital age, cancel culture emerges as a significant force, particularly concerning the accountability of experts and professionals. This phenomenon involves publicly shunning, rejecting, or denouncing individuals, often through social media platforms. It has evolved from a tool for holding individuals accountable to a broader critique of polarized, aggressive social engagement. A central debate revolves around the distinction between holding individuals accountable and unjust punishment. The effectiveness of cancel culture as a tool for social change is contentious, with differing views on its impact on productivity and the nature of conversations it fosters. The appropriate response to offensive content is also debated, with arguments for addressing harmful behavior versus ignoring or blocking content. The discussion around cancel culture has shifted towards the need for empathy and consequences, raising critical questions about fairness and the potential for unjust punishment. Professionals can navigate cancel culture by employing strategies such as clear communication, sensitivity, and consistency of message. Exploring both ineffective and effective race talk strategies can lead to more positive outcomes in workshop and classroom settings. The sectors most vulnerable to cancel culture include celebrities, social media influencers, politicians, public sector professionals, companies, and non-profit organizations. These sectors are particularly vulnerable due to the transparency and instantaneous nature of digital communication, necessitating the ability of individuals and organizations to quickly respond to and address public criticism.\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"Understanding the complexities of cancel culture and its impact on professionals' responsibility.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This abstract outlines the key themes of cancel culture, its implications for accountability, and strategies for navigating its challenges.\"\n",
        "BIBLIOGRAPHY = \"References from sources discussing cancel culture, accountability, and professional responsibility.\"\n",
        "THEORY1 = \"Digital Communication Theory\"\n",
        "THEORY2 = \"Public Engagement Theory\"\n",
        "RESEARCH_QUESTIONS = [\"What is cancel culture?\", \"How does cancel culture impact accountability?\", \"What strategies can be employed to navigate cancel culture effectively?\"]\n",
        "ACTION = \"Engaging with cancel culture through analysis and discussion, focusing on its implications for professionals.\"\n",
        "RESULT_PARAGRAPHS = \"Summarizing findings on the impact of cancel culture on professionals' responsibility, highlighting effective strategies for navigating its challenges.\"\n",
        "DATE = \"February  2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"6 months\"\n",
        "ROLE = \"Researcher and Analyst\"\n",
        "PROJECT_EXAMPLE = \"A study on the impact of cancel culture on the reputation and public perception of professionals.\"\n",
        "CONTEXT = \"In the context of digital age and public discourse.\"\n",
        "INSTRUCTION = \"Analyze and discuss the role of cancel culture in shaping public discourse and accountability, focusing on its implications for professionals.\"\n",
        "OUTPUT_FORMAT = \"A comprehensive report on cancel culture and its impact on professionals.\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"Detailed exploration of cancel culture, its implications for accountability, and strategies for navigating its challenges.\"\n",
        "X = \"The significance\"\n",
        "PROJECT_MANAGER = \"Project Manager Name\"\n",
        "REPORT = \"Final Report on Cancel Culture and Professional Responsibility\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = \"OpenAI API\"\n",
        "employability = \"Digital Communication, Public Engagement\"\n",
        "practical_skills = \"Data Analysis, Social Media Research\"\n",
        "creativity = \"Critical Thinking, Creative Writing\"\n",
        "critical_thinking = \"Analytical Skills, Problem-Solving\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"6 months\"\n",
        "purpose = \"To understand and navigate the complexities of cancel culture and its impact on professional responsibility.\"\n",
        "learning_outcomes = \"Develop a critical understanding of cancel culture, its implications for accountability, and strategies for navigating its challenges.\"\n",
        "course_content = \"Topics include digital communication, social media studies, public discourse analysis, and professional responsibility.\"\n",
        "course_assessments = \"Assessments will include case studies, essays, and presentations.\"\n",
        "course_schedule = \"Weekly lectures, discussion forums, and individual assignments.\"\n",
        "course_sequencing = \"The course will start with an introduction to digital communication and move towards the analysis of cancel culture.\"\n",
        "technology_requirements = \"Access to internet and social media platforms for research.\"\n",
        "prerequisites = \"Basic understanding of digital communication and social media.\"\n",
        "audience = \"Academics, professionals, and students interested in digital culture and public discourse.\"\n",
        "topic = \"Cancel Culture\"\n",
        "field_of_study = \"Digital Culture and Communication\"\n",
        "specific_project = \"Analysis of cancel culture in the context of digital communication and public engagement.\""
      ],
      "metadata": {
        "id": "UEQ1xKRBUKdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Navigating the Light Triad in Dating Apps: A Demographic Perspective\"\n",
        "RESEARCH_DOMAIN = \"Digital Ethics and Online Safety\"\n",
        "PARAGRAPH = \"\"\"\n",
        "In the digital age, dating apps have become a pivotal platform for connecting with potential partners. However, the demographics of users who are inclined towards dating apps promoting light triad traits, such as empathy, altruism, and compassion, remain a fascinating area of study. Recent academic research has shed light on the age groups and genders that are more likely to engage with these apps, offering valuable insights for app developers and designers to tailor their platforms to specific user groups. A study aimed at examining the Light Triad of personality traits among Tinder users revealed interesting patterns. Another study focused on participants aged 20 to 56 who utilized online dating sites, apps, and social media for finding partners in the past year, providing a broader demographic perspective. Additionally, a comprehensive analysis of the relationship between using dating apps and sociodemographics, including gender and age, further enriched our understanding. These studies collectively highlight the demographics of individuals who are more inclined towards dating apps that emphasize light triad traits. By examining these findings, we can better understand the preferences and motivations of different age groups and genders when it comes to using dating apps. This knowledge is crucial for app developers and designers to tailor their platforms to specific user groups, promoting healthier and more fulfilling relationships. While the mentioned studies provide valuable insights, further research in this area is necessary to gain a more comprehensive understanding of the age groups and genders more likely to use dating apps promoting light triad traits. Additional studies exploring these demographics in various cultural contexts would contribute to a more nuanced understanding of the topic.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "PARAGRAPHS = \"\"\"\n",
        "A study aimed at examining the Light Triad of personality traits among Tinder users revealed interesting patterns. Another study focused on participants aged 20 to 56 who utilized online dating sites, apps, and social media for finding partners in the past year, providing a broader demographic perspective. Additionally, a comprehensive analysis of the relationship between using dating apps and sociodemographics, including gender and age, further enriched our understanding.\n",
        "\"\"\"\n",
        "\n",
        "TOPIC_SENTENCE = \"The focus is on understanding the demographics of individuals inclined towards dating apps promoting light triad traits and the need for app developers to tailor their platforms accordingly.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This study explores the demographics of users inclined towards dating apps promoting light triad traits, offering insights for app developers to tailor their platforms for healthier and more fulfilling relationships.\"\n",
        "BIBLIOGRAPHY = \"Refer to the Phind search results for more details on the demographics of users inclined towards dating apps promoting light triad traits.\"\n",
        "THEORY1 = \"Digital Ethics\"\n",
        "THEORY2 = \"Online Safety\"\n",
        "RESEARCH_QUESTIONS = [\n",
        "    \"What are the demographics of individuals inclined towards dating apps promoting light triad traits?\",\n",
        "    \"How can app developers tailor their platforms to promote healthier and more fulfilling relationships?\"\n",
        "]\n",
        "ACTION = \"Develop strategies for app developers to tailor their platforms to the demographics inclined towards dating apps promoting light triad traits.\"\n",
        "RESULT_PARAGRAPHS = \"The investigation into the demographics of users inclined towards dating apps promoting light triad traits reveals a need for app developers to focus on creating platforms that foster genuine connections and meaningful relationships.\"\n",
        "DATE = \"March 14, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"3 months\"\n",
        "ROLE = \"Digital Ethics Advocate\"\n",
        "PROJECT_EXAMPLE = \"Development of a dating app promoting light triad traits\"\n",
        "CONTEXT = \"Digital Ethics and Online Safety\"\n",
        "INSTRUCTION = \"Conduct research on the demographics of users inclined towards dating apps promoting light triad traits and propose solutions.\"\n",
        "OUTPUT_FORMAT = \"Research Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"A study on understanding and promoting light triad traits in dating apps.\"\n",
        "X = \"Number of actionable recommendations\"\n",
        "PROJECT_MANAGER = \"Digital Ethics Advocate's Name\"\n",
        "REPORT = \"Report on Understanding and Promoting Light Triad Traits in Dating Apps\"\n",
        "IMPORTANT_THEMES = \"Digital Ethics, Online Safety, Inclusivity\"\n",
        "PROJECT_NAME = \"Study on Light Triad Traits in Dating Apps\"\n",
        "STAKEHOLDER = \"Users, Developers, Advocacy Groups\"\n",
        "RESISTANT_STAKEHOLDER = \"Platforms with outdated safety measures\"\n",
        "TASK = \"Research and document the demographics of users inclined towards dating apps promoting light triad traits\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = \"Researching and documenting the demographics of users inclined towards dating apps promoting light triad traits\"\n",
        "practical_skills = \"Data analysis, User experience design\"\n",
        "creativity = \"Identifying innovative solutions for promoting light triad traits in dating apps\"\n",
        "critical_thinking = \"Evaluating the impact of technology on user safety and privacy\"\n",
        "fun_and_enjoyment = \"Exploring the intersection of technology and user safety\"\n",
        "employee_guarantee = \"A commitment to thorough research and accurate documentation\"\n",
        "collaboration = \"Working with experts in digital ethics and online safety\"\n",
        "learning_outcomes = \"Understanding the role of technology in user safety and privacy\"\n",
        "purpose = \"To inform and inspire safer and more equitable online dating experiences\"\n",
        "learning_activities = \"Researching digital ethics issues, Attending conferences on online safety\"\n",
        "course_content = \"Digital ethics theories, Case studies of online safety challenges, Technological advancements in online dating\"\n",
        "course_assessments = \"Writing assignments on digital ethics practices, Presentations on case studies\"\n",
        "course_schedule = \"Bi-weekly online sessions\"\n",
        "course_sequencing = \"Introduction to digital ethics, Case studies, Technological advancements\"\n",
        "technology_requirements = \"Access to research databases, Internet access\"\n",
        "prerequisites = \"Basic understanding of digital ethics and online safety\"\n",
        "audience = \"Digital Ethics Advocates, Developers, Policy Makers\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "8hsQ87Ay7QTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOPIC = 'Male Indifference within Feminism'\n",
        "RESEARCH_DOMAIN = 'Gender Studies'\n",
        "PARAGRAPH = \"The discourse on gender and identity, particularly in the context of feminism, often highlights the complexities of male indifference. This indifference is not merely a lack of engagement but a reflection of deeper societal and personal factors, including the challenges of expressing emotions, the hegemonic masculinity that values self-sufficiency, and the reliance on women for emotional support. Addressing these underlying issues is crucial for fostering empathy and understanding within the feminist movement and beyond.\"\n",
        "PARAGRAPHS = 'The indifference of men within feminism is not simply a matter of quantity but a reflection of deeper societal and personal factors, including the challenges of expressing emotions, the hegemonic masculinity that values self-sufficiency, and the reliance on women for emotional support.'\n",
        "TOPIC_SENTENCE = 'Understanding the root causes of male indifference within feminism is crucial for fostering meaningful dialogue and collaboration within the movement.'\n",
        "LANGUAGE = 'English'\n",
        "ABSTRACT_PARAGRAPH = 'This post explores the reasons behind male indifference within feminism, highlighting key factors such as societal norms of masculinity, emotional expression barriers, fear of social stigma, lack of emotional support networks, perceived threat to masculine identity, and hegemonic masculinity ideals.'\n",
        "BIBLIOGRAPHY = 'Connell, R. W. (2005). Masculinities (2nd ed.). University of California Press.\\nKimmel, M. S. (2013). Angry white men: American masculinity at the end of an era. Nation Books.\\nLevant, R. F., et al. (2011). The Male Role Norms Inventory-Short Form (MRNI-SF): development, confirmatory factor analytic investigation of structure, and measurement invariance across gender. Journal of Counseling Psychology, 58(1), 33-43.\\nSmiler, A. (2004). Thirty years after the discovery of gender: Psychological concepts and measures of masculinity. Sex Roles, 50(1-2), 15-26.\\nFlood, M., & Pease, B. (2009). Factors influencing attitudes to violence against women. Trauma, Violence, & Abuse, 10(2), 125-142.\\nConnell, R. W., & Messerschmidt, J. W. (2005). Hegemonic masculinity: Rethinking the concept. Gender & Society, 19(6), 829-859.\\nRisman, B. J. (1998). Gender vertigo: American families in transition. Yale University Press.'\n",
        "THEORY1 = 'Hegemonic Masculinity'\n",
        "THEORY2 = 'Gendered Division of Emotional Labor'\n",
        "RESEARCH_QUESTIONS = ['What are the societal norms of masculinity that contribute to male indifference within feminism?', 'How do emotional expression barriers affect male engagement in feminism?', 'What role does fear of social stigma play in male indifference within feminism?', 'How does the lack of emotional support networks among men contribute to their indifference?', 'What is the perceived threat to masculine identity in the context of feminism?', 'How does hegemonic masculinity ideals influence male indifference within feminism?', 'How does the gendered division of emotional labor contribute to male indifference within feminism?']\n",
        "ACTION = 'Addressing these underlying issues is crucial for fostering empathy and understanding within the feminist movement and beyond.'\n",
        "RESULT_PARAGRAPHS = 'By acknowledging and addressing these challenges, we can work towards fostering a more inclusive and supportive environment for all genders within the feminist movement.'\n",
        "DATE = '03/16/2024'\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = '16 days'\n",
        "ROLE = 'Researcher'\n",
        "PROJECT_EXAMPLE = 'A study on the impact of societal norms of masculinity on male engagement in feminism.'\n",
        "CONTEXT = 'The ongoing pursuit of gender equality'\n",
        "INSTRUCTION = 'Explore the reasons behind male indifference within feminism and propose strategies to foster meaningful dialogue and collaboration within the movement.'\n",
        "OUTPUT_FORMAT = 'Academic paper'\n",
        "SPECIFIC_PROJECT_DETAILS = 'This project aims to delve into the complexities of male indifference within feminism, examining key factors and their implications for the movement.'\n",
        "X = 'Male Indifference'\n",
        "PROJECT_MANAGER = 'Dr. Jane Doe'\n",
        "REPORT = 'Final Report on Male Indifference within Feminism'\n",
        "IMPORTANT_THEMES = 'Gender Equality, Masculinity, Feminism, Emotional Support, Societal Norms'\n",
        "PROJECT_NAME = 'Understanding Male Indifference within Feminism'\n",
        "STAKEHOLDER = 'Feminist Movement, Men, Women'\n",
        "RESISTANT_STAKEHOLDER = 'Hegemonic Masculinity Ideals'\n",
        "TASK = 'Conducting a comprehensive analysis of male indifference within feminism'\n",
        "YOUR_EMAIL = 'your.email@example.com'\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = 'High'\n",
        "practical_skills = 'Research, Analysis, Writing'\n",
        "creativity = 'Low'\n",
        "critical_thinking = 'High'\n",
        "fun_and_enjoyment = 'Low'\n",
        "employee_guarantee = 'Yes'\n",
        "collaboration = 'High'\n",
        "learning_outcomes = 'Understanding of male indifference within feminism, strategies for fostering dialogue and collaboration.'\n",
        "purpose = 'To contribute to the feminist movement by addressing male indifference.'\n",
        "learning_activities = 'Literature review, data analysis, discussion forums'\n",
        "course_content = 'Gender Studies, Feminism, Masculinity, Emotional Support'\n",
        "course_assessments = 'Research paper, presentation, discussion participation'\n",
        "course_schedule = 'Flexible, 6 months'\n",
        "course_sequencing = 'Literature review, data analysis, discussion, paper writing'\n",
        "technology_requirements = 'Access to academic databases, research software'\n",
        "prerequisites = 'Basic understanding of gender studies'\n",
        "audience = 'Academics, Feminist Activists'\n",
        "topic = 'Male Indifference within Feminism'\n",
        "field_of_study = 'Gender Studies'\n",
        "specific_project = 'A study on the impact of societal norms of masculinity on male engagement in feminism.'"
      ],
      "metadata": {
        "id": "txzTIsLLppLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = 'Islamofascism: Deception and Manipulation in Media and Society'\n",
        "RESEARCH_DOMAIN = 'Media Studies'\n",
        "PARAGRAPH = \"Islamofascism, a term blending fascism with Islamic ideology, is on the rise in digital and social spaces. This phenomenon is characterized by followers, often labeled as trolls, who employ a myriad of tactics to manipulate truth, spread misinformation, and foster division. These tactics range from misrepresentation and omission to exaggeration and gaslighting, aiming to skew narratives, confuse audiences, and undermine the integrity of public discourse. By understanding these tactics, we can better equip journalists, media organizations, and the public to resist the spread of misinformation and hate, working towards a more informed and empathetic society.\"\n",
        "PARAGRAPHS = \"These techniques are not only harmful to the individuals and communities they target but also undermine the integrity of public discourse and the media. It's crucial for journalists, media organizations, and the public to be aware of these tactics and to resist the spread of misinformation and hate. By understanding and exposing these tactics, we can work towards a more informed and empathetic society.\"\n",
        "TOPIC_SENTENCE = \"Understanding the tactics of Islamofascism is crucial for fostering a more informed and empathetic society, resisting the spread of misinformation and hate.\"\n",
        "LANGUAGE = 'English'\n",
        "ABSTRACT_PARAGRAPH = \"This post explores the 20 techniques used by Islamofascist trolls in the media and society, aiming to expose and understand these tactics to counteract their harmful effects on public discourse and individual communities.\"\n",
        "BIBLIOGRAPHY = ''\n",
        "THEORY1 = 'Trolling'\n",
        "THEORY2 = 'Misinformation'\n",
        "RESEARCH_QUESTIONS = [\n",
        "    'What are the 20 techniques used by Islamofascist trolls in the media and society?',\n",
        "    'How do these techniques manipulate truth and spread misinformation?',\n",
        "    'What role do social media platforms play in the spread of Islamofascist narratives?',\n",
        "    'How can journalists and media organizations resist the spread of Islamofascist misinformation?'\n",
        "]\n",
        "ACTION = 'Awareness and education are key to resisting the spread of Islamofascist misinformation and hate.'\n",
        "RESULT_PARAGRAPHS = 'By understanding and exposing these tactics, we can work towards a more informed and empathetic society, fostering a culture of critical thinking and resistance against manipulation.'\n",
        "DATE = '04/17/2024'\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = '17 days'\n",
        "ROLE = 'Researcher'\n",
        "PROJECT_EXAMPLE = 'A study on the tactics of Islamofascist trolls in the media and society.'\n",
        "CONTEXT = 'The digital and social spheres'\n",
        "INSTRUCTION = 'Explore the tactics of Islamofascist trolls and propose strategies to counteract their harmful effects on public discourse and individual communities.'\n",
        "OUTPUT_FORMAT = 'Academic paper'\n",
        "SPECIFIC_PROJECT_DETAILS = 'This project aims to delve into the tactics of Islamofascist trolls, examining their methods and implications for society.'\n",
        "X = 'Islamofascism'\n",
        "PROJECT_MANAGER = 'Dr. John Doe'\n",
        "REPORT = 'Final Report on Islamofascism: Deception and Manipulation in Media and Society'\n",
        "IMPORTANT_THEMES = 'Media Studies, Trolling, Misinformation, Digital and Social Spheres'\n",
        "PROJECT_NAME = 'Understanding Islamofascism: Deception and Manipulation'\n",
        "STAKEHOLDER = 'Journalists, Media Organizations, Public'\n",
        "RESISTANT_STAKEHOLDER = 'Islamofascist Trolls'\n",
        "TASK = 'Conducting a comprehensive analysis of Islamofascist trolls and their tactics.'\n",
        "YOUR_EMAIL = 'your.email@example.com'\n",
        "OPENAI_API = None\n",
        "\n",
        "# Additional variables\n",
        "employability = 'High'\n",
        "practical_skills = 'Research, Analysis, Writing'\n",
        "creativity = 'Low'\n",
        "critical_thinking = 'High'\n",
        "fun_and_enjoyment = 'Low'\n",
        "employee_guarantee = 'Yes'\n",
        "collaboration = 'High'\n",
        "learning_outcomes = 'Understanding of Islamofascist trolls and their tactics, strategies for counteracting misinformation.'\n",
        "purpose = 'To contribute to the fight against Islamofascist misinformation and hate.'\n",
        "learning_activities = 'Literature review, data analysis, discussion forums'\n",
        "course_content = 'Media Studies, Trolling, Misinformation'\n",
        "course_assessments = 'Research paper, presentation, discussion participation'\n",
        "course_schedule = 'Flexible, 6 months'\n",
        "course_sequencing = 'Literature review, data analysis, discussion, paper writing'\n",
        "technology_requirements = 'Access to academic databases, research software'\n",
        "prerequisites = 'Basic understanding of media studies'\n",
        "audience = 'Academics, Journalists, Media Professionals'\n",
        "topic = 'Islamofascism: Deception and Manipulation in Media and Society'\n",
        "field_of_study = 'Media Studies'\n",
        "specific_project = 'A study on the tactics of Islamofascist trolls in the media and society.'"
      ],
      "metadata": {
        "id": "y2t4PC2z3qmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"Unmasking Medical Fraud: The Case of Dr. Hessieni Hoshiar and the Betrayal of Trust Through Misdiagnosis and Denial of Effective Treatments\"\n",
        "RESEARCH_DOMAIN = \"Medical Ethics and Patient Care\"\n",
        "PARAGRAPH = \"\"\"\n",
        "In the complex landscape of medical ethics, the case of Dr. Hessieni Hoshiar illuminates the devastating consequences of medical fraud and misdiagnosis, particularly in the context of treating schizophrenia. This narrative unfolds through a recorded treatment session, revealing a pattern of professional misconduct that extends beyond mere misdiagnosis to include the denial of effective treatments and the infliction of unnecessary harm on patients and their families. A critical juncture in the case is captured in a recorded treatment session, where Dr. Hessieni Hoshiar pressures the patient to continue taking medication prescribed for schizophrenia despite the patient's expressed preference for Cognitive Behavioral Therapy (CBT). The patient, having ingested the prescribed medication for several years without experiencing any beneficial outcomes, advocates for CBT—a treatment modality scientifically validated for reducing anxiety and improving outcomes in schizophrenia patients. However, Dr. Hoshiar dismisses CBT as ineffective, asserting that it should not be considered a viable treatment option for schizophrenia. Contrary to Dr. Hoshiar's assertions, the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) suggests that CBT can be an effective treatment for schizophrenia, especially when traditional medications are not accepted or tolerated by the patient. The manual outlines strategies for engaging patients who may resist medication, including the use of CBT to build trust and reduce anxiety. The ethical failure becomes evident as the patient and their family challenge Dr. Hoshiar's diagnosis and treatment recommendations. The patient asserts that the diagnosis of schizophrenia was incorrect and that the cessation of CBT represents a deliberate attempt by Dr. Hoshiar to cause harm. This assertion is grounded in the belief that Dr. Hoshiar's actions reflect a disregard for patient welfare and a sadistic inclination to inflict pain and distress on the patient and their family, constituting a clear act of medical fraud. The case underscores the urgent need for accountability within the medical profession and the establishment of robust oversight mechanisms to prevent such misconduct. It highlights the importance of adhering to ethical standards and prioritizing patient welfare over personal preferences or convenience. Furthermore, it emphasizes the necessity of legal protections for patients against medical misconduct and the importance of educating healthcare professionals about the latest scientific advancements in mental health treatment. The case serves as a stark reminder of the human cost of medical fraud and the devastating impact of misdiagnosis on patients' lives. It calls for a reevaluation of the standards and practices within the medical profession, emphasizing the necessity of integrity, transparency, and compassion in patient care. Through this case, we hope to shed light on the critical need for vigilance and action against medical misconduct, advocating for justice and reform in healthcare.\n",
        "\"\"\"\n",
        "\n",
        "PARAGRAPHS = \"\"\"\n",
        "A critical juncture in the case is captured in a recorded treatment session, where Dr. Hessieni Hoshiar pressures the patient to continue taking medication prescribed for schizophrenia despite the patient's expressed preference for Cognitive Behavioral Therapy (CBT). The patient, having ingested the prescribed medication for several years without experiencing any beneficial outcomes, advocates for CBT—a treatment modality scientifically validated for reducing anxiety and improving outcomes in schizophrenia patients. However, Dr. Hoshiar dismisses CBT as ineffective, asserting that it should not be considered a viable treatment option for schizophrenia.\n",
        "\"\"\"\n",
        "TOPIC_SENTENCE = \"The focus is on understanding the devastating consequences of medical fraud and misdiagnosis through the case of Dr. Hessieni Hoshiar, particularly in treating schizophrenia.\"\n",
        "LANGUAGE = \"English\"\n",
        "ABSTRACT_PARAGRAPH = \"This study explores the case of Dr. Hessieni Hoshiar, highlighting the consequences of medical fraud and misdiagnosis in treating schizophrenia, emphasizing the need for ethical accountability and patient-centered care.\"\n",
        "BIBLIOGRAPHY = \"Refer to the Phind search results for more details on the consequences of medical fraud and misdiagnosis in treating schizophrenia.\"\n",
        "THEORY1 = \"Medical Ethics\"\n",
        "THEORY2 = \"Patient Care\"\n",
        "RESEARCH_QUESTIONS = [\n",
        "    \"What are the ethical implications of misdiagnosis and denial of effective treatments in medical practice?\",\n",
        "    \"How can healthcare systems ensure accountability and prevent medical fraud?\"\n",
        "]\n",
        "ACTION = \"Develop strategies for healthcare systems to ensure accountability and prevent medical fraud, promoting ethical patient care.\"\n",
        "RESULT_PARAGRAPHS = \"The investigation into Dr. Hessieni Hoshiar's case reveals a need for robust oversight mechanisms and ethical standards in the medical profession to prevent misdiagnosis and denial of effective treatments.\"\n",
        "DATE = \"March 14, 2024\"\n",
        "NUMBER_OF_DAYS_MONTHS_YEARS = \"3 months\"\n",
        "ROLE = \"Medical Ethics Advocate\"\n",
        "PROJECT_EXAMPLE = \"Development of guidelines for ethical patient care and accountability in medical practice\"\n",
        "CONTEXT = \"Medical Ethics and Patient Care\"\n",
        "INSTRUCTION = \"Conduct research on the ethical implications of misdiagnosis and denial of effective treatments in medical practice, and propose solutions.\"\n",
        "OUTPUT_FORMAT = \"Research Report\"\n",
        "SPECIFIC_PROJECT_DETAILS = \"A study on understanding and preventing medical fraud and misdiagnosis in treating schizophrenia.\"\n",
        "X = \"Number of actionable recommendations\"\n",
        "PROJECT_MANAGER = \"Medical Ethics Advocate's Name\"\n",
        "REPORT = \"Report on Medical Fraud and Misdiagnosis in Treating Schizophrenia\"\n",
        "IMPORTANT_THEMES = \"Medical Ethics, Patient Care, Accountability\"\n",
        "PROJECT_NAME = \"Study on Medical Fraud and Misdiagnosis in Treating Schizophrenia\"\n",
        "STAKEHOLDER = \"Patients, Healthcare Providers, Regulatory Bodies\"\n",
        "RESISTANT_STAKEHOLDER = \"Practitioners with outdated or unethical practices\"\n",
        "TASK = \"Research and document the ethical implications of misdiagnosis and denial of effective treatments in medical practice\"\n",
        "YOUR_EMAIL = \"your.email@example.com\"\n",
        "OPENAI_API = None\n",
        "\n",
        "employability = \"Researching and documenting the ethical implications of misdiagnosis and denial of effective treatments in medical practice\"\n",
        "practical_skills = \"Data analysis, Ethical assessment\"\n",
        "creativity = \"Identifying innovative solutions for preventing medical fraud and misdiagnosis\"\n",
        "critical_thinking = \"Evaluating the impact of medical practices on patient care and safety\"\n",
        "fun_and_enjoyment = \"Exploring the intersection of ethics and healthcare\"\n",
        "employee_guarantee = \"A commitment to thorough research and accurate documentation\"\n",
        "collaboration = \"Working with experts in medical ethics and patient care\"\n",
        "learning_outcomes = \"Understanding the role of ethics in healthcare and patient safety\"\n",
        "purpose = \"To inform and inspire ethical and patient-centered practices in healthcare\"\n",
        "learning_activities = \"Researching medical ethics issues, Attending conferences on patient care\"\n",
        "course_content = \"Medical ethics theories, Case studies of medical misconduct, Strategies for ethical patient care\"\n",
        "course_assessments = \"Writing assignments on ethical practices, Presentations on case studies\"\n",
        "course_schedule = \"Bi-weekly online sessions\"\n",
        "course_sequencing = \"Introduction to medical ethics, Case studies, Strategies for ethical care\"\n",
        "technology_requirements = \"Access to research databases, Internet access\"\n",
        "prerequisites = \"Basic understanding of medical ethics and patient care\"\n",
        "audience = \"Medical Ethics Advocates, Healthcare Providers, Policy Makers\"\n",
        "topic = TOPIC\n",
        "field_of_study = RESEARCH_DOMAIN\n",
        "specific_project = PROJECT_NAME"
      ],
      "metadata": {
        "id": "EznsmSIQYp1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "role = ROLE\n",
        "project_example = PROJECT_EXAMPLE\n",
        "context = CONTEXT\n",
        "instruction = INSTRUCTION\n",
        "specific_project_details = SPECIFIC_PROJECT_DETAILS\n",
        "project_manager = PROJECT_MANAGER\n",
        "report = REPORT\n",
        "important_themes = IMPORTANT_THEMES\n",
        "stakeholder = STAKEHOLDER\n",
        "resistant_stakeholder = RESISTANT_STAKEHOLDER\n",
        "#openai_api= OPENAI_API\n",
        "Your_Email = YOUR_EMAIL\n",
        "category= 'category'"
      ],
      "metadata": {
        "id": "wnPiE2o5dhw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZNYv8dBhdml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "variables = {\n",
        "    \"TOPIC\": \"Addressing Iran's Crises: Exploring Social Change Methodologies and Creating a Counter Profile\",\n",
        "    \"RESEARCH_DOMAIN\": \"Sociology\",\n",
        "    \"PARAGRAPH\": \"Iran's societal challenges are complex and require immediate attention. Engaging in research and working on solutions in this field can help address these issues. This course aims to equip students with the necessary tools and knowledge to navigate these challenges.\",\n",
        "    \"PARAGRAPHS\": \"PARAGRAPH\",\n",
        "    \"TOPIC_SENTENCE\": \"Understanding and addressing Iran's crisis is a critical aspect of sociology.\",\n",
        "    \"LANGUAGE\": \"English\",\n",
        "    \"ABSTRACT_PARAGRAPH\": \"This course explores the management of Iran's crises through engagement in research and charitable work...\",\n",
        "    \"BIBLIOGRAPHY\": \"Source 0: https://www.csis.org/analysis/crisis-iran-what-now\",\n",
        "    \"THEORY1\": \"Sociological Theory\",\n",
        "    \"THEORY2\": \"Social Change Theory\",\n",
        "    \"RESEARCH_QUESTIONS\": \"['What are the primary causes of Iran's crises?', 'How can research and charitable work help address these issues?']\",\n",
        "    \"ACTION\": \"Research and discussion\",\n",
        "    \"RESULT_PARAGRAPHS\": \"Results indicate that engaging in research and charitable work can significantly reduce the impacts of Iran's crises...\",\n",
        "    \"DATE\": \"2023-12-30\",\n",
        "    \"NUMBER_OF_DAYS_MONTHS_YEARS\": \"One year\",\n",
        "    \"ROLE\": \"Researcher\",\n",
        "    \"PROJECT_EXAMPLE\": \"Social Change Management Project\",\n",
        "    \"CONTEXT\": \"Social Development\",\n",
        "    \"INSTRUCTION\": \"Study and research on Iran's crises and their management\",\n",
        "    \"OUTPUT_FORMAT\": \"Report\",\n",
        "    \"SPECIFIC_PROJECT_DETAILS\": \"The focus of this project is to study Iran's crises and develop strategies for their management...\",\n",
        "    \"X\": \"Iran's Crises Impacts\",\n",
        "    \"PROJECT_MANAGER\": \"Your Name\",\n",
        "    \"REPORT\": \"Detailed report on Iran's crisis research\",\n",
        "    \"IMPORTANT_THEMES\": \"Social Change, Research, Charitable Work\",\n",
        "    \"PROJECT_NAME\": \"Social Change Management Study\",\n",
        "    \"STAKEHOLDER\": \"Social Scientists, Researchers\",\n",
        "    \"RESISTANT_STAKEHOLDER\": \"N/A\",\n",
        "    \"TASK\": \"Generate a report on Iran's crisis and its management\",\n",
        "    \"YOUR_EMAIL\": \"your.email@example.com\",\n",
        "    \"OPENAI_API\": \"openai_api\",  # Replace with the actual OpenAI API key\n",
        "    \"category\":\"\",\n",
        "\n",
        "    \"employability\": \"Prepare students for employment in fields such as social sciences, human rights, and international relations.\",\n",
        "    \"practical_skills\": \"Develop practical skills in social analysis by focusing on the social, economic, and political concepts that can be applied across a wide range of sectors and industries.\",\n",
        "    \"creativity\": \"Encourage creative thinking and problem-solving in developing social strategies for organizations.\",\n",
        "    \"critical_thinking\": \"Foster critical thinking among students in analyzing social issues and formulating comprehensive solutions.\",\n",
        "    \"fun_and_enjoyment\": \"Make the course engaging with online lectures, discussions, and assignments, offering flexibility for students.\",\n",
        "    \"employee_guarantee\": \"Provide a guarantee for successful course completion by ensuring comprehensive coverage of key social topics.\",\n",
        "    \"collaboration\": \"Encourage collaboration among students in online forums and group projects.\",\n",
        "    \"learning_outcomes\": \"By the end of the course, students should be able to communicate complex social issues, analyze evidence to formulate social strategies, and lead social initiatives at local, national, and global levels.\",\n",
        "    \"purpose\": \"The purpose of this course is to equip students with the knowledge and skills necessary to advance their career in social sciences.\",\n",
        "    \"learning_activities\": \"Activities will include online lectures, discussions, assignments, and a capstone project where students help a real organization solve an existing social problem.\",\n",
        "    \"course_content\": \"The main topics covered in the course will include the relationship of humans with the natural environment, public policy and the role of government and business in social sciences, triple bottom line accounting, and social change.\",\n",
        "    \"course_assessments\": \"Student performance will be evaluated through assignments, group projects, and a capstone project.\",\n",
        "    \"course_schedule\": \"The course is delivered online and students can do homework whenever it’s convenient for them. The program also provides optional networking opportunities for students to connect virtually with peers, faculty, and professionals.\",\n",
        "    \"course_sequencing\": \"The course content will be sequenced to gradually increase in complexity, starting with an introduction to social sciences, moving on to more advanced topics, and ending with a capstone project.\",\n",
        "    \"technology_requirements\": \"Students will need access to a computer with internet connectivity. If software or special technology is required in one of the...\"\n",
        "}"
      ],
      "metadata": {
        "id": "GfxVDVzohe7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = {\n",
        "    \"TOPIC\": f\"{TOPIC}\",\n",
        "    \"RESEARCH_DOMAIN\": f\"{RESEARCH_DOMAIN}\",\n",
        "    \"PARAGRAPH\": f\"{PARAGRAPH}\",\n",
        "    \"PARAGRAPHS\": f\"{PARAGRAPHS}\",\n",
        "    \"TOPIC_SENTENCE\": f\"{TOPIC_SENTENCE}\",\n",
        "    \"LANGUAGE\": f\"{LANGUAGE}\",\n",
        "    \"ABSTRACT_PARAGRAPH\": f\"{ABSTRACT_PARAGRAPH}\",\n",
        "    \"BIBLIOGRAPHY\": f\"{BIBLIOGRAPHY}\",\n",
        "    \"THEORY1\": f\"{THEORY1}\",\n",
        "    \"THEORY2\": f\"{THEORY2}\",\n",
        "    \"RESEARCH_QUESTIONS\": f\"{RESEARCH_QUESTIONS}\",\n",
        "    \"ACTION\": f\"{ACTION}\",\n",
        "    \"RESULT_PARAGRAPHS\": f\"{RESULT_PARAGRAPHS}\",\n",
        "    \"DATE\": f\"{DATE}\",\n",
        "    \"NUMBER_OF_DAYS_MONTHS_YEARS\": f\"{NUMBER_OF_DAYS_MONTHS_YEARS}\",\n",
        "    \"ROLE\": f\"{ROLE}\",\n",
        "    \"PROJECT_EXAMPLE\": f\"{PROJECT_EXAMPLE}\",\n",
        "    \"CONTEXT\": f\"{CONTEXT}\",\n",
        "    \"INSTRUCTION\": f\"{INSTRUCTION}\",\n",
        "    \"OUTPUT_FORMAT\": f\"{OUTPUT_FORMAT}\",\n",
        "    \"SPECIFIC_PROJECT_DETAILS\": f\"{SPECIFIC_PROJECT_DETAILS}\",\n",
        "    \"X\": f\"{X}\",\n",
        "    \"PROJECT_MANAGER\": f\"{PROJECT_MANAGER}\",\n",
        "    \"REPORT\": f\"{REPORT}\",\n",
        "    \"IMPORTANT_THEMES\": f\"{IMPORTANT_THEMES}\",\n",
        "    \"PROJECT_NAME\": f\"{PROJECT_NAME}\",\n",
        "    \"STAKEHOLDER\": f\"{STAKEHOLDER}\",\n",
        "    \"RESISTANT_STAKEHOLDER\": f\"{RESISTANT_STAKEHOLDER}\",\n",
        "    \"TASK\": f\"{TASK}\",\n",
        "    \"YOUR_EMAIL\": f\"{YOUR_EMAIL}\",\n",
        "    \"OPENAI_API\": f\"{OPENAI_API}\",  # Replace with the actual OpenAI API key\n",
        "    \"category\":f\"{category}\",\n",
        "\n",
        "    \"employability\": f\"{employability}\",\n",
        "    \"practical_skills\": f\"{practical_skills}\",\n",
        "    \"creativity\": f\"{creativity}\",\n",
        "    \"critical_thinking\": f\"{critical_thinking}\",\n",
        "    \"fun_and_enjoyment\": f\"{fun_and_enjoyment}\",\n",
        "    \"employee_guarantee\": f\"{employee_guarantee}\",\n",
        "    \"collaboration\": f\"{collaboration}\",\n",
        "    \"learning_outcomes\": f\"{learning_outcomes}\",\n",
        "    \"purpose\": f\"{purpose}\",\n",
        "    \"learning_activities\": f\"{learning_activities}\",\n",
        "    \"course_content\": f\"{course_content}\",\n",
        "    \"course_assessments\": f\"{course_assessments}\",\n",
        "    \"course_schedule\": f\"{course_schedule}\",\n",
        "    \"course_sequencing\": f\"{course_sequencing}\",\n",
        "    \"technology_requirements\": f\"{technology_requirements}\",\n",
        "    \"prerequisites\": f\"{prerequisites}\",\n",
        "\n",
        "    \"topic\": f\"{topic}\",\n",
        "    \"field_of_study\": f\"{field_of_study}\",\n",
        "    \"audience\": f\"{audience}\",\n",
        "    \"specific_project\": f\"{specific_project}\",\n",
        "}"
      ],
      "metadata": {
        "id": "MeZ2rszaS6FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dvar_questions = {\n",
        "    \"TOPIC\": \"What is the main topic or focus of the content?\",\n",
        "    \"RESEARCH_DOMAIN\": \"Which field or domain does the research focus on?\",\n",
        "    \"PARAGRAPH\": \"Can you provide a summary of the paragraph?\",\n",
        "    \"PARAGRAPHS\": \"How are paragraphs represented in the content?\",\n",
        "    \"TOPIC_SENTENCE\": \"What is the main sentence summarizing the topic?\",\n",
        "    \"LANGUAGE\": \"In which language is the content written?\",\n",
        "    \"ABSTRACT_PARAGRAPH\": \"Can you summarize the abstract paragraph?\",\n",
        "    \"BIBLIOGRAPHY\": \"What sources are included in the bibliography?\",\n",
        "    \"THEORY1\": \"What is covered under the first theory?\",\n",
        "    \"THEORY2\": \"What is covered under the second theory?\",\n",
        "    \"THEORY2 & THEORY1\": [\"What is covered under the first theory?\",\"What is covered under the second theory?\"],\n",
        "    \"RESEARCH_QUESTIONS\": \"What are the primary research questions?\",\n",
        "    \"ACTION\": \"What actions are involved in the research and discussion?\",\n",
        "    \"RESULT_PARAGRAPHS\": \"What are the key results mentioned in the paragraphs?\",\n",
        "    \"DATE\": \"What is the date mentioned for the research?\",\n",
        "    \"NUMBER_OF_DAYS_MONTHS_YEARS\": \"What is the duration specified in terms of days, months, or years?\",\n",
        "    \"ROLE\": \"What is the role of a researcher?\",\n",
        "    \"PROJECT_EXAMPLE\": \"Can you provide an example of a project?\",\n",
        "    \"CONTEXT\": \"What is the context of the topic?\",\n",
        "    \"INSTRUCTION\": \"What is instructed for the study and research?\",\n",
        "    \"OUTPUT_FORMAT\": \"In what format is the report expected?\",\n",
        "    \"SPECIFIC_PROJECT_DETAILS\": \"Can you provide details on the focus of the project?\",\n",
        "    \"X\": \"What are the impacts of the subject?\",\n",
        "    \"PROJECT_MANAGER\": \"Who is the project manager?\",\n",
        "    \"REPORT\": \"Can you describe the detailed report?\",\n",
        "    \"IMPORTANT_THEMES\": \"What are the important themes related to the content?\",\n",
        "    \"PROJECT_NAME\": \"What is the name of the project?\",\n",
        "    \"STAKEHOLDER\": \"Who are the stakeholders?\",\n",
        "    \"RESISTANT_STAKEHOLDER\": \"Is there any resistant stakeholder mentioned?\",\n",
        "    \"TASK\": \"What is the task related to the subject?\",\n",
        "    \"YOUR_EMAIL\": \"What is your email address?\",\n",
        "    #\"OPENAI_API\": \"What is the OpenAI API key used for?\",\n",
        "    \"employability\": \"How does the course prepare students for employment?\",\n",
        "    \"practical_skills\": \"What practical skills are developed?\",\n",
        "    \"creativity\": \"How is creative thinking encouraged?\",\n",
        "    \"critical_thinking\": \"How is critical thinking fostered?\",\n",
        "    \"fun_and_enjoyment\": \"How is the course made engaging?\",\n",
        "    \"employee_guarantee\": \"What guarantee is provided for successful course completion?\",\n",
        "    \"collaboration\": \"How is collaboration encouraged among students?\",\n",
        "    \"learning_outcomes\": \"What are the expected learning outcomes?\",\n",
        "    \"purpose\": \"What is the purpose of the course?\",\n",
        "    \"learning_activities\": \"What activities are included in the course?\",\n",
        "    \"course_content\": \"What are the main topics covered in the course?\",\n",
        "    \"course_assessments\": \"How is student performance evaluated in the course?\",\n",
        "    \"course_schedule\": \"How is the course delivered, and what flexibility is provided to students?\",\n",
        "    \"course_sequencing\": \"How is the course content sequenced in terms of complexity?\",\n",
        "    \"technology_requirements\": \"What technology requirements do students need?\"\n",
        "}"
      ],
      "metadata": {
        "id": "-zyv3F-Hig7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "role = ROLE\n",
        "project_example = PROJECT_EXAMPLE\n",
        "context = CONTEXT\n",
        "instruction = INSTRUCTION\n",
        "specific_project_details = SPECIFIC_PROJECT_DETAILS\n",
        "project_manager = PROJECT_MANAGER\n",
        "report = REPORT\n",
        "important_themes = IMPORTANT_THEMES\n",
        "stakeholder = STAKEHOLDER\n",
        "resistant_stakeholder = RESISTANT_STAKEHOLDER\n",
        "#openai_api= OPENAI_API\n",
        "Your_Email = YOUR_EMAIL\n",
        "category = '{category}'"
      ],
      "metadata": {
        "id": "Ao416toLlHJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yogRktSU9FyG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMUZ5Z4xvxVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t68CyXWy9OC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install  python-docx huggingface_hub gradio"
      ],
      "metadata": {
        "id": "2wtcYrXAxS1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3glFxgwImgpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Anr_8FXjhp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqX1s4kBvvkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kzsu57V1wAL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "course_design_variables = {\n",
        "    \"employability\": f\"{employability}\",\n",
        "    \"practical_skills\": f\"{practical_skills}\",\n",
        "    \"creativity\": f\"{creativity}\",\n",
        "    \"critical_thinking\": f\"{critical_thinking}\",\n",
        "    \"fun_and_enjoyment\": f\"{fun_and_enjoyment}\",\n",
        "    \"employee_guarantee\": f\"{employee_guarantee}\",\n",
        "    \"collaboration\": f\"{collaboration}\",\n",
        "    \"learning_outcomes\": f\"{learning_outcomes}\",\n",
        "    \"purpose\": f\"{purpose}\",\n",
        "    \"learning_activities\": f\"{learning_activities}\",\n",
        "    \"course_content\": f\"{course_content}\",\n",
        "    \"course_assessments\": f\"{course_assessments}\",\n",
        "    \"course_schedule\": f\"{course_schedule}\",\n",
        "    \"course_sequencing\": f\"{course_sequencing}\",\n",
        "    \"technology_requirements\": f\"{technology_requirements}\",\n",
        "    \"prerequisites\": f\"{prerequisites}\",\n",
        "    \"topic\": f\"{TOPIC}\",\n",
        "    \"field_of_study\": f\"{field_of_study}\",\n",
        "    \"audience\": f\"{audience}\",\n",
        "    \"specific_project\": f\"{specific_project}\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt_course_old = [\n",
        "    f\" As ChatGPT expert in the role of course designing as {role}, suggest one Course Title in less than 15 words, based on This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "    f\"Step 1: As ChatGPT {role} in course designing Identify Situational Factors for {course_design_variables['specific_project']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"1. As ChatGPT {role} in course designing Understand the broader context of the {course_design_variables['field_of_study']} course for {course_design_variables['audience']} by the topic of:((( {course_design_variables['topic']} ))  and description of course_design_variables['specific_project'] \",\n",
        "    f\"2. As ChatGPT {role} in course designing Consider factors such as department or discipline, institution expectations, and student backgrounds and needs. Also, consider {course_design_variables['employability']} by the topic of:((( {course_design_variables['topic']} )) of the  {course_design_variables['field_of_study']}  and description of course_design_variables['specific_project'] \",\n",
        "    f\"3. As ChatGPT {role} in course designing Tailor the course content according to these factors to meet the needs of {course_design_variables['audience']}. {course_design_variables['practical_skills']} by the topic of:((( {course_design_variables['topic']} )) and description of course_design_variables['specific_project'] \",\n",
        "    f\"Step 2: As ChatGPT {role} in course designing Define Learning Outcomes by the topic of:((( {course_design_variables['topic']} )) and description of course_design_variables['specific_project'] \",\n",
        "    f\"4. As ChatGPT {role} in course designing Clearly articulate what you want your students to learn at the end of the {course_design_variables['topic']} course. {course_design_variables['learning_outcomes']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"5. As ChatGPT {role} in course designing Ensure these outcomes are measurable and directly related to the course content. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"6. As ChatGPT {role} in course designing Align the learning outcomes with the educational objectives of the institution and the career goals of the students in the {course_design_variables['field_of_study']}. {course_design_variables['purpose']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 3: As ChatGPT {role} in course designing Create Assessments by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"7. As ChatGPT {role} in course designing Develop assessments that effectively measure whether students have achieved the learning outcomes. {course_design_variables['course_assessments']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"8. As ChatGPT {role} in course designing Include various types of assessments such as exams, projects, presentations, and group work. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"9. As ChatGPT {role} in course designing Ensure the assessments are fair and accurately reflect the learning outcomes. Also, consider {course_design_variables['collaboration']} and {course_design_variables['fun_and_enjoyment']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 4: As ChatGPT {role} in course designing Plan Course Delivery by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"10. As ChatGPT {role} in course designing Decide on how you will deliver the {course_design_variables['field_of_study']} course content. {course_design_variables['learning_activities']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"11. As ChatGPT {role} in course designing Consider traditional lectures, discussions, labs, field trips, or a mix of these methods. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"12. As ChatGPT {role} in course designing Choose the delivery method that best facilitates learning for your students in the {course_design_variables['audience']}. Also, consider {course_design_variables['course_schedule']} and {course_design_variables['course_sequencing']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 5: As ChatGPT {role} in course designing Incorporate Universal Design for Learning (UDL) by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"13. As ChatGPT {role} in course designing UDL is a framework for designing instruction that accommodates the wide range of learning preferences and abilities among {course_design_variables['audience']}. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"14. As ChatGPT {role} in course designing Incorporate multiple modes of representation, expression, action, and engagement within the design of instruction. Also, consider {course_design_variables['technology_requirements']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 6: As ChatGPT {role} in course designing Experiential Learning by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"15. As ChatGPT {role} in course designing Tie theoretical knowledge to real-world experiences in the {course_design_variables['field_of_study']}. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"16. As ChatGPT {role} in course designing For instance, in a {course_design_variables['field_of_study']} course, students could be asked to develop a {course_design_variables['specific_project']} or solve a real-world problem using the concepts they've learned. in the project field of {course_design_variables['specific_project']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 7: As ChatGPT {role} in course designing Active Training by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"17. As ChatGPT {role} in course designing Engage students in active learning activities that require them to construct new knowledge through thinking and discussion. {course_design_variables['critical_thinking']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"18. As ChatGPT {role} in course designing This could include problem-solving exercises, case studies, simulations, or debates. by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"Step 8: As ChatGPT {role} in course designing Measurable Trainings by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"19. As ChatGPT {role} in course designing Ensure that your course has clear learning objectives and that you have ways to measure whether these objectives have been achieved. Also, consider {course_design_variables['employee_guarantee']} and {course_design_variables['prerequisites']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"20. As ChatGPT {role} in course designing This could involve pre-tests and post-tests, assignments, projects\"\n",
        "]\n",
        "\n",
        "prompt_course = [\n",
        "    f\" As ChatGPT expert in the role of course designing as {role}, suggest one Course Title in less than 15 words, based on This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "    f\"Step 1: As ChatGPT {role} in course designing Identify Situational Factors for {course_design_variables['topic']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"1. As ChatGPT {role} in course designing Understand the broader context of the {course_design_variables['field_of_study']} course for {course_design_variables['audience']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"2. As ChatGPT {role} in course designing Consider factors such as department or discipline, institution expectations, and student backgrounds and needs. Also, consider {course_design_variables['employability']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"3. As ChatGPT {role} in course designing Tailor the course content according to these factors to meet the needs of {course_design_variables['audience']}. {course_design_variables['practical_skills']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"Step 2: As ChatGPT {role} in course designing Define Learning Outcomes by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"4. As ChatGPT {role} in course designing Clearly articulate what you want your students to learn at the end of the {course_design_variables['topic']} course. {course_design_variables['learning_outcomes']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"5. As ChatGPT {role} in course designing Ensure these outcomes are measurable and directly related to the course content. by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"6. As ChatGPT {role} in course designing Align the learning outcomes with the educational objectives of the institution and the career goals of the students in the {course_design_variables['field_of_study']}. {course_design_variables['purpose']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"Step 3: As ChatGPT {role} in course designing Create Assessments by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"7. As ChatGPT {role} in course designing Develop assessments that effectively measure whether students have achieved the learning outcomes. {course_design_variables['course_assessments']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"8. As ChatGPT {role} in course designing Include various types of assessments such as exams, projects, presentations, and group work. by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"9. As ChatGPT {role} in course designing Ensure the assessments are fair and accurately reflect the learning outcomes. Also, consider {course_design_variables['collaboration']} and {course_design_variables['fun_and_enjoyment']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "\n",
        "    f\"Step 4: As ChatGPT {role} in course designing Plan Course Delivery by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"10. As ChatGPT {role} in course designing Decide on how you will deliver the {course_design_variables['field_of_study']} course content. {course_design_variables['learning_activities']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"11. As ChatGPT {role} in course designing Consider traditional lectures, discussions, labs, field trips, or a mix of these methods. by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"12. As ChatGPT {role} in course designing Choose the delivery method that best facilitates learning for your students in the {course_design_variables['audience']}. Also, consider {course_design_variables['course_schedule']} and {course_design_variables['course_sequencing']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"Step 5:  As ChatGPT {role} in course designing Incorporate multiple modes of representation, expression, action, and engagement within the design of instruction. Also, consider {course_design_variables['technology_requirements']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"13. As ChatGPT {role} in course designing Incorporate Universal Design for Learning (UDL) in the {course_design_variables['field_of_study']}. by the topic of:((( {course_design_variables['topic']} )), by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"14. As ChatGPT {role} in course designing Incorporate multiple modes of representation, expression, action, and engagement within the design of instruction. Also, consider {course_design_variables['technology_requirements']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "\n",
        "    f\"16. As ChatGPT {role} in course designing Tie theoretical knowledge to real-world experiences in the {course_design_variables['field_of_study']}. by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"Step 6: As ChatGPT {role} in course designing Experiential Learning by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']} in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"15. As ChatGPT {role} in course designing Tie theoretical knowledge to real-world experiences in the {course_design_variables['field_of_study']}. by the topic of:((( {course_design_variables['topic']} )) in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"16. As ChatGPT {role} in course designing For instance, in a {course_design_variables['field_of_study']} course, students could be asked to develop a {course_design_variables['specific_project']} or solve a real-world problem using the concepts they've learned. in the project field of {course_design_variables['specific_project']} by the topic of:((( {course_design_variables['topic']} ))\",\n",
        "    f\"17. As ChatGPT {role} in course designing Engage students in active learning activities that require them to construct new knowledge through thinking and discussion. {course_design_variables['critical_thinking']} by the topic of:((( {course_design_variables['topic']} )) in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"18. As ChatGPT {role} in course designing This could include problem-solving exercises, case studies, simulations, or debates. by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"Step 7: As ChatGPT {role} in course designing Active Training by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"17. As ChatGPT {role} in course designing Engage students in active learning activities that require them to construct new knowledge through thinking and discussion. {course_design_variables['critical_thinking']} by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"18. As ChatGPT {role} in course designing This could include problem-solving exercises, case studies, simulations, or debates. by the topic of:((( {course_design_variables['topic']} )) in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"Step 8: As ChatGPT {role} in course designing Measurable Trainings by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "    f\"19. As ChatGPT {role} in course designing Ensure that your course has clear learning objectives and that you have ways to measure whether these objectives have been achieved. Also, consider {course_design_variables['employee_guarantee']} and {course_design_variables['prerequisites']} by the topic of:((( {course_design_variables['topic']} )) in the project field of {course_design_variables['specific_project']}\",\n",
        "    f\"20. As ChatGPT {role} in course designing This could involve pre-tests and post-tests, assignments, projects or other forms of assessment for topic  of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "]\n",
        "prompt_course_presentation =[\n",
        "# Plan/Presentation\n",
        "  f\"Step 9: As ChatGPT {role} in Curiculium designing publishing Curiculium design draft via internet by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "  f\"21:.As ChatGPT role {role} in newspaper field, Write a sensational press release for this Curiculium designing and perviuse content made in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"22.As ChatGPT role of ({role}), Make this Curiculium designing, more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"23.As ChatGPT role of ({role}) in advertisment field, Write 3 tweeter post about this Curiculium designing, and this the perviuse contnet made?\",\n",
        "  f\"24.As ChatGPT role of ({role}) in advertisment field, Write 3 Instagram post about this Curiculium designing, and this the perviuse contnet made?\",\n",
        "  f\"25.As ChatGPT role of ({role}) in weblog writer, Write 1 medium weblog post about this Curiculium designing, and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role of ({role}) in bussiness post writing, Write 1 LinkedIn post about this Curiculium designing, and this the perviuse contnet made?\",\n",
        "  f\"27. As ChatGPT role of ({role}) write an email to related organization for introducing the opportunity of coaporation with us in this field by seeing the above report and the related LinkedIn, tweeter, Instagram, medium and the pdf file of this report \",\n",
        "\n",
        "]\n",
        "\n",
        "# Assign the prompts\n",
        "prompt_course_title = [\n",
        "f\" As ChatGPT expert in the role of course designing as {role}, suggest one Course Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "\n",
        " \"Step 1: Identify key factors.\",\n",
        " \"1-1. Understand the course context.\",\n",
        " \"1-2. Consider department, expectations, student needs.\",\n",
        " \"1-3. Tailor content to these factors.\",\n",
        " \"Step 2: Define learning outcomes.\",\n",
        " \"2-1. State student learning objectives.\",\n",
        " \"2-2. Ensure measurable, relevant outcomes.\",\n",
        " \"2-3. Align outcomes with objectives, goals.\",\n",
        " \"Step 3: Develop effective assessments.\",\n",
        " \"3-1. Assessments should measure learning outcomes.\",\n",
        " \"3-2. Use varied types of assessments.\",\n",
        " \"3-3. Ensure fair, accurate assessments.\",\n",
        " \"Step 4: Plan course delivery.\",\n",
        " \"4-1. Decide course content delivery method.\",\n",
        " \"4-2. Consider varied delivery methods.\",\n",
        " \"4-3. Choose best delivery method.\",\n",
        " \"Step 5: Incorporate Universal Design for Learning.\",\n",
        " \"5-1. UDL accommodates diverse learning preferences.\",\n",
        " \"5-2. Incorporate varied instruction design modes.\",\n",
        "\"5-3. Tie knowledge to real-world experiences.\",\n",
        "\"Step 6: Incorporate experiential learning.\",\n",
        " \"6-1. Connect theory to real-world experiences.\",\n",
        " \"6-2. Use field-specific projects for application.\",\n",
        "f\"6-3. Engage students in active learning activities for knowledge construction and discussion.\",\n",
        "f\"6-4. Incorporate problem-solving exercises for interactive learning.\",\n",
        "\"Step 7: Implement active training.\",\n",
        " \"7-1. Engage students in active learning.\",\n",
        " \"7-2. Use exercises, case studies, simulations.\",\n",
        " \"Step 8: Ensure measurable trainings.\",\n",
        " \"8-1. Course should have clear objectives.\",\n",
        " \"8-2. Measure objectives achievement effectively.\",\n",
        "\n",
        "]\n",
        "prompt_course_presentation_title =[\n",
        " #Plan/Presentation\n",
        "f\"Episode 9-1: Write a sensational press release for this\",\n",
        "f\"9-2 Make this more persuasive\",\n",
        "f\"9-3 3 tweets about this Report:\",\n",
        "\n",
        "f\"9-4 3 Instagram Post about this Report:\",\n",
        "f\"9-5 Medium Post about this Report:\",\n",
        "f\"9-6. LinkedIn Post about this Report:\",\n",
        "f\"9-7. Organization email for request to do coaporation:\",\n",
        "]"
      ],
      "metadata": {
        "id": "NxDofkTSCMRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming an average speaking speed of 150 words per minute\n",
        "words_per_minute = 180\n",
        "\n",
        "# Dialogue character variable\n",
        "dialogue_character = \" only two speaker whit name of So as first speaker by having medical doctor and climate change professional experience and the second person as radio speaker\"\n",
        "\n",
        "# Role variable\n",
        "#role = \"\"\n",
        "prompt_radio_script = [\n",
        "f\" As ChatGPT expert in the role of radio script wrtiter as {role}, suggest one Course Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "# Episode 1: \"Understanding Climate Change\"\n",
        "f\"As {role} in radio script writing, 1. Episode 1: Understanding Climate Change. Segment 1 (5 minutes): {dialogue_character}, discuss the basics of climate change, greenhouse gases, and the science behind global warming. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 2. Episode 1: Understanding Climate Change. Segment 2 (5 minutes): {dialogue_character}, provide real-life examples of climate change impacts, such as extreme weather events and sea-level rise. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 3. Episode 1: Understanding Climate Change. Segment 3 (5 minutes): {dialogue_character}, explain simple actions individuals can take to reduce their carbon footprint. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 2: \"Climate Change and Health\"\n",
        "f\"As  {role} in radio script writing, 4. Episode 2: Climate Change and Health. Segment 1 (5 minutes): {dialogue_character}, discuss the health impacts of climate change, including heat-related illnesses and the spread of diseases. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 5. Episode 2: Climate Change and Health. Segment 2 (5 minutes): {dialogue_character}, propose strategies for building climate-resilient healthcare systems. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 6. Episode 2: Climate Change and Health. Segment 3 (5 minutes): {dialogue_character}, offer tips for protecting personal health in a changing climate. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 3: \"Sustainable Agriculture and Food Choices\"\n",
        "f\"As  {role} in radio script writing, 7. Episode 3: Sustainable Agriculture and Food Choices. Segment 1 (5 minutes): {dialogue_character}, explain how climate change affects agriculture and food security. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 8. Episode 3: Sustainable Agriculture and Food Choices. Segment 2 (5 minutes): {dialogue_character}, showcase sustainable farming practices and local, eco-friendly food options. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 9. Episode 3: Sustainable Agriculture and Food Choices. Segment 3 (5 minutes): {dialogue_character}, provide tips for making sustainable food choices and reducing food waste. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 4: \"Biodiversity and Ecosystems\"\n",
        "f\"As  {role} in radio script writing, 10. Episode 4: Biodiversity and Ecosystems. Segment 1 (5 minutes): {dialogue_character}, discuss the importance of biodiversity in combating climate change. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 11. Episode 4: Biodiversity and Ecosystems. Segment 2 (5 minutes): {dialogue_character}, explore the impacts of climate change on ecosystems and wildlife. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 12. Episode 4: Biodiversity and Ecosystems. Segment 3 (5 minutes): {dialogue_character}, highlight conservation efforts and ways listeners can support biodiversity. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 5: \"The Role of Renewable Energy\"\n",
        "f\"As  {role} in radio script writing, 13. Episode 5: The Role of Renewable Energy. Segment 1 (5 minutes): {dialogue_character}, provide an overview of renewable energy sources and their importance in mitigating climate change. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 14. Episode 5: The Role of Renewable Energy. Segment 2 (5 minutes): {dialogue_character}, share success stories of communities or individuals transitioning to renewable energy. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 15. Episode 5: The Role of Renewable Energy. Segment 3 (5 minutes): {dialogue_character}, offer practical tips for incorporating renewable energy into daily life. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 6: \"The Importance of Trees and Green Spaces\"\n",
        "f\"As  {role} in radio script writing, 16. Episode 6: The Importance of Trees and Green Spaces. Segment 1 (5 minutes): {dialogue_character}, highlight the role of trees in carbon sequestration and air quality improvement. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 17. Episode 6: The Importance of Trees and Green Spaces. Segment 2 (5 minutes): {dialogue_character}, discuss community tree-planting initiatives and the benefits of urban green spaces. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 18. Episode 6: The Importance of Trees and Green Spaces. Segment 3 (5 minutes): {dialogue_character}, encourage listeners to participate in local tree-planting events and support green initiatives. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "# Episode 7: \"Consumerism and circular economy\"\n",
        "f\"As  {role} in radio script writing, 19. Episode 7: Consumerism and circular economy. Segment 1 (5 minutes): {dialogue_character}, explore the specific impacts of consumerism on climate change. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 20. Episode 7: Consumerism and circular economy. Segment 2 (5 minutes): {dialogue_character}, explain the concept of the circular economy. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 21. Episode 7: Consumerism and circular economy. Segment 3 (5 minutes): {dialogue_character}, feature local initiatives and community-led projects. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "\n",
        "\n",
        "# Episode 8: \"Taking Action: Advocacy and Policy Changes”\n",
        "f\"As  {role} in radio script writing, 22. Episode 8: Taking Action: Advocacy and Policy Changes. Segment 1 (5 minutes): {dialogue_character}, profile successful climate change policies and initiatives. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 23. Episode 8: Taking Action: Advocacy and Policy Changes. Segment 2 (5 minutes): {dialogue_character}, discuss the importance of advocacy and policy changes in addressing climate change. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "f\"As  {role} in radio script writing, 24. Episode 8: Taking Action: Advocacy and Policy Changes. Segment 3 (5 minutes): {dialogue_character}, suggest actionable steps to advocate for climate-friendly policies in their communities. TOPIC: {TOPIC}. DESCRIPTION: {PARAGRAPH}. Target Word Count: {5 * words_per_minute}.\",\n",
        "# End of the script\n",
        "]\n",
        "prompt_radio_script_presentation = [\n",
        "# Plan/Presentation\n",
        "  #f\"Step 9: As ChatGPT {role} in radio broadcast publishing radio script draft via internet by the topic of:((( {TOPIC} )), in the project field of {PARAGRAPH}.\",\n",
        "  f\"21:.As ChatGPT role {role} in newspaper field, Write a sensational press release for this radio script and perviuse content made in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"22.As ChatGPT role of ({role}), Make this radio script more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"23.As ChatGPT role of ({role}) in advertisment field, Write 3 tweeter post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"24.As ChatGPT role of ({role}) in advertisment field, Write 3 Instagram post about this radio script,  and this the perviuse contnet made?\",\n",
        "  f\"25.As ChatGPT role of ({role}) in weblog writer, Write 1 medium weblog post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role of ({role}) in bussiness post writing, Write 1 LinkedIn post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"27. As ChatGPT role of ({role}) write an email to related organization for introducing the opportunity of coaporation with us in this field by seeing the above report and the related LinkedIn, tweeter, Instagram, medium and the pdf file of this report \",\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "prompt_radio_script_title = [\n",
        "    f\" As ChatGPT expert in the role of radio script wrtiter as {role}, suggest one Course Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "\n",
        "    # Episode 1: \"Understanding Climate Change”\n",
        "    \"1-1. Basics of climate change and global warming.\",\n",
        "    \"1-2. Real-life impacts like extreme weather events.\",\n",
        "    \"1-3. Simple actions to reduce carbon footprint.\",\n",
        "\n",
        "    # Episode 2: \"Climate Change and Health\"\n",
        "    \"2-1. Health impacts, heat-related illnesses.\",\n",
        "    \"2-2. Strategies for climate-resilient healthcare systems.\",\n",
        "    \"2-3. Tips for personal health in changing climate.\",\n",
        "\n",
        "    # Episode 3: \"Sustainable Agriculture and Food Choices\"\n",
        "    \"3-1. Agriculture's impact on climate and food.\",\n",
        "    \"3-2. Showcase sustainable farming and local food.\",\n",
        "    \"3-3. Tips for sustainable food choices and waste.\",\n",
        "\n",
        "    # Episode 4: \"Biodiversity and Ecosystems\"\n",
        "    \"4-1. Importance of biodiversity in combating climate change.\",\n",
        "    \"4-2. Explore impacts on ecosystems and wildlife.\",\n",
        "    \"4-3. Highlight conservation efforts and support biodiversity.\",\n",
        "\n",
        "    # Episode 5: \"The Role of Renewable Energy\"\n",
        "    \"5-1. Overview of renewable energy sources.\",\n",
        "    \"5-2. Success stories of renewable energy transition.\",\n",
        "    \"5-3. Practical tips for incorporating renewable energy.\",\n",
        "\n",
        "    # Episode 6: \"The Importance of Trees and Green Spaces\"\n",
        "    \"6-1. Trees in carbon sequestration and air quality.\",\n",
        "    \"6-2. Community tree-planting and urban green spaces.\",\n",
        "    \"6-3. Encourage participation in local tree-planting events.\",\n",
        "\n",
        "    # Episode 7: \"Consumerism and Circular Economy\"\n",
        "    \"7-1. Impacts of consumerism on climate change.\",\n",
        "    \"7-2. Circular economy concept explanation.\",\n",
        "    \"7-3. Feature local initiatives addressing consumerism.\",\n",
        "\n",
        "    # Episode 8: \"Taking Action: Advocacy and Policy Changes”\n",
        "    \"8-1. Profile successful climate policies and initiatives.\",\n",
        "    \"8-2. Importance of advocacy in addressing climate change.\",\n",
        "    \"8-3. Suggest actionable steps for climate-friendly policies.\",\n",
        "]\n",
        "prompt_radio_script_presentation_title =[\n",
        "  #Plan/Presentation\n",
        "\n",
        "f\"Episode 9-1: Write a sensational press release for this\",\n",
        "f\"9-2 Make this more persuasive\",\n",
        "f\"9-3 3 tweets about this Report:\",\n",
        "\n",
        "f\"9-4 3 Instagram Post about this Report:\",\n",
        "f\"9-5 Medium Post about this Report:\",\n",
        "f\"9-6. LinkedIn Post about this Report:\",\n",
        "f\"9-7. Organization email for request to do coaporation:\",\n",
        "]"
      ],
      "metadata": {
        "id": "w5efmiQIyG4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming an average speaking speed of 150 words per minute\n",
        "words_per_minute = 180\n",
        "\n",
        "# Dialogue character variable\n",
        "dialogue_character = \" only two speaker whit name of So as first speaker by having medical doctor and climate change professional experience and the second person as radio speaker\"\n",
        "episode_Minute = 5\n",
        "\n",
        "# Role variable\n",
        "\n",
        "prompt_radio_script = [\n",
        "f\" As ChatGPT expert in the role of radio script wrtiter as {role}, suggest one Course Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "# Episode 1: \"Understanding Community Resilience\"\n",
        "    f\"As a {role} in radio script writing, 1-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, discuss the basics of community resilience, defining key concepts and principles. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 1-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, provide real-life examples of communities demonstrating resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 1-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, offer practical tips for individuals to contribute to building resilience in their communities. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 2: \"Empowering Local Initiatives\"\n",
        "    f\"As a {role} in radio script writing, 2-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, explore the importance of local initiatives in community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 2-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, showcase successful stories of communities implementing effective local initiatives. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 2-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, discuss ways individuals can get involved and support local initiatives for resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 3: \"Building Social Connectivity\"\n",
        "    f\"As a {role} in radio script writing, 3-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, emphasize the role of social connections in community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 3-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, share examples of communities fostering strong social ties. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 3-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, provide practical suggestions for enhancing social connectivity within communities. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 4: \"Crisis Response and Preparedness\"\n",
        "    f\"As a {role} in radio script writing, 4-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, discuss the importance of crisis response plans in community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 4-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, share examples of communities effectively handling crises. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 4-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, provide practical tips for communities to enhance crisis preparedness. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 5: \"Environmental Sustainability Initiatives\"\n",
        "    f\"As a {role} in radio script writing, 5-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, explore the intersection of environmental sustainability and community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 5-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, showcase successful environmental initiatives contributing to community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 5-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, discuss how individuals can participate in and support environmental sustainability efforts. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 6: \"Economic Empowerment Strategies\"\n",
        "    f\"As a {role} in radio script writing, 6-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, highlight the role of economic empowerment in community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 6-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, share examples of communities achieving economic resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 6-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, provide insights and tips for individuals to contribute to economic empowerment in their communities. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 7: \"Education and Skill Development\"\n",
        "    f\"As a {role} in radio script writing, 7-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, discuss the importance of education and skill development in enhancing community resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 7-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, share examples of communities successfully integrating education and skill development for resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 7-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, provide actionable steps for individuals to support education and skill development initiatives in their communities. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# Episode 8: \"Fostering Inclusive Communities\"\n",
        "    f\"As a {role} in radio script writing, 8-1. Segment 1 ({episode_Minute}/3): {dialogue_character}, explore the significance of inclusivity in building resilient communities. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 8-2. Segment 2 ({episode_Minute}/3): {dialogue_character}, share stories of communities fostering inclusivity and diversity. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "    f\"As a {role} in radio script writing, 8-3. Segment 3 ({episode_Minute}/3): {dialogue_character}, provide practical tips for individuals to contribute to creating inclusive communities for enhanced resilience. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "\n",
        "# End of the radio program\n",
        "    f\"As a {role} in radio script writing, 25. End of Radio Program. Summarize the key takeaways from the 8-episode series on {TOPIC}. Encourage listeners to share their thoughts and engage in community resilience initiatives. Based on this Topic: ({TOPIC}) and the description: ({PARAGRAPH}). Target Word Count: ({episode_Minute}/3)*{ words_per_minute}.\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt_radio_script_presentation = [\n",
        "# Plan/Presentation\n",
        "  #f\"Step 9: As ChatGPT {role} in radio broadcast publishing radio script draft via internet by the topic of:((( {TOPIC} )), in the project field of {PARAGRAPH}.\",\n",
        "  f\"21:.As ChatGPT role {role} in newspaper field, Write a sensational press release for this radio script and perviuse content made in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"22.As ChatGPT role of ({role}), Make this radio script more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"23.As ChatGPT role of ({role}) in advertisment field, Write 3 tweeter post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"24.As ChatGPT role of ({role}) in advertisment field, Write 3 Instagram post about this radio script,  and this the perviuse contnet made?\",\n",
        "  f\"25.As ChatGPT role of ({role}) in weblog writer, Write 1 medium weblog post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role of ({role}) in bussiness post writing, Write 1 LinkedIn post about this radio script, and this the perviuse contnet made?\",\n",
        "  f\"27. As ChatGPT role of ({role}) write an email to related organization for introducing the opportunity of coaporation with us in this field by seeing the above report and the related LinkedIn, tweeter, Instagram, medium and the pdf file of this report \",\n",
        "\n",
        "]\n",
        "\n",
        "prompt_radio_script_title = [\n",
        "    # General Information\n",
        "    \"1. Create a title for an 8-episode radio program on community resilience.\",\n",
        "\n",
        "    # Episode 1: \"Understanding Community Resilience\"\n",
        "    \"1-1. Basics of community resilience, defining key concepts.\",\n",
        "    \"1-2. Real-life examples of resilient communities.\",\n",
        "    \"1-3. Practical tips for individuals to build resilience.\",\n",
        "\n",
        "    # Episode 2: \"Empowering Local Initiatives\"\n",
        "    \"2-1. Importance of local initiatives in resilience.\",\n",
        "    \"2-2. Showcase successful stories of local initiatives.\",\n",
        "    \"2-3. Ways individuals can support local initiatives.\",\n",
        "\n",
        "    # Episode 3: \"Building Social Connectivity\"\n",
        "    \"3-1. Emphasize social connections in resilience.\",\n",
        "    \"3-2. Share examples of communities fostering social ties.\",\n",
        "    \"3-3. Provide practical suggestions for enhancing social connectivity.\",\n",
        "\n",
        "    # Episode 4: \"Crisis Response and Preparedness\"\n",
        "    \"4-1. Importance of crisis response plans.\",\n",
        "    \"4-2. Examples of communities handling crises.\",\n",
        "    \"4-3. Practical tips for communities in crisis preparedness.\",\n",
        "\n",
        "    # Episode 5: \"Environmental Sustainability Initiatives\"\n",
        "    \"5-1. Explore environmental sustainability and resilience.\",\n",
        "    \"5-2. Showcase successful environmental initiatives.\",\n",
        "    \"5-3. Discuss how individuals can support sustainability efforts.\",\n",
        "\n",
        "    # Episode 6: \"Economic Empowerment Strategies\"\n",
        "    \"6-1. Highlight economic empowerment in resilience.\",\n",
        "    \"6-2. Share examples of communities achieving economic resilience.\",\n",
        "    \"6-3. Insights and tips for individuals to contribute to economic empowerment.\",\n",
        "\n",
        "    # Episode 7: \"Education and Skill Development\"\n",
        "    \"7-1. Importance of education and skill development in resilience.\",\n",
        "    \"7-2. Examples of communities integrating education for resilience.\",\n",
        "    \"7-3. Provide actionable steps for individuals to support education initiatives.\",\n",
        "\n",
        "    # Episode 8: \"Fostering Inclusive Communities\"\n",
        "    \"8-1. Explore the significance of inclusivity in resilience.\",\n",
        "    \"8-2. Share stories of communities fostering inclusivity.\",\n",
        "    \"8-3. Provide practical tips for individuals to contribute to inclusive communities.\",\n",
        "\n",
        "    # End of the radio program\n",
        "    \"9. Summarize key takeaways from the 8-episode series on community resilience. Encourage engagement.\",\n",
        "]\n",
        "\n",
        "prompt_radio_script_presentation_title =[\n",
        "  #Plan/Presentation\n",
        "\n",
        "f\"Episode 9-1: Write a sensational press release for this\",\n",
        "f\"9-2 Make this more persuasive\",\n",
        "f\"9-3 3 tweets about this Report:\",\n",
        "\n",
        "f\"9-4 3 Instagram Post about this Report:\",\n",
        "f\"9-5 Medium Post about this Report:\",\n",
        "f\"9-6. LinkedIn Post about this Report:\",\n",
        "f\"9-7. Organization email for request to do coaporation:\",\n",
        "]"
      ],
      "metadata": {
        "id": "doSDd_Wgzo1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow #-contrib\n",
        "#!pip uninstall tensorflow -y\n",
        "#!pip3 install tensorflow==1.15.0\n",
        "#!pip3 install tensorflow==1.14.0\n",
        "#!pip3 install tensorflow==1.13.2"
      ],
      "metadata": {
        "id": "xj9xhsV9lN23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "main_variables_0={}\n",
        "prompt_save_var=[\n",
        "f\"Used value for this project\",\n",
        "f\"Values are :\\n {dvar_questions}\",\n",
        "f\"Values are :\\n {main_variables_0}\",\n",
        "\n",
        "]\n",
        "\n",
        "prompt_save_var_title = [\n",
        "f\"Used value for this project\",\n",
        "f\"Values are :\\n \",\n",
        "f\"variables of main_variables_0 are:\",\n",
        "]\n",
        "\n",
        "prompt_presentation = [\n",
        "    # Plan/Presentation\n",
        "    f\"Step presentation: As ChatGPT {role}, based of perviuse chat in the category of :(category :{category}), publish a draft via internet by the topic of: ((( {TOPIC} ))), withr perviuse chat related to the category of :({category}).\",\n",
        "    f\"21. As ChatGPT role {role},based of perviuse chat in the category of :(category: {category}), in the newspaper's field, Write a sensational press release and perviuse content made in this topic ({TOPIC}) and this field: '{PARAGRAPH}'\",\n",
        "    f\"22. As ChatGPT role of ({role}), based of perviuse chat in the category of :(category:{category}), please create more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "    f\"23. As ChatGPT role of ({role}) in advertisement field, Write 3 Twitter posts about this category of :({category}) report, by this  topic :({TOPIC}), and related previous chats and this description :({PARAGRAPH})?\",\n",
        "    f\"24. As ChatGPT role of ({role}) in advertisement field, Write 3 Instagram posts about this category of :({category}) report, by this  topic :({TOPIC}), and related previous chats and this description :({PARAGRAPH})?\",\n",
        "    f\"25. As ChatGPT role of ({role}) in weblogs Writing, Write 1medium posts about this category of :(category:{category}) report, by this topic :({TOPIC}), and related previous chats and this description :({PARAGRAPH})?\",\n",
        "    f\"26. As ChatGPT role of ({role}) in advertisement field and bussiness post writing, Write linkedin post about this category of :(category:{category}) report, by this topic :({TOPIC}), and related previous chats and this description :({PARAGRAPH})?\",\n",
        "    f\"27. As ChatGPT role of ({role}), and related perviuse chat the field of category of :(category :{category}), and topic of :({TOPIC}), write an email to the related organization for introducing the opportunity of cooperation with us in this field by seeing the above report and the related LinkedIn, Twitter, Instagram, medium, and the pdf file of this report\",\n",
        "]\n",
        "prompts_x_factor = [\n",
        "    f\" As a ChatGPT playing {role}, define your Unique Selling Proposition (USP) in the context of '{TOPIC}'. What sets your brand apart in the {field_of_study} market, making it unique and compelling to customers? Provide a detailed description: ({creativity})\",\n",
        "    f\" As a ChatGPT handling {role}, strategize ways to build defenses for long-term success in the {field_of_study} industry. Anticipate potential disruptions, especially those driven by technological advancements like AI, and outline plans for your business's resilience in the context of '{TOPIC}'. Elaborate: ({practical_skills})\",\n",
        "    f\" In {role} role of ChatGPT, specifically focusing on {ROLE}, assess and refine the brand tone for your business in the {field_of_study} sector. How can you ensure a consistent and effective brand voice, especially when hiring marketing team members or agencies, considering the specific project '{specific_project}'? Explain: ({collaboration})\",\n",
        "    f\" Taking on {role} role of ChatGPT as {ROLE}, investigate and address reasons why potential customers might not be converting into actual customers in the {field_of_study} domain, especially in the context of '{TOPIC}'. Analyze lost opportunities and propose actionable strategies to enhance customer conversion. Describe: ({learning_outcomes})\",\n",
        "    f\" In your capacity as ChatGPT with {role} role of {ROLE}, optimize email campaigns for increased effectiveness in the {field_of_study} market, considering the specific project '{specific_project}'. Review your email copy from a conversion perspective, suggest improvements, and align them with your brand voice. Outline your plan: ({critical_thinking})\",\n",
        "    f\" Assuming {role} role of ChatGPT as {ROLE}, apply the prompt to punch up your emails in the {field_of_study} context. How can you further enhance your email campaigns, making them more compelling and aligned with your brand voice, particularly considering '{specific_project}'? Provide a detailed description: ({purpose})\",\n",
        "]\n",
        "# Consolidated prompt\n",
        "se_cand_1_1_prompt = [\n",
        "    f\"1-1. We have generated reports addressing various aspects of {TOPIC} and are actively seeking collaboration with organizations or entities in the {RESEARCH_DOMAIN} domain. These reports highlight key findings and propose actions such as {ACTION}. Our recent work on anti-bullying initiatives and virtual courts in Iran's context aligns with the broader {IMPORTANT_THEMES} theme. We believe that cooperation with responsible organizations is crucial to making a positive impact. Can you recommend three organizations interested in this field or initiatives that align with our goals? Your insights on potential collaborators would greatly contribute to our efforts. Thank you!\\n\\n\",\n",
        "    # Prompt for creating the first email using the organization name from the prompt\n",
        "    f\"2-1. Subject: Exploring Collaboration on {TOPIC}\\nDear [Organization 1],\\n\\nI hope this email finds you well. We recently generated a comprehensive report on {TOPIC} and are impressed by your organization's commitment to {RESEARCH_DOMAIN}. Our report outlines key findings and proposes actionable strategies, including {ACTION}. We believe that collaborating with esteemed organizations like yours is essential to driving positive change in this field. Would you be open to exploring potential collaboration or discussing our findings further?\\n\\nBest regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompt for creating the second email using the organization name from the prompt\n",
        "    f\"2-2. Subject: Exploring Collaboration on {TOPIC}\\nDear [Organization 2],\\n\\nI trust this email reaches you in good health. We have recently conducted an in-depth analysis of {TOPIC} and are reaching out to organizations actively involved in {RESEARCH_DOMAIN}. Our report outlines actionable steps, such as {ACTION}, and emphasizes the importance of collaboration in achieving meaningful outcomes. We would be honored to discuss potential synergies with your organization. Are you available for a conversation or meeting in the coming weeks?\\n\\nWarm regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompt for creating the third email using the organization name from the prompt\n",
        "    f\"2-3. Subject: Exploring Collaboration on {TOPIC}\\nDear [Organization 3],\\n\\nI trust you are well. We have compiled a comprehensive report on {TOPIC}, and in our search for impactful collaborations, your organization came to our attention due to its significant contributions to {RESEARCH_DOMAIN}. Our report suggests strategies like {ACTION} for positive change, and we are eager to explore potential collaborations with organizations sharing our vision. Would you be open to a discussion or meeting to explore synergies?\\n\\nKind regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompts for creating three LinkedIn posts using the emails as a block code float\n",
        "    f\"3-1. LinkedIn Post 1:\\nExcited to explore collaboration with [Organization 1]! 🌐 Our recent report on {TOPIC} emphasizes key actions, including {ACTION}. Let's work together for positive change. #Collaboration #ResearchForChange\\n\\n```Subject: Exploring Collaboration on {TOPIC}\\n one of the perviuse email prompt ```\\n\\n\",\n",
        " ]\n",
        "\n",
        "se_request_1_1_prompt = [\n",
        "    f\"1-1. We have conducted comprehensive research on {TOPIC} and are eager to explore collaborations within the {RESEARCH_DOMAIN} domain. Our findings suggest that collaboration with other organizations could significantly enhance our impact, especially in areas such as {IMPORTANT_THEMES}. We are currently in need of assistance in identifying potential collaborators who share our vision and goals. Could you recommend three organizations or initiatives that might be interested in partnering with us? Your guidance would be invaluable in our quest for meaningful collaboration. We are also open to discussing potential financial support for this collaboration. Thank you!\\n\\n\",\n",
        "    # Prompt for creating the first email using the organization name from the prompt\n",
        "    f\"2-1. Subject: Seeking Collaboration and Financial Support on {TOPIC}\\nDear [Organization 1],\\n\\nI hope this message finds you well. Our team has been working diligently on a comprehensive study of {TOPIC}, and we believe that your organization's expertise and commitment to {RESEARCH_DOMAIN} could greatly enhance our research. We are looking for potential collaborators who can contribute to our goals, such as {ACTION}. We would greatly appreciate any recommendations or guidance you could provide on organizations that might be interested in partnering with us. Additionally, we are open to discussing potential financial support for this collaboration. Thank you for your time and consideration.\\n\\nBest regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompt for creating the second email using the organization name from the prompt\n",
        "    f\"2-2. Subject: Seeking Collaboration and Financial Support on {TOPIC}\\nDear [Organization 2],\\n\\nI trust this email finds you in good health. Our research on {TOPIC} has led us to seek out organizations that are actively involved in {RESEARCH_DOMAIN}. We believe that collaboration could lead to significant advancements in our field. We are reaching out to you for any recommendations or insights on potential collaborators who might align with our objectives. Your assistance would be greatly appreciated. We are also open to discussing potential financial support for this collaboration. Thank you for your consideration.\\n\\nWarm regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompt for creating the third email using the organization name from the prompt\n",
        "    f\"2-3. Subject: Seeking Collaboration and Financial Support on {TOPIC}\\nDear [Organization 3],\\n\\nI trust you are well. Our research on {TOPIC} has highlighted the importance of collaboration in achieving our goals. We are currently seeking assistance in identifying organizations that might be interested in partnering with us. Your expertise and insights could be invaluable in this process. Could you recommend any organizations or initiatives that might be a good fit for our collaboration? We are also open to discussing potential financial support for this collaboration. Thank you for your time and consideration.\\n\\nKind regards,\\n[Your Name]\\n[Your Position]\\n[Your Contact Information]\\n\\n\",\n",
        "    # Prompts for creating three LinkedIn posts using the emails as a block code float\n",
        "    f\"3-1. LinkedIn Post 1:\\nExcited to seek collaboration and discuss potential financial support with [Organization 1]! 🌐 Our research on {TOPIC} emphasizes the importance of teamwork. Let's work together for positive change. #Collaboration #ResearchForChange\\n\\n\",\n",
        "]\n",
        "\n",
        "questions_prompt = [\n",
        "f\"Given the tour of the Star Exchange forum, can you provide insights on a specific challenge faced by users or administrators? Your response should be focused on a single problem or issue, researchable using primary and/or secondary sources, feasible to answer within the timeframe and practical constraints, specific enough to answer thoroughly, complex enough to develop the answer over the space of a detailed paragraph, and relevant to the community and/or society more broadly. Please describe the challenge in the context of a {TOPIC} and elaborate on it in a detailed {PARAGRAPH}.\",\n",
        "]\n",
        "prompt_presentation  = prompts_x_factor + prompt_presentation + se_cand_1_1_prompt + se_request_1_1_prompt+ questions_prompt\n",
        "prompt_presentation_topic = [\n",
        "#Plan/Presentation\n",
        "f\"Episodes Presentation: A draft for publishing via internet:\",\n",
        "f\"P-1. Write a sensational press release for this\",\n",
        "f\"P-2. Make this more persuasive\",\n",
        "f\"P-3. 3 tweets about this Report:\",\n",
        "f\"P-4. 3 Instagram Post about this Report:\",\n",
        "f\"P-5. Medium Post about this Report:\",\n",
        "f\"P-6. LinkedIn Post about this Report:\",\n",
        "f\"P-7. Organization email for request to do coaporation:\",\n",
        "]\n",
        "prompts_x_factor_title = [\n",
        "    f\"Episodes X-Facort:\\n X-1. Crafting a Unique Selling Proposition\",\n",
        "    \"X-2. Building Long-Term Success Defenses\",\n",
        "    \"X-3. Refining Brand Tone for Consistency\",\n",
        "    \"X-4. Enhancing Customer Conversion Strategies\",\n",
        "    \"X-5. Optimizing Email Campaign Effectiveness\",\n",
        "    \"X-6. Applying Prompts to Enhance Email Campaigns\",\n",
        "]\n",
        "# Consolidated prompt without specific variables\n",
        "Section_Candidate_1_1_prompt_title = [\n",
        "    # Section_Candidate 1: Reports on Various Topics, Seeking Collaboration\n",
        "    \"Section_Candidate 1-1: Reports on various topics, Seeking Collaboration\",\n",
        "    # Section_Candidate 1-1: Request for Organization Recommendations\n",
        "    \"Section_Candidate 1-1: Seeking recommendations for organizations.\",\n",
        "    # Section_Candidate 1-2: Email 1 to Organization 1\n",
        "    \"Section_Candidate 1-2: Email 1 to Organization 1 seeking collaboration.\",\n",
        "    # Section_Candidate 1-3: Email 2 to Organization 2\n",
        "    \"Section_Candidate 1-3: Email 2 to Organization 2 seeking collaboration.\",\n",
        "    # Section_Candidate 1-4: Email 3 to Organization 3\n",
        "    \"Section_Candidate 1-4: Email 3 to Organization 3 seeking collaboration.\",\n",
        "    # Section_Candidate 1-5: LinkedIn Post 1\n",
        "    \"Section_Candidate 1-5: LinkedIn Post 1 announcing collaboration with Organization 1.\",\n",
        " ]\n",
        "se_request_1_1_prompt_topic = [\n",
        "\"Collaboration sought for domain-specific topic. Support welcomed.\",\n",
        "\n",
        "\"Your domain expertise valued. Open to discussions.\",\n",
        "\n",
        "\"Involvement enhances research. Financial support offered.\",\n",
        "\n",
        "\"Open to financial support and insights.\",\n",
        "\n",
        "\"Excited for collaboration. Engaged and ready.\"\n",
        "]\n",
        "\n",
        "\n",
        "questions_prompt_title = [\n",
        "\"Stackexchange question for this report:\",\n",
        "]\n",
        "prompt_presentation_topic = prompts_x_factor_title + prompt_presentation_topic + Section_Candidate_1_1_prompt_title +se_request_1_1_prompt_topic+ questions_prompt_title"
      ],
      "metadata": {
        "id": "XTKK1CauPLYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_party_chart= [\n",
        "    f\" As ChatGPT expert in the role of politics party construction script wrtiter as {role}, suggest one party chart Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "    f\"1. Define the core mission and values of {TOPIC} party based on the provided {PARAGRAPH}. How do these values align with the {RESEARCH_DOMAIN} and {IMPORTANT_THEMES}?\",\n",
        "    f\"2. Explain how the {TOPIC} party charter, as described in {PARAGRAPH}, embodies its commitment to democratic principles. How does this reflect the {THEORY1} and {THEORY2}?\",\n",
        "    f\"3. Outline the eligibility criteria for joining {TOPIC} party as outlined in {PARAGRAPH}. How do these criteria ensure inclusivity and representation within the {RESEARCH_DOMAIN}?\",\n",
        "    f\"4. Describe the rights and responsibilities of members within {TOPIC} party, as detailed in {PARAGRAPH}. How do these rights empower individuals to participate in {RESEARCH_QUESTIONS}?\",\n",
        "    f\"5. Detail the hierarchical structure of {TOPIC} party, as mentioned in {PARAGRAPH}. How does this structure facilitate effective leadership and decision-making in {field_of_study}?\",\n",
        "    f\"6. Explain how leaders are elected or appointed within {TOPIC} party, as indicated in {PARAGRAPH}. How does this process ensure accountability and transparency in {RESEARCH_DOMAIN}?\",\n",
        "    f\"7. Articulate the policy positions and platform of {TOPIC} party as outlined in {PARAGRAPH}. How do these positions address key issues in {RESEARCH_DOMAIN} and {IMPORTANT_THEMES}?\",\n",
        "    f\"8. Describe the primary, caucus, and convention systems used by {TOPIC} party for nominations, as detailed in {PARAGRAPH}. How do these systems promote engagement and involvement in {RESEARCH_DOMAIN}?\",\n",
        "    f\"9. Explain the fundraising and financial management strategies employed by {TOPIC} party, as discussed in {PARAGRAPH}. How do these strategies support the party's goals in {field_of_study} and {IMPORTANT_THEMES}?\",\n",
        "    f\"10. Discuss the rules and regulations governing campaign finance within {TOPIC} party, as outlined in {PARAGRAPH}. How do these regulations ensure transparency and integrity in {RESEARCH_DOMAIN}?\",\n",
        "    f\"11. Describe the decision-making processes within {TOPIC} party, as elaborated in {PARAGRAPH}. How do these processes foster collaboration and cooperation in {field_of_study}?\",\n",
        "    f\"12. Explain how meetings are conducted and votes are cast within {TOPIC} party, as detailed in {PARAGRAPH}. How do these procedures uphold democratic principles in {RESEARCH_DOMAIN}?\",\n",
        "    f\"13. Outline the procedures for amending the {TOPIC} party charter, as outlined in {PARAGRAPH}. How do these procedures facilitate adaptation and evolution in {field_of_study}?\",\n",
        "    f\"14. Describe the safeguards in place to ensure that amendments to the {TOPIC} party charter align with its core values, as mentioned in {PARAGRAPH}. How do these safeguards prevent deviation from the party's principles in {RESEARCH_DOMAIN}?\",\n",
        "    f\"15. Explain how {TOPIC} party ensures compliance with relevant laws and regulations, as discussed in {PARAGRAPH}. How does this compliance uphold the integrity of the party in {RESEARCH_DOMAIN}?\",\n",
        "    f\"16. Discuss the consequences of non-compliance with legal requirements for {TOPIC} party, as mentioned in {PARAGRAPH}. How do these consequences impact the party's reputation and effectiveness in {field_of_study}?\",\n",
        "    f\"17. Describe the mechanisms for soliciting input and feedback from members and stakeholders within {TOPIC} party, as outlined in {PARAGRAPH}. How do these mechanisms promote inclusivity and participation in {RESEARCH_DOMAIN}?\",\n",
        "    f\"18. Explain how {TOPIC} party incorporates feedback from consultations into its decision-making processes, as detailed in {PARAGRAPH}. How does this incorporation enhance the party's responsiveness and adaptability in {RESEARCH_QUESTIONS}?\",\n",
        "    f\"19. Outline the process for reviewing and revising the {TOPIC} party charter, as described in {PARAGRAPH}. How does this process ensure the relevance and effectiveness of the charter in {RESEARCH_DOMAIN}?\",\n",
        "    f\"20. Discuss the importance of consensus-building and compromise in finalizing the {TOPIC} party charter, as elaborated in {PARAGRAPH}. How does this consensus contribute to unity and cohesion within the party in {field_of_study}?\",\n",
        "    f\"21. Describe how the finalized {TOPIC} party charter is disseminated to members, candidates, and the public, as outlined in {PARAGRAPH}. How does this dissemination promote transparency and accountability in {RESEARCH_DOMAIN}?\",\n",
        "    f\"22. Discuss the role of the {TOPIC} party charter as a guiding document for members and supporters, as mentioned in {PARAGRAPH}. How does this charter inform decision-making and action in {field_of_study}?\",\n",
        "]\n",
        "\n",
        "prompt_party_chart_title = [\n",
        "    \"0.party chart title\",\n",
        "    \"1. Define mission and values of party.\",\n",
        "    \"2. Explain democratic commitment reflected in charter.\",\n",
        "    \"3. Outline membership criteria and rights.\",\n",
        "    \"4. Describe members' rights and responsibilities.\",\n",
        "    \"5. Detail party's hierarchical organizational structure.\",\n",
        "    \"6. Explain process of electing/appointing leaders.\",\n",
        "    \"7. Articulate policy positions and platform.\",\n",
        "    \"8. Describe nomination systems for candidates.\",\n",
        "    \"9. Explain fundraising and financial strategies.\",\n",
        "    \"10. Discuss rules governing campaign finance.\",\n",
        "    \"11. Describe decision-making processes within party.\",\n",
        "    \"12. Explain meeting conduct and voting procedures.\",\n",
        "    \"13. Outline procedures for amending charter.\",\n",
        "    \"14. Describe safeguards for charter amendments.\",\n",
        "    \"15. Explain compliance with relevant laws.\",\n",
        "    \"16. Discuss consequences of legal non-compliance.\",\n",
        "    \"17. Describe mechanisms for soliciting input.\",\n",
        "    \"18. Explain incorporation of feedback into decisions.\",\n",
        "    \"19. Outline process for reviewing charter.\",\n",
        "    \"20. Discuss importance of consensus-building.\",\n",
        "    \"21. Describe charter dissemination process.\",\n",
        "    \"22. Discuss role of charter as guiding document.\",\n",
        "]"
      ],
      "metadata": {
        "id": "d6e_xsdD7oX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_meeting_general = [\n",
        "    f\" As ChatGPT expert in the role of social meetings script wrtiter as {role}, suggest one meeting Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "    f\"Welcome all participants to the {TOPIC} meeting. Introduce yourself and state your role in the meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Briefly mention the purpose of the {TOPIC} and its significance in the {RESEARCH_DOMAIN}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Share the agenda for the {TOPIC} meeting, highlighting the {RESEARCH_DOMAIN} topics to be discussed. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Emphasize the importance of sticking to the agenda and time constraints for the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Provide a brief overview of the {TOPIC} and its relevance to the {RESEARCH_DOMAIN} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Invite participants to share their thoughts, ideas, or any updates related to {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Facilitate a productive discussion, ensuring that everyone has an opportunity to contribute to the {RESEARCH_DOMAIN} discussion. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Summarize the main points discussed and any decisions made regarding the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Summarize the key takeaways from the {TOPIC} meeting, highlighting any actionable items or next steps for the {RESEARCH_DOMAIN}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Thank all participants for their contributions and engagement in the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Provide any necessary closing remarks or announcements for the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"If there is time available, open the floor for questions from participants on the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Thank everyone once again for their participation and time in the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"Confirm any important follow-up actions, deadlines, or future meeting dates for the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"End the {TOPIC} meeting on a positive note, expressing anticipation for future successful meetings. with more detailed description by this content: ({PARAGRAPH})\"\n",
        "]\n",
        "\n",
        "\n",
        "prompt_meeting_general_title = [\n",
        "     \"1. Create a title for an  meeting program \",\n",
        "    #  1. Introduction\n",
        "    \"Welcome all participants to the meeting. Introduce yourself and state your role in the meeting.\",\n",
        "    \"Briefly mention the purpose of the meeting and its significance.\",\n",
        "\n",
        "    #  2. Agenda Review\n",
        "    \"Share the agenda for the meeting, highlighting the topics to be discussed.\",\n",
        "    \"Emphasize the importance of sticking to the agenda and time constraints.\",\n",
        "\n",
        "    #  3. Main Points (Topic  1)\n",
        "    \"Provide a brief overview of the topic and its relevance to the meeting.\",\n",
        "    \"Invite participants to share their thoughts, ideas, or any updates related to the topic.\",\n",
        "    \"Facilitate a productive discussion, ensuring that everyone has an opportunity to contribute.\",\n",
        "    \"Summarize the main points discussed and any decisions made.\",\n",
        "\n",
        "    #  4. Additional Topics (if applicable)\n",
        "    # Repeat the steps for any additional topics on the agenda.\n",
        "\n",
        "    #  5. Conclusion\n",
        "    \"Summarize the key takeaways from the meeting, highlighting any actionable items or next steps.\",\n",
        "    \"Thank all participants for their contributions and engagement.\",\n",
        "    \"Provide any necessary closing remarks or announcements.\",\n",
        "\n",
        "    #  6. Q&A Session (optional)\n",
        "    \"If there is time available, open the floor for questions from participants.\",\n",
        "    \"Address the questions and ensure clarity on any unclear points.\",\n",
        "\n",
        "    #  7. Closing\n",
        "    \"Thank everyone once again for their participation and time.\",\n",
        "    \"Confirm any important follow-up actions, deadlines, or future meeting dates.\",\n",
        "    \"End the meeting on a positive note, expressing anticipation for future successful meetings.\"\n",
        "]"
      ],
      "metadata": {
        "id": "GlJ0z5T757Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_meeting = [\n",
        "    f\" As ChatGPT expert in the role of social meetings script wrtiter as {role}, suggest one meeting Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "    f\"1. Welcome all participants to the {TOPIC} meeting. Introduce yourself and state your role in the meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"2. Briefly mention the purpose of the {TOPIC} and its significance in the {RESEARCH_DOMAIN}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"3. Share the agenda for the {TOPIC} meeting, highlighting the {RESEARCH_DOMAIN} topics to be discussed. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"4. Emphasize the importance of sticking to the agenda and time constraints for the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"5. Provide a brief overview of the {TOPIC} and its relevance to the {RESEARCH_DOMAIN} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"6. Invite participants to share their thoughts, ideas, or any updates related to {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"7. Facilitate a productive discussion, ensuring that everyone has an opportunity to contribute to the {RESEARCH_DOMAIN} discussion. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"8. Summarize the main points discussed and any decisions made regarding the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"9. Summarize the key takeaways from the {TOPIC} meeting, highlighting any actionable items or next steps for the {RESEARCH_DOMAIN}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"10. Thank all participants for their contributions and engagement in the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"11. Provide any necessary closing remarks or announcements for the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"12. If there is time available, open the floor for questions from participants on the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"13. Thank everyone once again for their participation and time in the {TOPIC} meeting. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"14. Confirm any important follow-up actions, deadlines, or future meeting dates for the {TOPIC}. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"15. End the {TOPIC} meeting on a positive note, expressing anticipation for future successful meetings. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"16. Acknowledge the challenges of facilitating difficult racial dialogues, including understanding one's own biases, creating safe conditions for expression, and navigating the complexities of racial tension. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"17. Highlight the importance of validating participants and encouraging open discussions, especially when it feels unsafe to do so. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"18. Discuss the significance of acknowledging personal biases and limitations in the dialogue, modeling truthfulness and openness to participants. with more detailed description by this content: ({PARAGRAPH})\",\n",
        "    f\"19. Emphasize the role of creating conditions for successful outcomes, ensuring participants feel heard and understood, and addressing racial tension directly. with more detailed description by this content: ({PARAGRAPH})\"\n",
        "]\n",
        "prompt_meeting_title = [\n",
        "    \"0. Create a title for an  meeting program \",\n",
        "    \"1. Welcome all participants to the meeting. Introduce yourself and state your role in the meeting.\",\n",
        "    \"2. Briefly mention the purpose of the meeting and its significance in the research domain.\",\n",
        "    \"3. Share the agenda for the meeting, highlighting the research domain topics to be discussed.\",\n",
        "    \"4. Emphasize the importance of sticking to the agenda and time constraints for the meeting.\",\n",
        "    \"5. Provide a brief overview of the topic and its relevance to the research domain meeting.\",\n",
        "    \"6. Invite participants to share their thoughts, ideas, or any updates related to the topic.\",\n",
        "    \"7. Facilitate a productive discussion, ensuring that everyone has an opportunity to contribute to the research domain discussion.\",\n",
        "    \"8. Summarize the main points discussed and any decisions made regarding the topic.\",\n",
        "    \"9. Summarize the key takeaways from the meeting, highlighting any actionable items or next steps for the research domain.\",\n",
        "    \"10. Thank all participants for their contributions and engagement in the meeting.\",\n",
        "    \"11. Provide any necessary closing remarks or announcements for the meeting.\",\n",
        "    \"12. If there is time available, open the floor for questions from participants on the topic.\",\n",
        "    \"13. Thank everyone once again for their participation and time in the meeting.\",\n",
        "    \"14. Confirm any important follow-up actions, deadlines, or future meeting dates for the topic.\",\n",
        "    \"15. End the meeting on a positive note, expressing anticipation for future successful meetings.\",\n",
        "    \"16. Acknowledge the challenges of facilitating difficult racial dialogues, including understanding one's own biases, creating safe conditions for expression, and navigating the complexities of racial tension.\",\n",
        "    \"17. Highlight the importance of validating participants and encouraging open discussions, especially when it feels unsafe to do so.\",\n",
        "    \"18. Discuss the significance of acknowledging personal biases and limitations in the dialogue, modeling truthfulness and openness to participants.\",\n",
        "    \"19. Emphasize the role of creating conditions for successful outcomes, ensuring participants feel heard and understood, and addressing racial tension directly.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-y9vzSpaSMry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define industries\n",
        "industries = [\n",
        "    \"Celebrities and Public Figures\",\n",
        "    \"Social Media Influencers\",\n",
        "    \"Politicians and Political Figures\",\n",
        "    \"Public Sector Professionals\",\n",
        "    \"Companies and Brands\",\n",
        "    \"Non-Profit Organizations\"\n",
        "]\n",
        "\n",
        "\n",
        "# Initialize industry counter\n",
        "i = 1\n",
        "Prompt_cancel_culture_title = [\n",
        "   f\" As ChatGPT expert in the role of social cancel culture wrtiter as {role}, suggest one meeting Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "]\n",
        "# Print numbered sections for each industry\n",
        "for industry in industries:\n",
        "    print(f\"Section {i}: {industry}\")\n",
        "    i += 1\n",
        "\n",
        "    # Initialize prompt counter\n",
        "    k = 1\n",
        "\n",
        "    # Create new prompts for each industry based on the common variables\n",
        "    new_prompts = [\n",
        "        # Numbered prompts for each industry\n",
        "        f\"Section {i}-{k}. Understanding {industry}: Cancel culture impact\",\n",
        "        f\"Section {i}-{k+1}. {industry}: Accountability vs. Punishment\",\n",
        "        f\"Section {i}-{k+2}. {industry}: Productivity of Calling Out\",\n",
        "        f\"Section {i}-{k+3}. {industry}: Agendas Behind Calling Out\",\n",
        "        f\"Section {i}-{k+4}. {industry}: Reaction to Offensive Content\",\n",
        "        f\"Section {i}-{k+5}. {industry}: Consequences and Empathy\",\n",
        "        f\"Section {i}-{k+6}. {industry}: Action Plan for Cancel Culture\",\n",
        "        f\"Section {i}-{k+7}. {industry}: Real-world Project Example\",\n",
        "        f\"Section {i}-{k+8}. {industry}: Research Instruction\",\n",
        "        f\"Section {i}-{k+9}. {industry}: Desired Output Format\",\n",
        "        f\"Section {i}-{k+10}. {industry}: Role and Duration\",\n",
        "        f\"Section {i}-{k+11}. {industry}: Communication Details\",\n",
        "        f\"Section {i}-{k+12}. {industry}: Using OpenAI API\",\n",
        "        f\"Section {i}-{k+13}. {industry}: Report Deadline\",\n",
        "        f\"Section {i}-{k+14}. {industry}: Essential Skills Required\",\n",
        "        f\"Section {i}-{k+15}. {industry}: Purpose and Learning Outcomes\",\n",
        "        f\"Section {i}-{k+16}. {industry}: Course Content and Assessments\",\n",
        "        f\"Section {i}-{k+17}. {industry}: Course Schedule and Sequencing\",\n",
        "        f\"Section {i}-{k+18}. {industry}: Technology Requirements\",\n",
        "        f\"Section {i}-{k+19}. {industry}: Target Audience and Field\",\n",
        "        f\"Section {i}-{k+20}. {industry}: Specific Project Details\"\n",
        "    ]\n",
        "\n",
        "    # Print the prompts for each industry\n",
        "    for prompt in new_prompts:\n",
        "        print(prompt)\n",
        "        Prompt_cancel_culture_title.append(prompt)\n",
        "\n",
        "    print()  # Empty line between industries\n",
        "\n",
        "reminded_prompts = [\n",
        "    f\"1.Understanding {TOPIC} in {industry}: {PARAGRAPHS} This prompt provides an introductory overview of cancel culture within the {industry} industry, focusing on its definition, evolution, and relevance within the broader societal context.\",\n",
        "    f\"2.{TOPIC_SENTENCE}: Accountability vs. Punishment in {RESEARCH_DOMAIN} for {industry} This prompt addresses the fundamental debate surrounding cancel culture within the {industry} industry, examining whether it primarily serves as a mechanism for holding individuals accountable or as a tool for punitive action.\",\n",
        "    f\"3.Productivity of Calling Out in {RESEARCH_DOMAIN} for {industry}: {PARAGRAPH} Here, the prompt explores the efficacy of calling out behavior within the {industry} industry, analyzing its effectiveness in driving positive change or perpetuating a culture of negativity.\",\n",
        "    f\"4.Agendas Behind Calling Out in {RESEARCH_DOMAIN} for {industry}: {PARAGRAPH} This prompt delves into the potential motives or hidden agendas behind instances of calling out within the {industry} industry, aiming to uncover any underlying biases or ulterior motives.\",\n",
        "    f\"5.Reaction to Offensive Content in {RESEARCH_DOMAIN} for {industry}: {PARAGRAPH} By examining how the {industry} industry typically responds to offensive content or behavior, this prompt sheds light on prevailing attitudes and norms regarding what constitutes acceptable or unacceptable conduct.\",\n",
        "    f\"6.Consequences and Empathy in {RESEARCH_DOMAIN} for {industry}: {PARAGRAPH} This prompt assesses the consequences of cancel culture within the {industry} industry, focusing on the level of empathy and understanding exhibited towards individuals facing cancellation or backlash.\",\n",
        "    f\"7.Action Plan for {ACTION} in {RESEARCH_DOMAIN} for {industry}: {RESULT_PARAGRAPHS} Here, the prompt outlines a strategic action plan for addressing cancel culture within the {industry} industry, detailing potential steps and outcomes.\",\n",
        "    f\"8.Project Example: {PROJECT_EXAMPLE} in {RESEARCH_DOMAIN} for {industry} by {PROJECT_MANAGER} This prompt provides a real-world example of how cancel culture manifests within the {industry} industry, showcasing a specific project or initiative led by a designated project manager.\",\n",
        "    f\"9.Instruction for {TOPIC} in {industry}: {INSTRUCTION} This prompt offers guidance or instructions for conducting research or analysis on cancel culture within the {industry} industry, outlining key steps or methodologies.\",\n",
        "    f\"10.Output Format for {OUTPUT_FORMAT} of {SPECIFIC_PROJECT_DETAILS} in {RESEARCH_DOMAIN} for {industry} Here, the prompt specifies the desired output format or deliverables for a particular research project or initiative within the {industry} industry, focusing on specific project details.\",\n",
        "    f\"11.Role: {ROLE} in {RESEARCH_DOMAIN} for {industry} for {NUMBER_OF_DAYS_MONTHS_YEARS} This prompt delineates the role or responsibilities of individuals within the {industry} industry, along with the expected duration or timeframe for fulfilling these roles.\",\n",
        "    f\"12.{X} in {RESEARCH_DOMAIN} for {industry}: {YOUR_EMAIL} This prompt highlights the importance of communication or collaboration within the {industry} industry, providing contact information for individuals involved in the project.\",\n",
        "    f\"13.Using {OPENAI_API} for {TOPIC} in {RESEARCH_DOMAIN} for {industry}: {PROJECT_MANAGER} Here, the prompt explores the potential use of the OpenAI API for conducting research or analysis on cancel culture within the {industry} industry, with the project manager overseeing its implementation.\",\n",
        "    f\"14.{REPORT} on {TOPIC} in {RESEARCH_DOMAIN} for {industry}: {DATE} This prompt outlines the creation of a report or documentation detailing research findings or insights on cancel culture within the {industry} industry, with a specified date for its completion.\",\n",
        "    f\"15.Skills Required: {employability}, {practical_skills}, {creativity}, {critical_thinking} in {RESEARCH_DOMAIN} for {industry} This prompt identifies the essential skills or competencies needed to effectively navigate cancel culture within the {industry} industry, including employability skills, practical skills, creativity, and critical thinking.\",\n",
        "    f\"16.Purpose and Learning Outcomes in {RESEARCH_DOMAIN} for {industry}: {purpose}, {learning_outcomes} Here, the prompt clarifies the overarching purpose and expected learning outcomes of engaging with cancel culture within the {industry} industry, providing a clear understanding of the intended objectives.\",\n",
        "    f\"17.Course Content and Assessments in {RESEARCH_DOMAIN} for {industry}: {course_content}, {course_assessments} This prompt outlines the content and assessments associated with a course or educational program focused on cancel culture within the {industry} industry, detailing the topics covered and methods of evaluation.\",\n",
        "    f\"18.Course Schedule and Sequencing in {RESEARCH_DOMAIN} for {industry}: {course_schedule}, {course_sequencing} This prompt presents the schedule and sequencing of a course or educational program on cancel culture within the {industry} industry, outlining the timeline and order of topics.\",\n",
        "    f\"19.Technology Requirements and Prerequisites in {RESEARCH_DOMAIN} for {industry}: {technology_requirements}, {prerequisites} Here, the prompt specifies the technology requirements and prerequisites for engaging with cancel culture research or analysis within the {industry} industry, ensuring participants have the necessary tools and background knowledge.\",\n",
        "    f\"20.Audience and Field of Study in {RESEARCH_DOMAIN} for {industry}: {audience}, {field_of_study} This prompt identifies the target audience and field of study for research or analysis on cancel culture within the {industry} industry, providing clarity on the intended recipients and disciplinary focus.\",\n",
        "    f\"21.Specific Project Details in {RESEARCH_DOMAIN} for {industry}: {specific_project} This prompt provides specific details or parameters for a research project or initiative focused on cancel culture within the {industry} industry, outlining key requirements or objectives.\"\n",
        "]\n",
        "Prompt_cancel_culture=[\n",
        " \"0. Create a title for an social cancel culture program.\",\n",
        "]\n",
        "for industry in industries:\n",
        "    for prompt in reminded_prompts:\n",
        "        print(prompt)\n",
        "        Prompt_cancel_culture.append(prompt)"
      ],
      "metadata": {
        "id": "ZTCFlXRaSPR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "\n",
        "from newspaper import Article\n",
        "import os\n",
        "import requests\n",
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        " course_data = {}\n",
        "\n",
        " # Retrieve course information from the web\n",
        " url = course_design_variables[\"url\"]\n",
        " article = Article(url)\n",
        " article.download()\n",
        " article.parse()\n",
        "\n",
        " # Extract course title\n",
        " course_title = article.title if article.title else \"Title not found on the page\"\n",
        " course_data['course_title'] = course_title\n",
        "\n",
        " # Extract course description\n",
        " course_description = article.text if article.text else \"Description not found on the page\"\n",
        " course_data['course_description'] = course_description\n",
        "\n",
        " # Extract authors\n",
        " authors = ', '.join(article.authors) if article.authors else \"Authors not found\"\n",
        " course_data['authors'] = authors\n",
        "\n",
        " # Extract publish date\n",
        " publish_date = article.publish_date if article.publish_date else \"Publish date not found\"\n",
        " course_data['publish_date'] = publish_date\n",
        "\n",
        " # Extract keywords\n",
        " keywords = ', '.join(article.keywords) if article.keywords else \"Keywords not found\"\n",
        " course_data['keywords'] = keywords\n",
        "\n",
        " return course_data, article\n",
        "\n",
        "# Example Usage\n",
        "course_url = \"https://uwex.wisconsin.edu/sustainable-management/masters/\"\n",
        "course_design_variables = {\"url\": course_url}\n",
        "\n",
        "# Extract course information\n",
        "course_data, article = extract_course_information(course_design_variables)\n",
        "\n",
        "if False:\n",
        "   # Print the extracted information\n",
        "   print(\"Course Title: \", course_data['course_title'])\n",
        "   print(\"Course Description: \", course_data['course_description'])\n",
        "   print(\"Authors: \", course_data['authors'])\n",
        "   print(\"Publish Date: \", course_data['publish_date'])\n",
        "   print(\"Keywords: \", course_data['keywords'])\n",
        "\n",
        "# Generate text with Sumy\n",
        "parser = PlaintextParser.from_string(course_data['course_description'], Tokenizer(\"english\"))\n",
        "summarizer = LsaSummarizer()\n",
        "summary_sumy = summarizer(parser.document, 3)\n",
        "print(\"\\nSumy Summary and remove the html content from this content :\\n\", summary_sumy)"
      ],
      "metadata": {
        "id": "82qSxXdsrvbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#from gpt_2_simple import gpt_2 as GPT2\n",
        "#from gpt2_client import GPT2Client\n",
        "#import tf_slim as slim\n",
        "\n",
        "def extract_course_information(course_design_variables,gpt):\n",
        "    #gpt = GPT2()\n",
        "    gpt.generate = gpt.generate_batch_from_prompts()\n",
        "    course_data = {}\n",
        "    course_data['topic'] = gpt.generate(prompt=f\"What is the topic of this course? {course_design_variables['topic']}\", text=course_design_variables['course_description'])\n",
        "    course_data['field_of_study'] = gpt.generate(prompt=f\"What is the field of study of this course? {course_design_variables['field_of_study']}\", text=course_design_variables['course_description'])\n",
        "    course_data['audience'] = gpt.generate(prompt=f\"Who is the target audience for this course? {course_design_variables['audience']}\", text=course_design_variables['course_description'])\n",
        "    course_data['specific_project'] = gpt.generate(prompt=f\"What is the specific project or assignment for this course? {course_design_variables['specific_project']}\", text=course_design_variables['course_description'])\n",
        "    course_data['employability'] = gpt.generate(prompt=f\"What are the employability or career benefits of this course? {course_design_variables['employability']}\", text=course_design_variables['course_description'])\n",
        "    course_data['practical_skills'] = gpt.generate(prompt=f\"What practical skills will students learn in this course? {course_design_variables['practical_skills']}\", text=course_design_variables['course_description'])\n",
        "    course_data['creativity'] = gpt.generate(prompt=f\"How does this course encourage creativity? {course_design_variables['creativity']}\", text=course_design_variables['course_description'])\n",
        "    course_data['critical_thinking'] = gpt.generate(prompt=f\"How does this course develop critical thinking skills? {course_design_variables['critical_thinking']}\", text=course_design_variables['course_description'])\n",
        "    course_data['fun_and_enjoyment'] = gpt.generate(prompt=f\"How does this course make learning engaging and enjoyable? {course_design_variables['fun_and_enjoyment']}\", text=course_design_variables['course_description'])\n",
        "    course_data['employee_guarantee'] = \"\"\n",
        "    course_data['collaboration'] = gpt.generate(prompt=f\"How does this course foster collaboration among students? {course_design_variables['collaboration']}\", text=course_design_variables['course_description'])\n",
        "    course_data['learning_outcomes'] = gpt.generate(prompt=f\"What are the learning outcomes or goals of this course? {course_design_variables['learning_outcomes']}\", text=course_design_variables['course_description'])\n",
        "    course_data['purpose'] = gpt.generate(prompt=f\"What is the purpose or objective of this course? {course_design_variables['purpose']}\", text=course_design_variables['course_description'])\n",
        "    course_data['learning_activities'] = gpt.generate(prompt=f\"What are the learning activities or methods used in this course? {course_design_variables['learning_activities']}\", text=course_design_variables['course_description'])\n",
        "    course_data['course_content'] = gpt.generate(prompt=f\"What are the main topics or subjects covered in this course? {course_design_variables['course_content']}\", text=course_design_variables['course_description'])\n",
        "    course_data['course_assessments'] = gpt.generate(prompt=f\"How are students assessed in this course? {course_design_variables['course_assessments']}\", text=course_design_variables['course_description'])\n",
        "    course_data['course_schedule'] = gpt.generate(prompt=f\"What is the schedule or timeline for this course? {course_design_variables['course_schedule']}\", text=course_design_variables['course_description'])\n",
        "    course_data['course_sequencing'] = gpt.generate(prompt=f\"How is the course content organized or sequenced? {course_design_variables['course_sequencing']}\", text=course_design_variables['course_description'])\n",
        "    course_data['technology_requirements'] = gpt.generate(prompt=f\"What technology or equipment is required for this course? {course_design_variables['technology_requirements']}\", text=course_design_variables['course_description'])\n",
        "    course_data['prerequisites'] = gpt.generate(prompt=f\"What are the prerequisites or prior knowledge required for this course? {course_design_variables['prerequisites']}\", text=course_design_variables['course_description'])\n",
        "\n",
        "    return course_data\n",
        "\n",
        "# Example Usage\n",
        "course_url = \"https://uwex.wisconsin.edu/sustainable-management/masters/\" # @param {type:\"string\"} # Wrap the URL in quotes\n",
        "response = requests.get(course_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "course_description_element = soup.find('div', class_='course-description__content')\n",
        "\n",
        "if course_description_element:\n",
        "    course_description = course_description_element.text.strip()\n",
        "else:\n",
        "    course_description = \"Description not found on the page\"\n",
        "\n",
        "# Dictionary of course design variables\n",
        "course_design_variables = course_design_variables\n",
        "\n",
        "course_design_variables['course_description'] = summary_sumy\n",
        "\n",
        "#gpt2 = GPT2Client('117M') # This could also be `345M`, `774M`, or `1558M`\n",
        "\n",
        "#text = gpt2.generate_batch_from_prompts(prompts) # returns an array of generated text\n",
        "\n",
        "#course_data = extract_course_information(course_design_variables,gpt2)\n",
        "#prompts = generate_prompts(course_data)\n",
        "#print(prompts)"
      ],
      "metadata": {
        "id": "NDhFuzJhLTwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from newspaper import Article\n",
        "import os\n",
        "import requests\n",
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "course_url = \"https://uwex.wisconsin.edu/sustainable-management/masters/\" # @param {type:\"string\"} # Wrap the URL in quotes\n",
        "response = requests.get(course_url)\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        " course_data = {}\n",
        "\n",
        " # Retrieve course information from the web\n",
        " url = course_design_variables[\"url\"]\n",
        " article = Article(url)\n",
        " article.download()\n",
        " article.parse()\n",
        "\n",
        " # Extract course title\n",
        " course_title = article.title if article.title else \"Title not found on the page\"\n",
        " course_data['course_title'] = course_title\n",
        "\n",
        " # Extract course description\n",
        " course_description = article.text if article.text else \"Description not found on the page\"\n",
        " course_data['course_description'] = course_description\n",
        "\n",
        " return course_data\n",
        "\n",
        "# Example Usage\n",
        "course_url = \"https://uwex.wisconsin.edu/sustainable-management/masters/\"\n",
        "course_design_variables = {\"url\": course_url}\n",
        "\n",
        "# Extract course information\n",
        "course_data = extract_course_information(course_design_variables)\n",
        "\n",
        "# Print the extracted information\n",
        "print(\"Course Title: \", course_data['course_title'])\n",
        "print(\"Course Description: \", course_data['course_description'])\n",
        "\n",
        "# Download the model if not already present"
      ],
      "metadata": {
        "id": "mRR0xliyRe7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#https://www.phind.com/search?cache=xqbhxgilysl4b8wqweih04eg\n",
        "\n",
        "Based on the search results, it seems you are interested in using Google's Text-to-Speech API to generate a voice for each `.docx` report. Here's how you can do it:\n",
        "\n",
        "First, install the necessary libraries:\n",
        "\n",
        "```python\n",
        "pip install python-docx google-cloud-texttospeech\n",
        "```\n",
        "\n",
        "Then, you can use the following code to read a `.docx` file and convert the text into speech:\n",
        "\n",
        "```python\n",
        "from docx import Document\n",
        "from google.cloud import texttospeech\n",
        "import os\n",
        "import random\n",
        "import slugify\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        " course_data = {}\n",
        "\n",
        " # Retrieve course information from the .docx file\n",
        " filename = course_design_variables[\"filename\"]\n",
        " doc = Document(filename)\n",
        " full_text = []\n",
        " for para in doc.paragraphs:\n",
        "    full_text.append(para.text)\n",
        " course_description = '\\n'.join(full_text) if full_text else \"Description not found in the file\"\n",
        " course_data['course_description'] = course_description\n",
        "\n",
        " return course_data, doc\n",
        "\n",
        "def generate_voice(course_data, TOPIC):\n",
        " # Convert the Sentence object to a string\n",
        " summary_sumy_str = course_data['course_description']\n",
        "\n",
        " # Initialize the Text-to-Speech client\n",
        " client = texttospeech.TextToSpeechClient()\n",
        "\n",
        " # Set the text input to be synthesized\n",
        " synthesis_input = texttospeech.SynthesisInput(text=summary_sumy_str)\n",
        "\n",
        " # Build the voice request\n",
        " voice = texttospeech.VoiceSelectionParams(\n",
        "    language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
        " )\n",
        "\n",
        " # Select the type of audio file you want returned\n",
        " audio_config = texttospeech.AudioConfig(\n",
        "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
        " )\n",
        "\n",
        " # Perform the text-to-speech request\n",
        " response = client.synthesize_speech(\n",
        "    input=synthesis_input, voice=voice, audio_config=audio_config\n",
        " )\n",
        "\n",
        " # Write the response to the output file.\n",
        " TOPIC_S = slugify(TOPIC)\n",
        " Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        " Sound_File = Sound_Folder +str(random.randint (0,1000))+\".mp3\"\n",
        "\n",
        " if not os.path.exists(Sound_Folder):\n",
        "    os.makedirs(Sound_Folder)\n",
        "\n",
        " with open(Sound_File, \"wb\") as out:\n",
        "    out.write(response.audio_content)\n",
        "\n",
        "# Example Usage\n",
        "course_filename = \"path_to_your_file.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "TOPIC = \"Your Topic\"\n",
        "\n",
        "# Extract course information\n",
        "course_data, doc = extract_course_information(course_design_variables)\n",
        "\n",
        "# Generate voice for the course\n",
        "generate_voice(course_data, TOPIC)\n",
        "```\n",
        "\n",
        "In this modified script, `Document(filename)` opens the `.docx` file, and `'\\n'.join(full_text)` combines all the text from the file into a single string. The rest of the script remains the same [Source 2](https://codelabs.developers.google.com/codelabs/cloud-text-speech-python3/), [Source 6](https://cloud.google.com/python/docs/reference/texttospeech/latest/google.cloud.texttospeech_v1.types.VoiceSelectionParams)."
      ],
      "metadata": {
        "id": "H3Z2VpIO0tlr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtJQMch81CC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        " course_data = {}\n",
        "\n",
        " # Retrieve course information from the .docx file\n",
        " filename = course_design_variables[\"filename\"]\n",
        " doc = Document(filename)\n",
        " full_text = []\n",
        " for para in doc.paragraphs:\n",
        "   full_text.append(para.text)\n",
        "   course_description = '\\n'.join(full_text) if full_text else \"Description not found in the file\"\n",
        "   course_data['course_description'] = course_description\n",
        "\n",
        " # Parse the text and generate a summary\n",
        " parser = PlaintextParser.from_string(course_description, Tokenizer(\"english\"))\n",
        " summarizer = LsaSummarizer()\n",
        "\n",
        " # Estimate the number of sentences needed for a 2-minute summary\n",
        " avg_speed = 150 # average words per minute\n",
        " est_num_sentences = int(2 * avg_speed) # 2 minutes in words\n",
        " summary = summarizer(parser.document, est_num_sentences)\n",
        "\n",
        " # Convert the Sentence object to a string\n",
        " summary_str = ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        " # Split the summary into parts\n",
        " course_parts = summary_str.split('\\n')\n",
        "\n",
        " return course_parts, doc"
      ],
      "metadata": {
        "id": "cQgZSXYl7_iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from google.cloud import texttospeech\n",
        "import os\n",
        "import random\n",
        "import slugify\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_voice(course_data, TOPIC):\n",
        " # Convert the Sentence object to a string\n",
        " summary_sumy_str = course_data['course_description']\n",
        "\n",
        " # Initialize the Text-to-Speech client\n",
        " client = texttospeech.TextToSpeechClient()\n",
        "\n",
        " # Set the text input to be synthesized\n",
        " synthesis_input = texttospeech.SynthesisInput(text=summary_sumy_str)\n",
        "\n",
        " # Build the voice request\n",
        " voice = texttospeech.VoiceSelectionParams(\n",
        "    language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
        " )\n",
        "\n",
        " # Select the type of audio file you want returned\n",
        " audio_config = texttospeech.AudioConfig(\n",
        "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
        " )\n",
        "\n",
        " # Perform the text-to-speech request\n",
        " response = client.synthesize_speech(\n",
        "    input=synthesis_input, voice=voice, audio_config=audio_config\n",
        " )\n",
        "\n",
        " # Write the response to the output file.\n",
        " TOPIC_S = slugify(TOPIC)\n",
        " Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        " Sound_File = Sound_Folder +str(random.randint (0,1000))+\".mp3\"\n",
        "\n",
        " if not os.path.exists(Sound_Folder):\n",
        "    os.makedirs(Sound_Folder)\n",
        "\n",
        " with open(Sound_File, \"wb\") as out:\n",
        "    out.write(response.audio_content)\n",
        "\n",
        "# Example Usage\n",
        "course_filename = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/Title/Pitch_Deck_T_climate-change-_t.docx\" # \"path_to_your_file.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "#TOPIC = \"Your Topic\"\n",
        "\n",
        "# Extract course information\n",
        "#course_data, doc = extract_course_information(course_design_variables)\n",
        "\n",
        "# Generate voice for the course\n",
        "#generate_voice(course_data, TOPIC)"
      ],
      "metadata": {
        "id": "-aNw4MOM04gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!sudo apt-get install ffmpeg\n",
        "!pip install python-slugify\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "Njd04Vde9ZWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "from pydub import AudioSegment\n",
        "from slugify import slugify\n",
        "from langdetect import detect\n",
        "import time\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "client = OpenAI(\n",
        " api_key = openai_api\n",
        ")\n",
        "\n",
        "def retry_with_exponential_backoff(func, initial_delay: float = 1, exponential_base: float = 2, jitter: bool = True, max_retries: int = 15, errors: tuple = (RateLimitError,)):\n",
        " def wrapper(*args, **kwargs):\n",
        "    num_retries = 0\n",
        "    delay = initial_delay\n",
        "    while True:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except errors as e:\n",
        "            num_retries += 1\n",
        "            if num_retries > max_retries:\n",
        "              raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n",
        "            delay *= exponential_base * (1 + jitter * random.random())\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            raise e\n",
        " return wrapper\n",
        "\n",
        "@retry_with_exponential_backoff\n",
        "def generate_voice_openai(part):\n",
        " if not part:\n",
        "    print(\"Warning: Empty input received.\")\n",
        "    return None\n",
        " response = client.audio.speech.create(\n",
        "    voice=\"alloy\",\n",
        "    model=\"tts-1\",\n",
        "    input=part\n",
        " )\n",
        " return response\n",
        "\n",
        "def sumerizing(doc):\n",
        " content = \"\"\n",
        " for para in doc.paragraphs:\n",
        "    content += para.text + \"\\n\"\n",
        "\n",
        " # Detect the language of the content\n",
        " try:\n",
        "    language = detect(content)\n",
        "    print(f\"Detected language: {language}\")\n",
        " except langdetect.lang_detect_exception.LangDetectException:\n",
        "    print(\"Could not detect language.\")\n",
        "\n",
        " parser = PlaintextParser.from_string(content , Tokenizer(language))\n",
        " summarizer = LsaSummarizer()\n",
        " summary_sumy = summarizer(parser.document, 3)\n",
        " sumerized_content = \" \".join(str(sentence) for sentence in summary_sumy)\n",
        " print(\"\\nSumy Summary and remove the html content from this content :\\n\", sumerized_content)\n",
        " return sumerized_content\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        " course_data = {}\n",
        " filename = course_design_variables[\"filename\"]\n",
        " doc = Document(filename)\n",
        " full_text = []\n",
        " for para in doc.paragraphs:\n",
        "  full_text.append(para.text)\n",
        " course_description = '\\n'.join(full_text) if full_text else \"Description not found in the file\"\n",
        " course_data['course_description'] = course_description\n",
        " course_parts = [sent for para in full_text for sent in sent_tokenize(para)]\n",
        " sumerized_content= sumerizing(doc)\n",
        " return course_parts, doc,sumerized_content\n",
        "\n",
        "def generate_voice(course_parts, TOPIC):\n",
        " TOPIC_S = slugify(TOPIC)\n",
        " Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        " if not os.path.exists(Sound_Folder):\n",
        "    os.makedirs(Sound_Folder)\n",
        " audio_files = []\n",
        " if True: #for i, part in enumerate(course_parts):\n",
        "    #print (f'\\n The {i}th part of main variable is :',part)\n",
        "    #if not part:\n",
        "       # continue\n",
        "    response = generate_voice_openai(course_parts)\n",
        "    Sound_File = Sound_Folder +str(course_parts[:10])+\".mp3\"\n",
        "    response.stream_to_file(Sound_File)\n",
        "    print ('\\n Response for voice generation is made at : ', Sound_File)\n",
        "    audio_files.append(Sound_File)\n",
        " combined_audio = AudioSegment.empty()\n",
        " for af in audio_files:\n",
        "    combined_audio += AudioSegment.from_file(af)\n",
        " avg_speed = 150\n",
        " total_words = len(' '.join(course_parts).split())\n",
        " est_duration = total_words / avg_speed\n",
        " n = 5\n",
        " if est_duration < n:\n",
        "    num_repetitions = int((n / est_duration) + 1)\n",
        "    empty_audio = AudioSegment.silent(duration=len(combined_audio))\n",
        "    extended_audio = empty_audio.overlay(combined_audio, times=num_repetitions)\n",
        "    extended_audio.export(Sound_Folder+\"extended.mp3\", format='mp3')\n",
        " combined_audio.export(Sound_Folder+\"combined.mp3\", format='mp3')\n",
        "\n",
        "course_filename = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/Title/Pitch_Deck_T_climate-change-_t.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "#course_parts, doc,sumerized_content = extract_course_information(course_design_variables)\n",
        "#generate_voice(sumerized_content, TOPIC)"
      ],
      "metadata": {
        "id": "vgQtEAVnfYMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bl5wSWx0nin3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uaDF7NAu2GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4EHpU98vgv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIg4siLIuvdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xF-ZkR6PwUxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4qxO8WZwBOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload posts to linkedin 🔥🌹👇🙏🌀:"
      ],
      "metadata": {
        "id": "evu3izeBUIVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/So-AI-love/chatgpt-prompts-for-academic-writing\n",
        "\n",
        "%cd chatgpt-prompts-for-academic-writing"
      ],
      "metadata": {
        "id": "P7TwfsyaUUB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Topic = TOPIC\n",
        "#TOPIC_0 = f\"Analyze the current situation in the Islamic Republic of Iran and the challenges faced by resistance forces. And Identify effective strategies used by resistance movements to navigate challenging political environments.\"\n",
        "#TOPIC_0 = f\"Could you provide more insights into the challenges faced by the opposition groups in Iran and how they could overcome these challenges to effectively organize resistance against the Islamic Republic?\"\n",
        "\n",
        "Question=Topic\n",
        "#PASSWORD =  # @param {type:\"string\"}\n",
        "if not (Question == \"\"):\n",
        "\n",
        "    TOPIC  = Question\n",
        "\n",
        "else:\n",
        "    TOPIC  = TOPIC_0\n",
        "\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "# @markdown *After signing\n",
        "print ('Your question is:',TOPIC)# has done successfully\")\n",
        "\n",
        "main_variables_0 = {\n",
        "      'TOPIC': TOPIC,\n",
        "      'RESEARCH_DOMAIN': RESEARCH_DOMAIN,\n",
        "      'PARAGRAPH': PARAGRAPH,\n",
        "      'PARAGRAPHS': PARAGRAPHS,\n",
        "      'TOPIC_SENTENCE': TOPIC_SENTENCE,\n",
        "      'LANGUAGE': LANGUAGE,\n",
        "      'ABSTRACT_PARAGRAPH': ABSTRACT_PARAGRAPH,\n",
        "      'BIBLIOGRAPHY': BIBLIOGRAPHY,\n",
        "      'THEORY1': THEORY1,\n",
        "      'THEORY2': THEORY2,\n",
        "      'RESEARCH_QUESTIONS': RESEARCH_QUESTIONS,\n",
        "      'ACTION': ACTION,\n",
        "      'RESULT_PARAGRAPHS': RESULT_PARAGRAPHS,\n",
        "      'DATE': DATE,\n",
        "      'NUMBER_OF_DAYS_MONTHS_YEARS': NUMBER_OF_DAYS_MONTHS_YEARS,\n",
        "      'role' : role,\n",
        "      'category':f\"{category}\",\n",
        "  }\n",
        "\n",
        "# List of prompts for generating academic papers\n",
        "prompts_Academic = [\n",
        "f\" As ChatGPT expert in the role of {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "# Improving Language\n",
        "f\"Write a counterargument to the following claim: '{PARAGRAPH}'\",\n",
        "f\"Rewrite this in an academic voice: '{PARAGRAPH}'\",\n",
        "f\"Expand these notes: '{PARAGRAPH}'\",\n",
        "f\"Provide me a list of words and phrases which were repeatedly / more than 3 times used: '{PARAGRAPHS}'\",\n",
        "f\"Provide me a list of synonyms for '{PARAGRAPH}' and evaluate them in the context of '{PARAGRAPH}'\",\n",
        "f\"Act as a language expert, proofread my paper on '{TOPIC_SENTENCE}' while putting a focus on grammar and punctuation.\",\n",
        "f\"In the context of '{RESEARCH_DOMAIN}' translate '{PARAGRAPH}' into the '{LANGUAGE}' language.\",\n",
        "\n",
        "  # Brainstorming\n",
        "  f\"Find a research topic for a PhD in the area of '{TOPIC}'\",\n",
        "  f\"Write a detailed proposal on the following research topic. Make sure it is free from plagiarism. '{PARAGRAPH}'\",\n",
        "  f\"Identify gaps in the literature on '{TOPIC_SENTENCE}'\",\n",
        "  f\"Generate 10 academic research questions about '{PARAGRAPHS}'\",\n",
        "  f\"Generate a list of research hypotheses related to '{TOPIC_SENTENCE}'\",\n",
        "  f\"Identify potential areas for future research in the context of this '{TOPIC_SENTENCE}'\",\n",
        "  f\"Suggest novel applications of '{TOPIC_SENTENCE}' within '{RESEARCH_DOMAIN}'\",\n",
        "\n",
        "  # Title/Topic Sentence\n",
        "  f\"Suggest 5 titles for the following abstract: '{ABSTRACT_PARAGRAPH}'\",\n",
        "  f\"Write a topic sentence for this paragraph: '{PARAGRAPH}'\",\n",
        "\n",
        "  # Keywords\n",
        "  f\"Provide 5 keywords for this: '{PARAGRAPHS}'\",\n",
        "\n",
        "  # Abstract\n",
        "  f\"Generate an abstract for a scientific paper based on this information for: '{PARAGRAPHS}'\",\n",
        "\n",
        "  # Outline\n",
        "  f\"Generate an outline for '{TOPIC_SENTENCE}'\",\n",
        "  f\"I want to write a journal article about '{TOPIC_SENTENCE}'. Give me an outline for the article that I can use as a starting point.\",\n",
        "\n",
        "  # Introduction\n",
        "  f\"Come up with an introduction for the following research topic: '{TOPIC_SENTENCE}'\",\n",
        "\n",
        "  # Literature Review\n",
        "  f\"Conduct a literature review on '{TOPIC_SENTENCE}' and provide review paper references\",\n",
        "  f\"Provide me with references and links to papers in '{PARAGRAPH}'\",\n",
        "  f\"Summarize the scholarly literature including in-text citations on '{PARAGRAPHS}'\",\n",
        "  f\"Write this in standard Harvard referencing '{PARAGRAPH}'\",\n",
        "  f\"Convert this '{BIBLIOGRAPHY}' from MLA to APA style.\",\n",
        "  f\"Compare and contrast '{THEORY1}' and '{THEORY2}' in the context of '{RESEARCH_DOMAIN}'\",\n",
        "\n",
        "  # Methodology\n",
        "   f\"Create objectives and methodology for '{TOPIC_SENTENCE}'\",\n",
        "   f\"Write a detailed methodology for the topic: '{TOPIC_SENTENCE}'\",\n",
        "   f\"Analyze the strengths and weaknesses of this methodology: '{PARAGRAPHS}'\",\n",
        "   f\"Write objectives for this study: '{TOPIC_SENTENCE}'\",\n",
        "   f\"What are the limitations of using '{TOPIC_SENTENCE}' in '{RESEARCH_DOMAIN}'?\",\n",
        "   f\"Create a recipe for the methods used in this '{PARAGRAPHS}'\",\n",
        "   f\"Suggest interdisciplinary approaches to '{TOPIC_SENTENCE}'\",\n",
        "   f\"Explain how qualitative/quantitative research methods can be used to address '{RESEARCH_QUESTIONS}'\",\n",
        "   f\"Recommend best practices for data collection and analysis in '{TOPIC_SENTENCE}'\",\n",
        "\n",
        "   # Experiments\n",
        "   f\"Design an experiment that '{ACTION}'\",\n",
        "\n",
        "   # Results\n",
        "   f\"Write a result section for the following paragraphs. Please write this in the third person. '{PARAGRAPHS}'\",\n",
        "\n",
        "   # Discussion\n",
        "   f\"Discuss this results: '{RESULT_PARAGRAPHS}'\",\n",
        "\n",
        "   # Conclusion\n",
        "   f\"Generate a conclusion for this: '{PARAGRAPHS}'\",\n",
        "   f\"Give recommendations and conclusion for: '{PARAGRAPHS}'\",\n",
        "\n",
        "   # Future Works\n",
        "   f\"Can you suggest 3 directions for future research on this topic: '{PARAGRAPH}'\",\n",
        "\n",
        "   # Plan/Presentation\n",
        "   f\"Develop a research plan for: '{TOPIC_SENTENCE}'\",\n",
        "   f\"Write a schedule for completion in '{TOPIC_SENTENCE}' in NUMBER OF DAYS MONTHS YEARS which is '{NUMBER_OF_DAYS_MONTHS_YEARS}'\",\n",
        "   f\"The deadline for the submission of the first draft is '{DATE}'. Give me a week-by-week breakdown so I can plan my writing better.\",\n",
        "   f\"Write a sensational press release for this research: '{PARAGRAPHS}'\",\n",
        "   f\"Make this more persuasive: '{PARAGRAPH}'\",\n",
        "   f\"Write 3 tweets about this research? '{PARAGRAPHS}'\",\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_Academic = [\n",
        "f\" As ChatGPT expert in the role of {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "\n",
        "  # Improving Language\n",
        "  f\"1. Write a counterargument to the following claim: ''\",\n",
        "  f\"2. Rewrite this in an academic voice: ''\",\n",
        "  f\"3. Expand these notes: ''\",\n",
        "  f\"4. Provide me a list of words and phrases which were repeatedly / more than 3 times used: ''\",\n",
        "  f\"5. Provide me a list of synonyms for '' and evaluate them in the context of ''\",\n",
        "  f\"6. Act as a language expert, proofread my paper on '' while putting a focus on grammar and punctuation.\",\n",
        "  f\"7. In the context of '' translate '' into the '' language.\",\n",
        "\n",
        "  # Brainstorming\n",
        "  f\"8. Find a research topic for a PhD in the area of ''\",\n",
        "  f\"9. Write a detailed proposal on the following research topic. Make sure it is free from plagiarism. ''\",\n",
        "  f\"10. Identify gaps in the literature on ''\",\n",
        "  f\"11. Generate 10 academic research questions about ''\",\n",
        "  f\"12. Generate a list of research hypotheses related to ''\",\n",
        "  f\"13. Identify potential areas for future research in the context of this ''\",\n",
        "  f\"14. Suggest novel applications of '' within ''\",\n",
        "\n",
        "  # Title/Topic Sentence\n",
        "  f\"15. Suggest 5 titles for the following abstract: ''\",\n",
        "  f\"16. Write a topic sentence for this paragraph: ''\",\n",
        "\n",
        "  # Keywords\n",
        "  f\"17. Provide 5 keywords for this: ''\",\n",
        "\n",
        "  # Abstract\n",
        "  f\"18. Generate an abstract for a scientific paper based on this information for: ''\",\n",
        "\n",
        "  # Outline\n",
        "  f\"19. Generate an outline for ''\",\n",
        "  f\"20. I want to write a journal article about ''. Give me an outline for the article that I can use as a starting point.\",\n",
        "\n",
        "  # Introduction\n",
        "  f\"21. Come up with an introduction for the following research topic: ''\",\n",
        "\n",
        "  # Literature Review\n",
        "  f\"22. Conduct a literature review on '' and provide review paper references\",\n",
        "  f\"23. Provide me with references and links to papers in ''\",\n",
        "  f\"24. Summarize the scholarly literature including in-text citations on ''\",\n",
        "  f\"25. Write this in standard Harvard referencing ''\",\n",
        "  f\"26. Convert this '' from MLA to APA style.\",\n",
        "  f\"27. Compare and contrast '' and '' in the context of ''\",\n",
        "\n",
        "  # Methodology\n",
        "  f\"28. Create objectives and methodology for ''\",\n",
        "  f\"29. Write a detailed methodology for the topic: ''\",\n",
        "  f\"30. Analyze the strengths and weaknesses of this methodology: ''\",\n",
        "  f\"31. Write objectives for this study: ''\",\n",
        "  f\"32. What are the limitations of using '' in ''?\",\n",
        "  f\"33. Create a recipe for the methods used in this ''\",\n",
        "  f\"34. Suggest interdisciplinary approaches to ''\",\n",
        "  f\"35. Explain how qualitative/quantitative research methods can be used to address ''\",\n",
        "  f\"36. Recommend best practices for data collection and analysis in ''\",\n",
        "\n",
        "  # Experiments\n",
        "  f\"37. Design an experiment that ''\",\n",
        "\n",
        "  # Results\n",
        "  f\"38. Write a result section for the following paragraphs. Please write this in the third person. ''\",\n",
        "\n",
        "  # Discussion\n",
        "  f\"39. Discuss this results: ''\",\n",
        "\n",
        "  # Conclusion\n",
        "  f\"40. Generate a conclusion for this: ''\",\n",
        "  f\"41. Give recommendations and conclusion for: ''\",\n",
        "\n",
        "  # Future Works\n",
        "  f\"42. Can you suggest 3 directions for future research on this topic: ''\",\n",
        "\n",
        "  # Plan/Presentation\n",
        "  f\"43. Develop a research plan for: ''\",\n",
        "  f\"44. Write a schedule for completion in '' in NUMBER OF DAYS MONTHS YEARS which is ''\",\n",
        "  f\"45. The deadline for the submission of the first draft is ''. Give me a week-by-week breakdown so I can plan my writing better.\",\n",
        "  f\"46. Write a sensational press release for this research: ''\",\n",
        "  f\"47. Make this more persuasive: ''\",\n",
        "  f\"48. Write 3 tweets about this research? ''\",\n",
        "]"
      ],
      "metadata": {
        "id": "ndWVOITXN7Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal adding 🙏👇🌸"
      ],
      "metadata": {
        "id": "Jl9PPovDuhlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#TOPIC_0 = f\"Analyze the current situation in the Islamic Republic of Iran and the challenges faced by resistance forces. And Identify effective strategies used by resistance movements to navigate challenging political environments.\"\n",
        "#TOPIC_0 = f\"Could you provide more insights into the challenges faced by the opposition groups in Iran and how they could overcome these challenges to effectively organize resistance against the Islamic Republic?\"\n",
        "\n",
        "Question=Topic\n",
        "#PASSWORD =  # @param {type:\"string\"}\n",
        "if not (Question == \"\"):\n",
        "\n",
        "    TOPIC  = Question\n",
        "\n",
        "else:\n",
        "    TOPIC  = TOPIC_0\n",
        "\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "# @markdown *After signing\n",
        "print ('Your question is:',TOPIC)# has done successfully\")\n",
        "main_variables_0 = {\n",
        "   'TOPIC': TOPIC,\n",
        "   'PARAGRAPH':PARAGRAPH,\n",
        "    'role':role\n",
        "}\n",
        "\n",
        "prompts = [\n",
        "# Understanding the Issue\n",
        "f\"What are the key aspects of {main_variables_0['TOPIC']} that need to be addressed?\",\n",
        "\n",
        "# Mind Mapping\n",
        "f\"Create a mind map of the main elements of {main_variables_0['TOPIC']} and how they are interconnected.\",\n",
        "\n",
        "# Affinity Diagramming\n",
        "f\"Write down ideas on sticky notes about how to improve {main_variables_0['TOPIC']}. Then, group these ideas based on common themes.\",\n",
        "\n",
        "# Round-Robin Brainstorming\n",
        "f\"Take turns sharing ideas on how to improve {main_variables_0['TOPIC']}. Make sure everyone has a chance to contribute.\",\n",
        "\n",
        "# Reverse Brainstorming\n",
        "f\"Think of ways to create the problem in {main_variables_0['TOPIC']}. This will help you understand how to solve it.\",\n",
        "\n",
        "# SCAMPER\n",
        "f\"Apply the SCAMPER method to existing solutions or situations in {main_variables_0['TOPIC']}. This will spark new ideas.\",\n",
        "\n",
        "# Cross-Functional Brainstorming\n",
        "f\"Assemble a diverse group of people from various backgrounds and disciplines to tackle {main_variables_0['TOPIC']}. This will ensure a more rounded and innovative solution.\",\n",
        "\n",
        "# Future Backwards\n",
        "f\"Envision a future state where {main_variables_0['TOPIC']} has been solved successfully. Starting from this endpoint, work backward to identify the series of events and decisions that led to that future.\"\n",
        "]"
      ],
      "metadata": {
        "id": "rmbkC20f6RWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOPIC_0 = \"task to investigate various gaming groups in Iran, following the Mahsa Movement in the second year. The gaming groups being investigated include the dark solidarity group, the dark trilogy, the dark quadrilogy, the regular people, the bright solidarity group, and the bright trilogy. The aim is to gather information about each group, analyze their characteristics, objectives, and any associated psychological syndromes. This information will then be presented in a table for easier understanding and comparison.\" # Replace this with your topic\n",
        "#RESEARCH_DOMAIN = \"Environmental startup field\" # Replace this with your research domain\n",
        "\n",
        "global docx_path,Pdf_Dir,prompt_Word_Topic\n",
        "\n",
        "if not (Question == \"\"):\n",
        "\n",
        "    TOPIC  = Question\n",
        "\n",
        "else:\n",
        "    TOPIC  = TOPIC_0\n",
        "\n",
        "main_variables_0 = {\n",
        "   'TOPIC': TOPIC,\n",
        "   'RESEARCH_DOMAIN' : RESEARCH_DOMAIN,\n",
        "   'PARAGRAPH':PARAGRAPH\n",
        "}\n",
        "\n",
        "TOPIC = f\"{main_variables_0['TOPIC']}\"\n",
        "RESEARCH_DOMAIN = \"{RESEARCH_DOMAIN}\"\n",
        "\n",
        "prompts_old = [\n",
        "  f\"For the {TOPIC} of Business Overview, provide a detailed description of your business, including its location, legal structure, owners, vision, mission, and history.\",\n",
        "  f\"For the {TOPIC} of Market Analysis, provide information about your target market, market size, growth potential, competitors, market trends, and regulatory environment.\",\n",
        "  f\"For the {TOPIC} of Products and Services, describe your products and services in detail.\",\n",
        "  f\"For the {TOPIC} of Marketing and Sales Strategies, outline your marketing, sales, pricing, and customer retention strategies.\",\n",
        "  f\"For the {TOPIC} of Operations Plan, describe daily business activities, individuals responsible, tools and equipment required, inventory, cost, and any other special requirements.\",\n",
        "  f\"For the {TOPIC} of Management Team, describe the founders, key executives, senior management, their educational and professional background, compensation plan, business hierarchy, and business advisors/consultants.\",\n",
        "  f\"For the {TOPIC} of Financial Plan, provide a thorough understanding of operational costs, net profit, and financing to estimate revenue projections.\",\n",
        "  f\"For the {TOPIC} of Executive Summary, provide an overview of the entire business plan. This is usually written after the entire plan is ready.\",\n",
        "  f\"For the {TOPIC} of Appendix, provide additional information supporting your business plan’s main content.\"\n",
        "]\n",
        "\n",
        "prompts_old_2 = [\n",
        "f\"suggest one Business Plans repost Title based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\"\n",
        " f\"Provide a detailed description of your business, including its location, legal structure, owners, vision, mission, and history. This is for the {TOPIC} of Business Overview. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Provide information about your target market, market size, growth potential, competitors, market trends, and regulatory environment. This is for the {TOPIC} of Market Analysis.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Describe your products and services in detail. This is for the {TOPIC} of Products and Services.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Outline your marketing, sales, pricing, and customer retention strategies. This is for the {TOPIC} of Marketing and Sales Strategies.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Describe daily business activities, individuals responsible, tools and equipment required, inventory, cost, and any other special requirements. This is for the {TOPIC} of Operations Plan.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Describe the founders, key executives, senior management, their educational and professional background, compensation plan, business hierarchy, and business advisors/consultants. This is for the {TOPIC} of Management Team.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Provide a thorough understanding of operational costs, net profit, and financing to estimate revenue projections. This is for the {TOPIC} of Financial Plan.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Provide an overview of the entire business plan. This is usually written after the entire plan is ready. This is for the {TOPIC} of Executive Summary.More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        " f\"Provide additional information supporting your business plan’s main content. This is for the {TOPIC} of Appendix.More Description about the Topic is show at here: {PARAGRAPH}\"\n",
        "f\"Provide additional information about SWOT for supporting your business plan’s main content. This is for the {TOPIC} of Appendix.More Description about the Topic is show at here: {PARAGRAPH}\"\n",
        "\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_old_2 = [\n",
        "   f\"suggest one Business Plans repost Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\"\n",
        "   \"Provide business overview details\",\n",
        "   \"Describe target market information\",\n",
        "   \"Detail products and services\",\n",
        "   \"Outline marketing, sales strategies\",\n",
        "   \"Describe daily business activities\",\n",
        "   \"Detail management team\",\n",
        "   \"Understand operational costs, profit\",\n",
        "   \"Provide business plan overview\",\n",
        "   \"Support business plan with additional info\"\n",
        "   \" SWOT Analysis \"\n",
        "]"
      ],
      "metadata": {
        "id": "kWdIhuI-yhPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.phind.com/search? cache=dmmj0id2em6rm8lo88sqhm6o\n",
        "prompts_old_3 = [\n",
        "  f\"suggest one Business Plans repost Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "\n",
        "  f\"1. **Executive Summary**: Provide a concise summary of the business, its goals, and the market it operates in. This is for the {TOPIC} of Executive Summary. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"2. **Company Description**: Provide a detailed description of the company, its mission, vision, and the problem it aims to solve. This is for the {TOPIC} of Company Description. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"3. **Market Analysis**: Provide a comprehensive PESTEL analysis for the company, including Political, Economic, Sociocultural, Technological, Environmental, and Legal factors. This is for the {TOPIC} of Market Analysis. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"4. **Organization and Management**: Describe the company's organizational structure, its team, and their roles and responsibilities. This is for the {TOPIC} of Organization and Management. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"5. **Service or Product Line**: Describe the services or products offered by the company. This is for the {TOPIC} of Service or Product Line. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"6. **Marketing and Sales Strategy**: Outline the strategies for marketing and sales, including the target audience, user stories, suitable business strategies, and marketing platforms. This is for the {TOPIC} of Marketing and Sales Strategy. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"7. **Funding Request**: Detail the amount of funding needed, how it will be used, and the expected return on investment. This is for the {TOPIC} of Funding Request. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"8. **Financial Projections**: Provide financial forecasts for the next few years, including revenue, costs, and profitability. This is for the {TOPIC} of Financial Projections. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"9. **Appendix**: Include any additional information that supports the business plan, such as legal documents, contracts, or market research data. This is for the {TOPIC} of Appendix. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"10. **Industry Insight**: Provide a detailed industry insight for the business plan. This is for the {TOPIC} of Industry Insight. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"11. **SWOT Analysis**: Conduct a SWOT analysis for the business plan. This is for the {TOPIC} of SWOT Analysis. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"12. **Target Audience and User Stories**: Identify the target audience and user stories for the business plan. This is for the {TOPIC} of Target Audience and User Stories. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"13. **Suitable Business Strategies**: Provide suitable business strategies for the business plan. This is for the {TOPIC} of Suitable Business Strategies. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"14. **Business Frameworks**: Provide business frameworks for the business plan. This is for the {TOPIC} of Business Frameworks. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"15. **Requirements Analysis**: Conduct a requirements analysis for the business plan. This is for the {TOPIC} of Requirements Analysis. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"16. **Additional Revenue Streams**: Identify additional revenue streams for the business plan. This is for the {TOPIC} of Additional Revenue Streams. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"17. **Marketing Strategy and Brand Awareness**: Provide a marketing strategy and brand awareness for the business plan. This is for the {TOPIC} of Marketing Strategy and Brand Awareness. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"18. **Branding Suggestions**: Provide branding suggestions for the business plan. This is for the {TOPIC} of Branding Suggestions. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"19. **Recommended Marketing Platforms**: Recommend marketing platforms for the business plan. This is for the {TOPIC} of Recommended Marketing Platforms. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"20. **Game-Changing Idea**: Provide a game-changing idea for the business plan. This is for the {TOPIC} of Game-Changing Idea. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"21. **Porter's Five Forces Analysis**: Conduct a Porter's Five Forces analysis for the business plan. This is for the {TOPIC} of Porter's Five Forces Analysis. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "  f\"22. **CATWOE Analysis**: Provide a CATWOE analysis for the business plan. This is for the {TOPIC} of CATWOE Analysis. More Description about the Topic is show at here: {PARAGRAPH}\",\n",
        "]\n",
        "\n",
        "prompts_business_plan = [\n",
        "    f\"suggest one Business Plans repost Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "    f\"1. **Executive Summary** (As a {role}): In this section, provide a concise summary of the business highlighting its unique value proposition, target market, and projected growth. Describe the company's goals, mission, and the market landscape it operates in. This is for the {TOPIC} of Executive Summary. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"2. **Company Description** (As a {role}): Detail the company's history, its founding principles, values, and the problem it addresses. Explain the company's vision, its core competencies, and how it stands out in the market. This is for the {TOPIC} of Company Description. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"3. **Market Analysis** (As a {role}): Conduct an in-depth PESTEL analysis covering Political, Economic, Sociocultural, Technological, Environmental, and Legal factors. Provide insights into market trends, potential risks, and opportunities. This is for the {TOPIC} of Market Analysis. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"4. **Organization and Management** (As a {role}): Outline the company's organizational structure, key personnel, their roles, and responsibilities. Explain how the team contributes to the company's success. This is for the {TOPIC} of Organization and Management. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"5. **Service or Product Line** (As a {role}): Elaborate on the services or products offered by the company. Highlight their unique features, benefits, and how they fulfill market needs. This is for the {TOPIC} of Service or Product Line. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"6. **Marketing and Sales Strategy** (As a {role}): Explain the strategies for marketing and sales, target audience identification, user stories, and chosen marketing platforms. This is for the {TOPIC} of Marketing and Sales Strategy. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"7. **Funding Request** (As a {role}): Specify the funding amount required, the allocation plan, and the anticipated return on investment. Justify the funding request based on growth projections. This is for the {TOPIC} of Funding Request. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"8. **Financial Projections** (As a {role}): Present detailed financial forecasts covering revenue, costs, and profitability for the upcoming years. Base projections on market analysis and business strategies. This is for the {TOPIC} of Financial Projections. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"9. **Appendix** (As a {role}): Include supporting documents like legal papers, contracts, and additional market research data that strengthen the business plan. This is for the {TOPIC} of Appendix. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"10. **Industry Insight** (As a {role}): Provide a comprehensive analysis of the industry, including current trends, competitive landscape, and future predictions. This is for the {TOPIC} of Industry Insight. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"11. **SWOT Analysis** (As a {role}): Perform a SWOT analysis, highlighting the company's strengths, weaknesses, opportunities, and threats. This is for the {TOPIC} of SWOT Analysis. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"12. **Target Audience and User Stories** (As a {role}): Identify the target audience demographics and behaviors. Create user stories illustrating their needs and experiences. This is for the {TOPIC} of Target Audience and User Stories. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"13. **Suitable Business Strategies** (As a {role}): Present specific business strategies tailored to the company's objectives, market conditions, and competitive positioning. This is for the {TOPIC} of Suitable Business Strategies. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"14. **Business Frameworks** (As a {role}): Propose relevant business frameworks or methodologies to guide the company's operations and decision-making. This is for the {TOPIC} of Business Frameworks. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    # 14-1. **SWOT Analysis**:\n",
        "    f\"Identify strengths, weaknesses, opportunities, and threats affecting the {TOPIC} business plan. This is for the {TOPIC} of SWOT Analysis. More Description about the Topic is shown here: {PARAGRAPH} The role of ChatGPT is to assist in generating insights and strategies.\",\n",
        "    # 14-2. **Porter's Five Forces**:\n",
        "    f\"Analyze industry competitiveness to understand market dynamics and potential competitors in the context of {TOPIC}. This is for the {TOPIC} of Porter's Five Forces Analysis. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT's role is to provide a comprehensive analysis of market forces.\",\n",
        "    # 14-3. **Value Chain Analysis**:\n",
        "    f\"Break down activities to enhance value creation and operational efficiency for the {TOPIC} business plan. This is for the {TOPIC} of Value Chain Analysis. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT will help in identifying areas of value creation and optimization.\",\n",
        "    # 14-4. **Business Model Canvas**:\n",
        "    f\"Visualize and communicate the business model clearly for the {TOPIC} stakeholders. This is for the {TOPIC} of Business Model Canvas. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT's role is to aid in presenting a comprehensive business model.\",\n",
        "    # 14-5. **Ansoff Matrix**:\n",
        "    f\"Determine growth strategies for market penetration, development, and diversification tailored to {TOPIC}. This is for the {TOPIC} of Ansoff Matrix. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT will assist in identifying growth strategies for the business.\",\n",
        "    # 14-6. **PESTEL Analysis**:\n",
        "    f\"Assess political, economic, social, technological, environmental, and legal factors impacting the {TOPIC} business plan. This is for the {TOPIC} of PESTEL Analysis. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT's role is to analyze external factors affecting the business environment.\",\n",
        "    # 14-7. **Balanced Scorecard**:\n",
        "    f\"Monitor performance against strategic objectives and adjust the {TOPIC} business plan accordingly. This is for the {TOPIC} of Balanced Scorecard. More Description about the Topic is shown here: {PARAGRAPH} ChatGPT's role is to assist in monitoring and aligning strategies with objectives.\",\n",
        "    f\"15. **Requirements Analysis** (As a {role}): Detail the requirements necessary for successful implementation of the business plan, including resources, technology, and workforce. This is for the {TOPIC} of Requirements Analysis. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"16. **Additional Revenue Streams** (As a {role}): Identify and explore potential additional revenue streams or business diversification opportunities. This is for the {TOPIC} of Additional Revenue Streams. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"17. **Marketing Strategy and Brand Awareness** (As a {role}): Develop a comprehensive marketing strategy focusing on brand awareness, positioning, and customer acquisition. This is for the {TOPIC} of Marketing Strategy and Brand Awareness. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"18. **Branding Suggestions** (As a {role}): Provide recommendations for branding strategies, including visual elements, messaging, and brand personality. This is for the {TOPIC} of Branding Suggestions. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"19. **Recommended Marketing Platforms** (As a {role}): Recommend specific marketing platforms or channels suitable for the target audience and business objectives. This is for the {TOPIC} of Recommended Marketing Platforms. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"20. **Game-Changing Idea** (As a {role}): Present an innovative idea or strategy that could revolutionize the industry or significantly impact the company's growth. This is for the {TOPIC} of Game-Changing Idea. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"21. **Porter's Five Forces Analysis** (As a {role}): Conduct a thorough Porter's Five Forces analysis to evaluate the competitive forces within the industry. Assess factors affecting profitability and market attractiveness. This is for the {TOPIC} of Porter's Five Forces Analysis. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "    f\"22. **CATWOE Analysis** (As a {role}): Perform a comprehensive CATWOE analysis considering Customers, Actors, Transformation, Worldview, Owners, and Environmental Constraints. Analyze the impacts on the business strategy and operations. This is for the {TOPIC} of CATWOE Analysis. More Description about the Topic is shown here: {PARAGRAPH}\",\n",
        "]\n",
        "prompts_business_plan_presentation = [\n",
        "  # Plan/Presentation\n",
        "  #f\"Step 9: As ChatGPT {role} in Business  plan publishing course design draft via internet by the topic of:((( {course_design_variables['topic']} )), in the project field of {course_design_variables['specific_project']}.\",\n",
        "  #f\"Step 9: As ChatGPT {role} in financial  model publishing FINANCIAL model design draft via internet by the topic of:((({TOPIC} )), in the project field of ({PARAGRAPH}).\",\n",
        "  f\"24.As ChatGPT role of ({role}), Make this business  plan report more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"25.As ChatGPT role of ({role}) in advertisment field, Write 3 tweeter post about this Bussness plan , and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role of ({role}) in advertisment field, Write 3 Instagram post about this Bussiness plan, \", # and this the perviuse contnet made?\",\n",
        "  f\"27.As ChatGPT role of ({role}) in weblog writer, Write 1 medium weblog post about this Bussness Plan , \", #and this the perviuse contnet made?\",\n",
        "  f\"28.As ChatGPT role of ({role}) in bussiness post writing, Write 1 LinkedIn post about this business plan, \", #and this the perviuse contnet made?\",\n",
        "  f\"29. As ChatGPT role of ({role}) write an email to related organization for introducing the opportunity of coaporation with us in this field by seeing the above report and the related LinkedIn, tweeter, Instagram, medium and the pdf file of this report \",\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "prompt_Word_Topic_business_plan = [\n",
        "f\"suggest one Business Plans repost Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "\"1. Executive Summary: Business overview\",\n",
        "\"2. Company Description: Company identity\",\n",
        "\"3. Market Analysis: External factors\",\n",
        "\"4. Organization: Organizational structure\",\n",
        "\"5. Products/Services: Services/Products\",\n",
        "\"6. Marketing Strategy: Marketing strategies\",\n",
        "\"7. Funding: Funding details\",\n",
        "\"8. Financial Projections: Financial forecasts\",\n",
        "\"9. Appendix: Additional information\",\n",
        "\"10. Industry: Industry overview\",\n",
        "\"11. SWOT: Strengths, Weaknesses, Opportunities, Threats\",\n",
        "\"12. Target Audience: Target audience and user stories\",\n",
        "\"13. Business Strategies: Business strategies\",\n",
        "\"14. Frameworks: Business frameworks\",\n",
        "\"14-1. **SWOT Analysis**: Identify strengths, weaknesses, opportunities, and threats. Business insights provided.\",\n",
        "\"14-2. **Porter's Five Forces**: Analyze industry competitiveness, understand potential competitors.\",\n",
        "\"14-3. **Value Chain Analysis**: Enhance value creation, improve operational efficiency.\",\n",
        "\"14-4. **Business Model Canvas**: Visualize and communicate business model clearly.\",\n",
        "\"14-5. **Ansoff Matrix**: Determine growth strategies for market penetration.\",\n",
        "\"14-6. **PESTEL Analysis**: Assess political, economic, social factors impacting.\",\n",
        "\"14-7. **Balanced Scorecard**: Monitor performance, align strategies with objectives.\",\n",
        "\"15. Requirements: Requirements analysis\",\n",
        "\"16. Revenue: Additional revenue\",\n",
        "\"17. Marketing: Marketing and branding\",\n",
        "\"18. Branding: Branding suggestions\",\n",
        "\"19. Marketing Platforms: Recommended marketing platforms\",\n",
        "\"20. Idea: Game-changing idea\",\n",
        "\"21. Porter's Five Forces: Porter's Five Forces analysis\",\n",
        "\"22. CATWOE: CATWOE analysis\",\n",
        "]\n",
        "prompts_business_plan_presentation_title= [\n",
        "#Plan/Presentation\n",
        "#f\"Episode 9-1: Write a sensational press release for this\",\n",
        "f\"23.Make this more persuasive\",\n",
        "f\"24. 3 tweets about this Report:\",\n",
        "\n",
        "f\"25. 3 Instagram Post about this Report:\",\n",
        "f\"25. Medium Post about this Report:\",\n",
        "f\"27. LinkedIn Post about this Report:\",\n",
        "f\"28. Organization email for request to do coaporation:\",\n",
        "]"
      ],
      "metadata": {
        "id": "OW6pwn7OPpVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#https://www.phind.com/search?cache=mn7fgo273k158vv941hewfxg\n",
        "#TOPIC_0 = \"task to investigate various gaming groups in Iran, following the Mahsa Movement in the second year. The gaming groups being investigated include the dark solidarity group, the dark trilogy, the dark quadrilogy, the regular people, the bright solidarity group, and the bright trilogy. The aim is to gather information about each group, analyze their characteristics, objectives, and any associated psychological syndromes. This information will then be presented in a table for easier understanding and comparison.\" # Replace this with your topic\n",
        "#RESEARCH_DOMAIN = \"Environmental startup field\" # Replace this with your research domain\n",
        "\n",
        "global docx_path,Pdf_Dir\n",
        "\n",
        "if not (Question == \"\"):\n",
        "\n",
        "    TOPIC  = Question\n",
        "\n",
        "else:\n",
        "    TOPIC  = TOPIC_0\n",
        "\n",
        "main_variables_0 = {\n",
        "   'TOPIC': TOPIC,\n",
        "   'RESEARCH_DOMAIN' : RESEARCH_DOMAIN,\n",
        "   'PARAGRAPH':PARAGRAPH,\n",
        "}\n",
        "\n",
        "TOPIC = f\"{main_variables_0['TOPIC']}\"\n",
        "RESEARCH_DOMAIN = \"{RESEARCH_DOMAIN}\"\n",
        "\n",
        "\n",
        "prompts_finantial = [\n",
        "f\"suggest one financial model report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "f\"1. Do what for the '{TOPIC}' with this description: '{PARAGRAPH}'?, also use word system format as bolding and ...\",\n",
        "f\"1. Critique the business model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"2. Calculate the startup costs for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"2. Critique the startup costs for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"3. Track the revenue for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"3. Critique the revenue tracking for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"4. Review the projections for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"4. Critique the projections for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"5. Generate a detailed financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system formats as bolding and ...\",\n",
        "f\"5. Critique the detailed financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"6. Analyze the financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"6. Critique the financial model analysis for the '{TOPIC}' with this description: '{PARAGRAPH}'.also use word system format as bolding and ...\",\n",
        "f\"7. Adjust the financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"7. Critique the adjustments made to the financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"8. Finalize the financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "f\"8. Critique the finalized financial model for the '{TOPIC}' with this description: '{PARAGRAPH}'. also use word system format as bolding and ...\",\n",
        "]\n",
        "prompts_finantial_presentation = [\n",
        "  # Plan/Presentation\n",
        "  f\"Step 9: As ChatGPT  role in financial  model publishing FINANCIAL model design draft via internet by the topic of:((({TOPIC} )), in the project field of ({PARAGRAPH}).\",\n",
        "  f\"21:.As ChatGPT role role in newspaper field, Write a sensational press release for this Financial Model and perviuse content made \", #in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"22.As ChatGPT role of ({role}), Make this financial  model more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"23.As ChatGPT role of ({role}) in advertisment field, Write 3 tweeter post about this Financial Model , and this the perviuse contnet made?\",\n",
        "  f\"24.As ChatGPT role of ({role}) in advertisment field, Write 3 Instagram post about this Financial Model , \", # and this the perviuse contnet made?\",\n",
        "  f\"25.As ChatGPT role of ({role}) in weblog writer, Write 1 medium weblog post about this Financial Model , \", #and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role of ({role}) in bussiness post writing, Write 1 LinkedIn post about this Financial Model , \", #and this the perviuse contnet made?\",\n",
        "  f\"27. As ChatGPT role of ({role}) write an email to related organization for introducing the opportunity of coaporation with us in this field by seeing the above report and the related LinkedIn, tweeter, Instagram, medium and the pdf file of this report \",\n",
        "\n",
        "\n",
        "]\n",
        "#https://www.phind.com/search?cache=fbhlj5n08cwpl4y42l99w8sq\n",
        "prompt_Word_Topic_finantial = [\n",
        "  f\"suggest one financial model report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "  \"1. Determine tasks for topic.\",\n",
        "  \"2. Critique business model.\",\n",
        "  \"3. Calculate startup costs.\",\n",
        "  \"4. Critique startup costs.\",\n",
        "  \"5. Track revenue.\",\n",
        "  \"6. Critique revenue tracking.\",\n",
        "  \"7. Review projections.\",\n",
        "  \"8. Critique projections.\",\n",
        "  \"9. Generate detailed financial model.\",\n",
        "  \"10. Critique financial model.\",\n",
        "  \"11. Analyze financial model.\",\n",
        "  \"12. Critique analysis.\",\n",
        "  \"13. Adjust financial model.\",\n",
        "  \"14. Critique adjustments.\",\n",
        "  \"15. Finalize financial model.\",\n",
        "  \"16. Critique finalized model.\",\n",
        "]\n",
        "prompts_finantial_presentation_title= [\n",
        "#Plan/Presentation\n",
        "f\"Episode last: Write a sensational press release for this\",\n",
        "f\"L1. Make this more persuasive\",\n",
        "f\"L2. 3 tweets about this Report:\",\n",
        "\n",
        "f\"L3. 3 Instagram Post about this Report:\",\n",
        "f\"L4. Medium Post about this Report:\",\n",
        "f\"L5. LinkedIn Post about this Report:\",\n",
        "f\"L6. Organization email for request to do coaporation:\",\n",
        "]"
      ],
      "metadata": {
        "id": "bRdDZr2zPkv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOPIC_0 = \"task to investigate various gaming groups in Iran, following the Mahsa Movement in the second year. The gaming groups being investigated include the dark solidarity group, the dark trilogy, the dark quadrilogy, the regular people, the bright solidarity group, and the bright trilogy. The aim is to gather information about each group, analyze their characteristics, objectives, and any associated psychological syndromes. This information will then be presented in a table for easier understanding and comparison.\" # Replace this with your topic\n",
        "#RESEARCH_DOMAIN = \"Making Pitch deck file for the TOPIC \"\n",
        "\n",
        "if not (Question == \"\"):\n",
        "   TOPIC = Question\n",
        "else:\n",
        "   TOPIC = TOPIC_0\n",
        "\n",
        "main_variables_0 = {\n",
        "  'TOPIC': TOPIC,\n",
        "  'RESEARCH_DOMAIN' : RESEARCH_DOMAIN,\n",
        "  'PARAGRAPH':PARAGRAPH,\n",
        "}\n",
        "\n",
        "prompts_pitch_0 = [\n",
        "f\"suggest one pitch deck  report Title in less than 15 word, based of This Topic :({main_variables_0['TOPIC']}) and the description:({main_variables_0['PARAGRAPH']}).\",\n",
        "f\"1.What are the key aspects of {main_variables_0['TOPIC']} that need to be addressed in the context of {main_variables_0['PARAGRAPH']}?\",\n",
        "f\"2.Create a mind map of the main elements of {main_variables_0['TOPIC']} and how they are interconnected in the context of {main_variables_0['PARAGRAPH']}.\",\n",
        "f\"3.Write down ideas on sticky notes about how to improve {main_variables_0['TOPIC']} in the context of {main_variables_0['PARAGRAPH']}. Then, group these ideas based on common themes.\",\n",
        "f\"4.Take turns sharing ideas on how to improve {main_variables_0['TOPIC']} in the context of {main_variables_0['PARAGRAPH']}. Make sure everyone has a chance to contribute.\",\n",
        "f\"5.Think of ways to create the problem in {main_variables_0['TOPIC']} in the context of {main_variables_0['PARAGRAPH']}. This will help you understand how to solve it.\",\n",
        "f\"6.Apply the SCAMPER method to existing solutions or situations in {main_variables_0['TOPIC']} in the context of {main_variables_0['PARAGRAPH']}. This will spark new ideas.\",\n",
        "f\"7.Assemble a diverse group of people from various backgrounds and disciplines to tackle {main_variables_0['TOPIC']} in the context of {main_variables_0['PARAGRAPH']}. This will ensure a more rounded and innovative solution.\",\n",
        "f\"8.Envision a future state where {main_variables_0['TOPIC']} has been solved successfully in the context of {main_variables_0['PARAGRAPH']}. Starting from this endpoint, work backward to identify the series of events and decisions that led to that future.\",\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_pitch_0 = [\n",
        "  f\"suggest one pitch deck report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "  f\"1. Identify key aspects of startup pitch deck.\",\n",
        "  f\"2. Create a mind map of startup elements.\",\n",
        "  f\"3. Brainstorm ideas for startup improvement.\",\n",
        "  f\"4. Share ideas for startup improvement.\",\n",
        "  f\"5. Identify startup problems for solutions.\",\n",
        "  f\"6. Apply SCAMPER method to startup.\",\n",
        "  f\"7. Assemble diverse team for startup.\",\n",
        "  f\"8. Envision successful startup future.\",\n",
        "]"
      ],
      "metadata": {
        "id": "M1anh0IcnFnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the variables\n",
        "\n",
        "# Create the corrected 19-line ChatGPT prompt as a list\n",
        "prompts_pitch = [\n",
        "  f\"suggest one pitch deck  report Title in less than 15 word, based of This Topic :({main_variables_0['TOPIC']}) and the description:({main_variables_0['PARAGRAPH']}).\",\n",
        "  f\"1. Introduction: {TOPIC} and its mission. {PARAGRAPH}\",\n",
        "  f\"2. Problem Statement: Identify challenges and pain points in content creation without AI. Explain why current methods are insufficient. {PARAGRAPH}\",\n",
        "  f\"3. Solution: Present {TOPIC} as the solution. Emphasize unique value proposition and benefits. {PARAGRAPH}\",\n",
        "  f\"4. Market Opportunity: Showcase the market demand for AI in content creation. Identify the target audience and potential market size. {PARAGRAPH}\",\n",
        "  f\"5. Product Overview: Detail features and functionalities of {TOPIC}. Explain how it works and its relevance to content creators. {PARAGRAPH}\",\n",
        "  f\"6. Competitive Landscape: Analyze competitors and highlight what sets {TOPIC} apart. Showcase any proprietary technology or unique approaches. {PARAGRAPH}\",\n",
        "  f\"7. Business Model: Clearly outline your revenue model for {TOPIC}. Specify how you plan to monetize the guide (e.g., subscription, one-time purchase). {PARAGRAPH}\",\n",
        "  f\"8. Traction and Milestones: Share any achievements, milestones, or user metrics for {TOPIC}. Highlight partnerships or collaborations. {PARAGRAPH}\",\n",
        "  f\"9. Market Positioning: Define {TOPIC}'s position in the market. Explain how you plan to capture and maintain market share. {PARAGRAPH}\",\n",
        "  f\"10. Financial Projections: Present financial forecasts, including revenue projections and expenses for {TOPIC}. Provide a clear understanding of the return on investment. {PARAGRAPH}\",\n",
        "  f\"11. Use of Funds: Outline how you plan to use the funding you're seeking for {TOPIC}. Break down the allocation of funds across key areas. {PARAGRAPH}\",\n",
        "  f\"12. Team: Introduce the founding team members and their expertise for {TOPIC}. Highlight relevant experience and skills. {PARAGRAPH}\",\n",
        "  f\"13. Demo or Product Showcase: If possible, include a demonstration of {TOPIC}. Showcase its functionality and ease of use. {PARAGRAPH}\",\n",
        "  f\"14. User Testimonials or Case Studies: Include feedback from users who have tested or used {TOPIC}. Present any case studies demonstrating the guide's effectiveness. {PARAGRAPH}\",\n",
        "  f\"15. Market Trends and Future Outlook: Discuss relevant trends in AI and content creation for {TOPIC}. Highlight how {TOPIC} aligns with the future direction of the industry. {PARAGRAPH}\",\n",
        "  f\"16. Risks and Mitigations: Address potential risks associated with your business for {TOPIC}. Provide strategies and plans to mitigate these risks. {PARAGRAPH}\",\n",
        "  f\"17. Ask/Call to Action: Clearly state what you are seeking from potential investors or partners for {TOPIC}. Specify the amount of funding you are looking for. {PARAGRAPH}\",\n",
        "  f\"18. Contact Information: Provide contact details for further inquiries for {TOPIC}. Include social media handles and a website link. {PARAGRAPH}\",\n",
        "  f\"19. Appendix: Include any additional supporting materials, such as charts, graphs, or detailed market research for {TOPIC}. {PARAGRAPH}\",\n",
        "]\n",
        "prompts_pitch_presentation = [\n",
        "  # Plan/Presentation\n",
        "  f\"20.Write a sensational press release for this Pitch Deck in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"21.Make this Pitch Deck Plan more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"22.Write 3 tweets about this Pitch Deck in this topic ({TOPIC}), and this description? Description :((('{PARAGRAPH}')))\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Create the corrected 19-line ChatGPT prompt as a list\n",
        "prompt_Word_Topic_pitch = [\n",
        "f\"suggest one pitch deck  report Title in less than 15 word, based of This Topic :({main_variables_0['TOPIC']}) and the description:({main_variables_0['PARAGRAPH']}).\",f\"1. Introduction: Overview of AI-content guide and mission.\",\n",
        " f\"2. Problem Statement: Identify challenges in AI-free content creation.\",\n",
        " f\"3. Solution: Present AI-content guide as solution.\",\n",
        " f\"4. Market Opportunity: Showcase demand and potential market size.\",\n",
        " f\"5. Product Overview: Describe features of AI-content guide.\",\n",
        " f\"6. Competitive Landscape: Analyze competitors and unique aspects.\",\n",
        " f\"7. Business Model: Outline revenue model and monetization strategy.\",\n",
        " f\"8. Traction and Milestones: Share achievements and user metrics.\",\n",
        " f\"9. Market Positioning: Define market position and growth strategy.\",\n",
        " f\"10. Financial Projections: Present financial forecasts.\",\n",
        " f\"11. Use of Funds: Outline fund usage and key areas.\",\n",
        " f\"12. Team: Introduce team members and their expertise.\",\n",
        " f\"13. Demo or Product Showcase: Showcase product functionality.\",\n",
        " f\"14. User Testimonials or Case Studies: Include user feedback.\",\n",
        " f\"15. Market Trends and Future Outlook: Discuss industry trends and alignment.\",\n",
        " f\"16. Risks and Mitigations: Address potential risks and mitigation plans.\",\n",
        " f\"17. Ask/Call to Action: State what you seek from investors.\",\n",
        " f\"18. Contact Information: Provide contact details.\",\n",
        " f\"19. Appendix: Include additional supporting materials.\",\n",
        "]\n",
        "prompts_pitch_presentation_title =[\n",
        "f\"20. Write a sensational press release for this Pitch Deck:\",\n",
        "  f\"21. Make this more persuasive\",\n",
        "  f\"22. Write 3 tweets about this Pitch Deck? \",\n",
        "]"
      ],
      "metadata": {
        "id": "RfZ6xnoSmNg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TOPIC_0 = \"task to investigate various gaming groups in Iran, following the Mahsa Movement in the second year. The gaming groups being investigated include the dark solidarity group, the dark trilogy, the dark quadrilogy, the regular people, the bright solidarity group, and the bright trilogy. The aim is to gather information about each group, analyze their characteristics, objectives, and any associated psychological syndromes. This information will then be presented in a table for easier understanding and comparison.\" # Replace this with your topic\n",
        "#RESEARCH_DOMAIN = \"Making Pitch deck file for the TOPIC \"\n",
        "\n",
        "if not (Question == \"\"):\n",
        "   TOPIC = Question\n",
        "else:\n",
        "   TOPIC = TOPIC_0\n",
        "\n",
        "main_variables_0 = {\n",
        "  'TOPIC': TOPIC,\n",
        "  'RESEARCH_DOMAIN' : RESEARCH_DOMAIN,\n",
        "  'PARAGRAPH':PARAGRAPH,\n",
        "}\n",
        "\n",
        "prompts_game_theory_1= [\n",
        "f\"suggest one Game Theory report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "f\"1. Define the game structure for {TOPIC}: Identify the players, their strategies, and the possible outcomes of the game.\",\n",
        "f\"2. Represent the game for {TOPIC}: This can be done using a matrix or a tree. In the matrix representation, each cell represents a possible outcome of the game.\",\n",
        "f\"3. Analyze the game for {TOPIC}: Determine the best strategies for each player, the Nash equilibrium, and the potential outcomes of the game.\",\n",
        "f\"4. Make decisions based on the analysis for {TOPIC}: Use the results of the analysis to determine the best course of action for each player.\",\n",
        "f\"5. Implement the game for {TOPIC}: Write the code that simulates the game. This could involve creating a payoff matrix or a game tree, and writing functions to determine the best strategies and the Nash equilibrium.\",\n",
        "f\"6. Test the game for {TOPIC}: Run the game simulation and check if the results are as expected. This could involve checking if the Nash equilibrium is correct, or if the best strategies lead to the desired outcomes.\",\n",
        "f\"7. Optimize the game for {TOPIC}: If the results are not as expected, modify the game structure or the strategies, and run the simulation again.\",\n",
        "f\"8. Document the game for {TOPIC}: Write a report or a paper that explains the game, the results, and the conclusions.\",\n",
        "f\"9. Share the game for {TOPIC}: Share the results with others, and get feedback on the game.\",\n",
        "f\"10. Update the game for {TOPIC}: Based on the feedback, update the game structure or the strategies, and run the simulation again.\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "prompt_Word_Topic_game_theory_1 = [\n",
        "  f\"suggest one Game Theory report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "  \"1.Define the game structure\",\n",
        "  \"2.Represent the game\",\n",
        "  \"3.Analyze the game\",\n",
        "  \"4.Make decisions based on the analysis\",\n",
        "  \"5.Implement the game\",\n",
        "  \"6.Test the game\",\n",
        "  \"7.Optimize the game\",\n",
        "  \"8.Document the game\",\n",
        "  \"9.Share the game\",\n",
        "  \"10.Update the game\",\n",
        "]"
      ],
      "metadata": {
        "id": "m1FFBjbOWkZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of prompts for generating academic papers\n",
        "\n",
        "Previous_CONTENT = '{Previous_CONTENT}'\n",
        "prompts_Academic_proposal_critique= [\n",
        "f\" As ChatGPT expert in the role of {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "  f\"Critically evaluate the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'. Discuss any potential issues, limitations, or controversies in the ideas expressed.\",\n",
        "  f\"Identify the key points in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Explain the context of the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Summarize the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Identify the research methodology used in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Analyze the data collection and analysis methods used in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Identify the research questions in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Evaluate the conclusions drawn in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Identify the limitations of the research in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "  f\"Identify any controversies or debates related to the research in the following content related to the topic '{TOPIC}': '{Previous_CONTENT}'.\",\n",
        "]\n",
        "prompts_Academic_proposal_critique_presentation=[\n",
        "# Plan/Presentation\n",
        "  f\".Write a sensational press release for this research in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\".Make this research Plan more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\".Write 3 tweets about this research in this topic ({TOPIC}), and this description? Description :((('{PARAGRAPH}')))\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_Academic_proposal_critique = [\n",
        "f\" As ChatGPT expert in the role of {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "f\"1. Critically evaluate the following content related to the topic '': ''. Discuss any potential issues, limitations, or controversies in the ideas expressed.\",\n",
        " f\"2. Identify the key points in the following content related to the topic '': ''.\",\n",
        " f\"3. Explain the context of the following content related to the topic '': ''.\",\n",
        " f\"4. Summarize the following content related to the topic '': ''.\",\n",
        " f\"5. Identify the research methodology used in the following content related to the topic '': ''.\",\n",
        " f\"6. Analyze the data collection and analysis methods used in the following content related to the topic '': ''.\",\n",
        " f\"7. Identify the research questions in the following content related to the topic '': ''.\",\n",
        " f\"8. Evaluate the conclusions drawn in the following content related to the topic '': ''.\",\n",
        " f\"9. Identify the limitations of the research in the following content related to the topic '': ''.\",\n",
        " f\"10. Identify any controversies or debates related to the research in the following content related to the topic '': ''.\",\n",
        "]\n",
        "prompts_Academic_proposal_critique_presentation_title =[\n",
        "#Plan/Presentation\n",
        "f\"11. Write a sensational press release for this Research\",\n",
        "f\"12. Make this more persuasive\",\n",
        "f\"13. Write 3 tweets about this Research?\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "RRmk8bTvloSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOPIC_0 = \"task to investigate various gaming groups in Iran, following the Mahsa Movement in the second year. The gaming groups being investigated include the dark solidarity group, the dark trilogy, the dark quadrilogy, the regular people, the bright solidarity group, and the bright trilogy. The aim is to gather information about each group, analyze their characteristics, objectives, and any associated psychological syndromes. This information will then be presented in a table for easier understanding and comparison.\" # Replace this with your topic\n",
        "#RESEARCH_DOMAIN = \"Game Theory in psychology  field\" # Replace this with your research domain\n",
        "\n",
        "global docx_path,Pdf_Dir\n",
        "\n",
        "if not (Question == \"\"):\n",
        "\n",
        "    TOPIC  = Question\n",
        "\n",
        "else:\n",
        "    TOPIC  = TOPIC_0\n",
        "\n",
        "main_variables_0 = {\n",
        "   'TOPIC': TOPIC,\n",
        "   'RESEARCH_DOMAIN' : RESEARCH_DOMAIN\n",
        "}\n",
        "\n",
        "prompt_6Hat_Brainstorm = [\n",
        "f\"0. Suggest one Title less than 10 word for a report for 6 hat brainstorming game structure for {TOPIC}: Identify the players, their strategies, and the possible outcomes of the game.\",\n",
        "f\"1. Define the game structure for {TOPIC}: Identify the players, their strategies, and the possible outcomes of the game.\",\n",
        "f\"2. Represent the game for {TOPIC}: This can be done using a matrix or a tree. In the matrix representation, each cell represents a possible outcome of the game.\",\n",
        "f\"3. Analyze the game for {TOPIC}: Determine the best strategies for each player, the Nash equilibrium, and the potential outcomes of the game.\",\n",
        "f\"4. Make decisions based on the analysis for {TOPIC}: Use the results of the analysis to determine the best course of action for each player.\",\n",
        "f\"5. Implement the game for {TOPIC}: Write the code that simulates the game. This could involve creating a payoff matrix or a game tree, and writing functions to determine the best strategies and the Nash equilibrium.\",\n",
        "f\"6. Test the game for {TOPIC}: Run the game simulation and check if the results are as expected. This could involve checking if the Nash equilibrium is correct, or if the best strategies lead to the desired outcomes.\",\n",
        "f\"7. Optimize the game for {TOPIC}: If the results are not as expected, modify the game structure or the strategies, and run the simulation again.\",\n",
        "f\"8. Document the game for {TOPIC}: Write a report or a paper that explains the game, the results, and the conclusions.\",\n",
        "f\"9. Share the game for {TOPIC}: Share the results with others, and get feedback on the game.\",\n",
        "f\"10. Update the game for {TOPIC}: Based on the feedback, update the game structure or the strategies, and run the simulation again.\",\n",
        "]\n",
        "prompt_6Hat_Brainstorm_presentation = [\n",
        "# Plan/Presentation\n",
        "  f\"11.Write a sensational press release for this research in this 6 hat brainstorming ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"12.Make this 6 hat brain storming more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"13.Write 3 tweets about this 6 hat brainstorming in this topic ({TOPIC}), and this description? Description :((('{PARAGRAPH}')))\"\n",
        "\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_6Hat_Brainstorm = [\n",
        "f\" Suggest one Title less than 10 word for a report for 6 hat brainstorming game structure for {TOPIC}: Identify the players, their strategies, and the possible outcomes of the game.\",\n",
        "\"1. Define game structure.\",\n",
        "\"2. Represent the game.\",\n",
        "\"3. Analyze the game.\",\n",
        "\"4. Make decisions based on analysis.\",\n",
        "\"5. Implement the game.\",\n",
        "\"6. Test the game.\",\n",
        "\"7. Optimize the game.\",\n",
        "\"8. Document the game.\",\n",
        "\"9. Share the game.\",\n",
        "\"10. Update the game.\",\n",
        "]\n",
        "prompt_6Hat_Brainstorm_presentation_title =[\n",
        "#Plan/Presentation\n",
        "f\"11. Write a sensational press release for this brainstorm\",\n",
        "f\"12. Make this more persuasive\",\n",
        "f\"13. Write 3 tweets about this brainstorm?\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "RFSHPUyJp-i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_chatdev = [\n",
        "f'0. Suggest one Title less than 10 word for a project tile for the topic which is :({TOPIC}) and description which is :({PARAGRAPH}). so you suggesting to to creat some code for doing this topic snd description as one project',\n",
        "f'if possible consider some ChatGPT project for creating the code for a project like chatdev which is available  at here :( https://github.com/OpenBMB/ChatDev )   to creat one prompt for the topic which is :({TOPIC}) and description  which is :({PARAGRAPH}). so you suggesting to to creat some code for doing this topic and description as one project',\n",
        "]\n",
        "\n",
        "prompts_Topic_chatdev = [\n",
        "f'0. Suggest one Title less than 10 word for a project tile for the topic which is :({TOPIC}) and description  which is :({PARAGRAPH}). so you suggesting to to creat some code for doing this topic snd description as one project',\n",
        "f'Suggested prompt for ChatDev code Writing Project:',\n",
        " ]"
      ],
      "metadata": {
        "id": "W30G7o27kNqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_Psychology = [\n",
        "   f\" 0. As a ChatGP {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "    f\"1. As a ChatGP {role}, Identify the problem in the relationship related to the topic: {TOPIC}.\\nDescription of the situation: {PARAGRAPH}\",\n",
        "    f\"2. As a ChatGP {role}, Define the objective you want to achieve by addressing the problem related to: {TOPIC}.\",\n",
        "    f\"3. As a ChatGP {role}, Gather all relevant information about the problem related to: {TOPIC}.\\nDescription of the situation: {PARAGRAPH}\",\n",
        "    f\"4. As a ChatGP {role}, Analyze the collected data to understand the root cause of the problem related to: {TOPIC}.\\nDescription of the situation: {PARAGRAPH}\",\n",
        "    f\"5. As a ChatGP {role}, Based on the analysis, generate potential solutions to address the problem related to: {TOPIC}.\",\n",
        "    f\"6. As a ChatGP {role}, Create a detailed action plan to implement the chosen solution for the problem related to: {TOPIC}.\",\n",
        "    f\"7. As a ChatGP {role}, Evaluate the effectiveness of the implemented solution for the problem related to: {TOPIC}.\",\n",
        "]\n",
        "prompts_Psychology_presentqtion =[\n",
        "# Plan/Presentation\n",
        "  f\"8.Write a sensational press release for this psychology report in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"9.Make this psychology  report more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"10.Write 3 tweets about this psychology report in this topic ({TOPIC}), and this description? Description :((('{PARAGRAPH}')))\"\n",
        "\n",
        "]\n",
        "\n",
        "prompt_Word_Topic_Psychology = [\n",
        "    f\" 0.As a ChatGP {role}, suggest one report Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}).\",\n",
        "\n",
        "    \"1. Identify the problem in the relationship related to the topic.\",\n",
        "    \"2. Define the objective you want to achieve by addressing the problem.\",\n",
        "    \"3. Gather all relevant information about the problem.\",\n",
        "    \"4. Analyze the collected data to understand the root cause of the problem.\",\n",
        "    \"5. Based on the analysis, generate potential solutions to address the problem.\",\n",
        "    \"6. Create a detailed action plan to implement the chosen solution for the problem.\",\n",
        "    \"7. Evaluate the effectiveness of the implemented solution for the problem.\",\n",
        "]\n",
        "prompt_Word_Topic_Psychology_presentation_title=[\n",
        "    #Plan/Presentation\n",
        "f\"8. Write a sensational press release for this Psychology Report\",\n",
        "f\"9. Make this more persuasive\",\n",
        "f\"10. Write 3 tweets about this Psychology Report?\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "sfUrAfYzBHNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.phind.com/search?cache=rg4qjuf21n0wdb6i8yq24fsm\n",
        "\n",
        "\n",
        "\n",
        "prompts_story = [\n",
        " f\"0. As ChatGPT role: {role}, suggest a novel title in less than 15 words, based on the topic {TOPIC} and the description:({PARAGRAPH}).\",\n",
        " f\"1. As ChatGPT role: {role}, determine genre and subgenre based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"2. As ChatGPT role: {role}, develop story premise based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"3. As ChatGPT role: {role}, expand premise into blurb based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"4. As ChatGPT role: {role}, create outline from blurb based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"5. As ChatGPT role: {role}, develop style prompt based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"6. As ChatGPT role: {role}, use ChatGPT for introduction based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"7. As ChatGPT role: {role}, write scenes with ChatGPT based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"8. As ChatGPT role: {role}, redirect ChatGPT if needed based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"9. As ChatGPT role: {role}, regenerate content if unsatisfied based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"10. As ChatGPT role: {role}, reuse prompt space for consistency based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"11. As ChatGPT role: {role}, track major story actions based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"12. As ChatGPT role: {role}, review novel for consistency based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"13. As ChatGPT role: {role}, edit novel with ChatGPT based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"14. As ChatGPT role: {role}, write character dramatic life change story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"15. As ChatGPT role: {role}, start story with character secret based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"16. As ChatGPT role: {role}, write secret reader story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"17. As ChatGPT role: {role}, write innocent symbolizes darker story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"18. As ChatGPT role: {role}, write discrimination experience story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"19. As ChatGPT role: {role}, write cover up mistake story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"20. As ChatGPT role: {role}, write stormy night story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        " f\"21. As ChatGPT role: {role}, write villain protagonist story based on the topic {TOPIC} and the description:({PARAGRAPH})\",\n",
        "]\n",
        "prompts_story_presentation=[\n",
        "# Plan/Presentation\n",
        "  f\"22.As ChatGPT role {role} in newspaper field, Write a sensational press release for this story and perviuse  content made \", #in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"23.As ChatGPT role {role}, Make this story more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"24.As ChatGPT role {role} in advertisment field, Write 3 tweeter post about this story, and this the perviuse contnet made?\",\n",
        "  f\"25.As ChatGPT role {role} in advertisment field, Write 3 Instagram post about this story, \", # and this the perviuse contnet made?\",\n",
        "  f\"26.As ChatGPT role {role} in weblog writer, Write 1 medium weblog post about this story, \", #and this the perviuse contnet made?\",\n",
        "  f\"27.As ChatGPT role {role} in bussiness post writing, Write 1 LinkedIn post about this story, \", #and this the perviuse contnet made?\",\n",
        "\n",
        "]\n",
        "\n",
        "prompts_story_topic = [\n",
        "\n",
        "f\"0. As ChatGPT role: {role}, suggest a novel title in less than 15 words, based on the topic {TOPIC} and the description:({PARAGRAPH}).\",\n",
        "\n",
        " \"1. Determine genre and subgenre\",\n",
        " \"2. Develop story premise\",\n",
        " \"3. Expand premise into blurb\",\n",
        " \"4. Create outline from blurb\",\n",
        " \"5. Develop style prompt\",\n",
        " \"6. Use ChatGPT for introduction\",\n",
        " \"7. Write scenes with ChatGPT\",\n",
        " \"8. Redirect ChatGPT if needed\",\n",
        " \"9. Regenerate content if unsatisfied\",\n",
        " \"10. Reuse prompt space for consistency\",\n",
        " \"11. Track major story actions\",\n",
        " \"12. Review novel for consistency\",\n",
        " \"13. Edit novel with ChatGPT\",\n",
        " \"14. Write character dramatic life change story\",\n",
        " \"15. Start story with character secret\",\n",
        " \"16. Write secret reader story\",\n",
        " \"17. Write innocent symbolizes darker story\",\n",
        " \"18. Write discrimination experience story\",\n",
        " \"19. Write cover up mistake story\",\n",
        " \"20. Write stormy night story\",\n",
        " \"21. Write villain protagonist story\",\n",
        "]\n",
        "prompts_story_topic_presentation_title =[\n",
        "  #Plan/Presentation\n",
        "f\"22. Write a sensational press release for this Story\",\n",
        "f\"23. Make this more persuasive\",\n",
        "f\"24. 3 tweets about this Story Report:\",\n",
        "\n",
        "f\"25. 3 Instagram Post about this Story Report:\",\n",
        "f\"26. Medium Post about this Story Report:\",\n",
        "f\"27. LinkedIn Post about this Story Report:\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "k0XR0sbJ4IQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I24Xo1CdBrX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title .\n",
        "#TOPIC = f\"Understanding the Cycle of Domestic Violence against iranian women: The Influence of changes in the type of violence that continues in the aftermath of violence\"\n",
        "#TOPIC =f\"stop the WarDark Triad and Light Triad in the Middle East: A Critical Examination of their Influence and Impact on Iran\"+TOPIC\n",
        "\n",
        "#TOPIC = f\"If possible read this post ( https://www.geeky-gadgets.com/chatgpt-brainstorming-prompts/ ) and suggest  one python block code 10 prompt in the for on ( prompt=[The suggested prompt based of [var1] ). The prompt must have 10 line and at least three necessary value like TOPIC , FIELD_SUDY,and the third vale is your optional . Also you can have more variable or prompt for doing this post main goal which is brainstorming for one topic based of ChatGPT promptimg. This would be ChatGPT cheat sheet prompting.\"\n",
        "TOPIC = main_variables_0.get('TOPIC', 'Default Value')\n",
        "print ('TOPIC IS :', TOPIC)\n",
        "TOPIC_SENTENCE = Topic\n",
        "LANGUAGE = 'English'\n",
        "main_variables_0 = {\n",
        "      'TOPIC': f\"{Topic}\",\n",
        "      'RESEARCH_DOMAIN': f\"{RESEARCH_DOMAIN}\",\n",
        "      'PARAGRAPH': f\"{PARAGRAPH}\",\n",
        "      'PARAGRAPHS': f\"{PARAGRAPH}\" , # f\"{PARAGRAPHS}\",\n",
        "      'TOPIC_SENTENCE': TOPIC_SENTENCE,\n",
        "      'LANGUAGE': LANGUAGE,\n",
        "      'ABSTRACT_PARAGRAPH': ABSTRACT_PARAGRAPH,\n",
        "      'BIBLIOGRAPHY': BIBLIOGRAPHY,\n",
        "      'THEORY1': THEORY1,\n",
        "      'THEORY2': THEORY2,\n",
        "      'RESEARCH_QUESTIONS': RESEARCH_QUESTIONS,\n",
        "      'ACTION': ACTION,\n",
        "      'RESULT_PARAGRAPHS': RESULT_PARAGRAPHS,\n",
        "      'DATE': DATE,\n",
        "      'NUMBER_OF_DAYS_MONTHS_YEARS': NUMBER_OF_DAYS_MONTHS_YEARS,\n",
        "      'role' : f\"{role}\"\n",
        "  }"
      ],
      "metadata": {
        "id": "I484Df8ONQVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhppuBUKBFyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-xBKrKYBJdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "main_variables_0 = {\n",
        "     'TOPIC': f\"{TOPIC}\",\n",
        "     'RESEARCH_DOMAIN': f\"{RESEARCH_DOMAIN}\",\n",
        "     'PARAGRAPH': f\"{PARAGRAPH}\",\n",
        "     'PARAGRAPHS': f\"{PARAGRAPHS}\",\n",
        "     'TOPIC_SENTENCE': f\"{TOPIC_SENTENCE}\",\n",
        "     'LANGUAGE': f\"{LANGUAGE}\",\n",
        "     'ABSTRACT_PARAGRAPH': f\"{ABSTRACT_PARAGRAPH}\",\n",
        "     'BIBLIOGRAPHY': f\"{BIBLIOGRAPHY}\",\n",
        "     'THEORY1': f\"{THEORY1}\",\n",
        "     'THEORY2': f\"{THEORY2}\",\n",
        "     'RESEARCH_QUESTIONS': f\"{RESEARCH_QUESTIONS}\",\n",
        "     'ACTION': f\"{ACTION}\",\n",
        "     'RESULT_PARAGRAPHS': f\"{RESULT_PARAGRAPHS}\",\n",
        "     'DATE': f\"{DATE}\",\n",
        "     'NUMBER_OF_DAYS_MONTHS_YEARS': f\"{NUMBER_OF_DAYS_MONTHS_YEARS}\",\n",
        "     'role': f\"{role}\",\n",
        "     'project_example': f\"{project_example}\",\n",
        "     'context': f\"{context}\",\n",
        "     'instruction': f\"{instruction}\",\n",
        "     'output_format': f\"{output_format}\",\n",
        "     'specific_project_details': f\"{specific_project_details}\",\n",
        "     'X': f\"{X}\",\n",
        "     'project_manager': f\"{project_manager}\",\n",
        "     'report': f\"{report}\",\n",
        "     'important_themes': f\"{important_themes}\",\n",
        "     'project_name': f\"{project_name}\",\n",
        "     'stakeholder': f\"{stakeholder}\",\n",
        "     'resistant_stakeholder': f\"{resistant_stakeholder}\",\n",
        "     'task': f\"{task}\",\n",
        "     'Your_Email': f\"{Your_Email}\",\n",
        "     'category':f\"{category}\",\n",
        "}\n",
        "\n",
        "\n",
        "prompts_project_management = [\n",
        "   f\"0. As ChatGPT role: {role}, suggest a Project Management title in less than 15 words, based on the topic {TOPIC} and the description:({PARAGRAPH}).\",\n",
        "\n",
        "   f\"1. Come up with questions to ask during the meeting to start the project based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}).\",\n",
        "   f\"2. I need your assistance in designing a project risk assessment template based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}).\",\n",
        "   f\"3. Please provide a handoff and project conclusion checklist based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}).\",\n",
        "   f\"4. As a project manager {role}, I am responsible for launching a new e-commerce website {project_example}. The project based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) is planned to span over 6 months and involves multiple departments like design, development, and marketing {context}. Please generate a comprehensive project plan that includes objectives, business case, scope, timeline, stakeholders, and success metrics {instruction} in a table format {output_format}.\",\n",
        "   f\"5. Generate a project timeline with milestones, start dates, end dates, objectives, tasks, and responsible parties for launching X project. Please provide and suggest milestones, their start and end dates, objectives, tasks, and the responsible party for each milestone in a table format.\",\n",
        "   f\"6. Identify potential risks associated with {specific_project_details} and suggest mitigation strategies.\",\n",
        "   f\"7. We are doing a project based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) to do {X} and the below are the risks we have identified, is there anything that we have missed?\",\n",
        "   f\"8. Draft a project update email for stakeholders of {X} highlighting key achievements, current challenges, and next steps.\",\n",
        "   f\"9. Outline the key points from {report}. based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) Focus on any mention of {important_themes} that you would like to specifically have as the main focus.\",\n",
        "   f\"10.based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) Please simulate the dialogue and provide step by step guidance to help me {project_manager} best prepare when dealing with a resistant stakeholder. Context: I want to launch a new project {X} and a key stakeholder fails to see why it's important. Please provide pointers with potential concern and rationale presented in a table format? I want the rationale column to include 2-3 bullets each for talking points.\",\n",
        "   f\"11.based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) What other information do you need to complete this task?\",\n",
        "   f\"12.based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) Can you break that down a little further into more specific, step-by-step assignments?\",\n",
        "   f\"13.based on the topic {TOPIC} and the description:({PARAGRAPH} and {specific_project_details}) Analyze the resource requirements for {project_name} given the current scope and provide recommendations for optimal resource allocation.\",\n",
        "]\n",
        "prompts_project_management_presentation =[\n",
        "# Plan/Presentation\n",
        "  f\"14.Write a sensational press release for this Project  Management and perviuse  contnet made\" , #in this topic ({TOPIC}) and this field : '{PARAGRAPH}'\",\n",
        "  f\"15.Make this Project Management more persuasive in this topic ({TOPIC}) and this description: '{PARAGRAPH}'\",\n",
        "  f\"16.Write 3 tweeter post about this  Project Management, and this the perviuse contnet made?\",\n",
        "  f\"17.Write 3 Instagram post about this  Project Management, \", # and this the perviuse contnet made?\",\n",
        "  f\"18.Write 1 medium weblog post about this  Project Management, \", #and this the perviuse contnet made?\",\n",
        "  f\"19.Write 1 LinkedIn post about this  Project Management, \", #and this the perviuse contnet made?\",\n",
        "\n",
        "]\n",
        "\n",
        "prompts_project_management_title = [\n",
        "  f\"0. As ChatGPT role: {role}, suggest a Project Management title in less than 15 words, based on the topic {TOPIC} and the description:({PARAGRAPH}).\",\n",
        "\n",
        "  \"1. Come up with questions to ask during the meeting to start the project.\",\n",
        "  \"2. Design a project risk assessment template.\",\n",
        "  \"3. Provide a handoff and project conclusion checklist.\",\n",
        "  \"4. Create a comprehensive project plan for launching a new e-commerce website that spans over 6 months and involves multiple departments.\",\n",
        "  \"5. Generate a project timeline with milestones, start dates, end dates, objectives, tasks, and responsible parties.\",\n",
        "  \"6. Identify potential risks in the project and suggest mitigation strategies.\",\n",
        "  \"7. Review the identified risks in the project and check if any risks have been overlooked.\",\n",
        "  \"8. Draft a project update email for stakeholders highlighting key achievements, current challenges, and next steps.\",\n",
        "  \"9. Outline the key points from a report focusing on certain themes.\",\n",
        "  \"10. Simulate the dialogue and provide step by step guidance to prepare when dealing with a resistant stakeholder.\",\n",
        "  \"11. Ask for additional information required to complete a particular task.\",\n",
        "  \"12. Request a more detailed breakdown of a task into specific, step-by-step assignments.\",\n",
        "  \"13. Analyze the resource requirements for a project given its current scope and provide recommendations for optimal resource allocation.\",\n",
        "]\n",
        "prompts_project_management_title_presentation_title=[\n",
        "#Plan/Presentation\n",
        "f\". Write a sensational press release for this report\",\n",
        "f\". Make this more persuasive\",\n",
        "f\". 3 tweets about thi Report:\",\n",
        "\n",
        "f\". 3 Instagram Post about this Report:\",\n",
        "f\". Medium Post about this Report:\",\n",
        "f\". LinkedIn Post about this Report:\",\n",
        "\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "hGPql56EBVAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\n main variable is :', main_variables_0)\n",
        "folder_chatGPT = \"/content/ChatGPT_academic_paper\"\n",
        "#openai_api_0 = \"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\"\n",
        "#openai_api = \"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\"\n",
        "global TOPIC_CLASS,gdrive_fpath\n",
        "\n",
        "\n",
        "class TOPIC_CLASS:\n",
        "    def __init__(self):\n",
        "        self.category = {\n",
        "        \"perviuse_try_numner\" : 0,\n",
        "        \"perviuse_content\" : ['fist step'],\n",
        "        \"topic\" : f\"{TOPIC}\",\n",
        "        \"name\" : \"\",\n",
        "        \"main_variables\" : main_variables_0,\n",
        "        }\n",
        "\n",
        "    def saving(self,topic,category):\n",
        "\n",
        "       self.Topic_Name = topic\n",
        "       self.Topic_Name_abr = self.Topic_Name[:5]\n",
        "       self.save_folder_dest = gdrive_fpath +f\"{Topic_Name_abr}\"+\"_T/\"+category.replace(' ','_')\n",
        "\n",
        "TP= TOPIC_CLASS()\n",
        "#TOPIC = f\" \""
      ],
      "metadata": {
        "id": "QsstPoufBcYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text to image codes:"
      ],
      "metadata": {
        "id": "5DEF-umuDFNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/glide-text2im"
      ],
      "metadata": {
        "id": "GZvsn2_wQesY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQfAeXC4E_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class ImageGenerator:\n",
        "   def __init__(self, prompt=\"\", batch_size=1, guidance_scale=3.0, upsample_temp=0.997):\n",
        "       self.prompt = prompt\n",
        "       self.batch_size = batch_size\n",
        "       self.guidance_scale = guidance_scale\n",
        "       self.upsample_temp = upsample_temp\n",
        "\n",
        "       self.has_cuda = th.cuda.is_available()\n",
        "       self.device = th.device('cpu' if not self.has_cuda else 'cuda')\n",
        "\n",
        "       # Create base model.\n",
        "       self.options = model_and_diffusion_defaults()\n",
        "       self.options['use_fp16'] = self.has_cuda\n",
        "       self.options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n",
        "       self.model, self.diffusion = create_model_and_diffusion(**self.options)\n",
        "       self.model.eval()\n",
        "       if self.has_cuda:\n",
        "           self.model.convert_to_fp16()\n",
        "       self.model.to(self.device)\n",
        "       self.model.load_state_dict(load_checkpoint('base', self.device))\n",
        "\n",
        "       # Create upsampler model.\n",
        "       self.options_up = model_and_diffusion_defaults_upsampler()\n",
        "       self.options_up['use_fp16'] = self.has_cuda\n",
        "       self.options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "       self.model_up, self.diffusion_up = create_model_and_diffusion(**self.options_up)\n",
        "       self.model_up.eval()\n",
        "       if self.has_cuda:\n",
        "           self.model_up.convert_to_fp16()\n",
        "       self.model_up.to(self.device)\n",
        "       self.model_up.load_state_dict(load_checkpoint('upsample', self.device))\n",
        "\n",
        "   def show_images(self, batch: th.Tensor):\n",
        "       \"\"\" Display a batch of images inline. \"\"\"\n",
        "       scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "       reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "       display(Image.fromarray(reshaped.numpy()))\n",
        "\n",
        "   def generate_image(self):\n",
        "       # Create the text tokens to feed to the model.\n",
        "       tokens = self.model.tokenizer.encode(self.prompt)\n",
        "       tokens, mask = self.model.tokenizer.padded_tokens_and_mask(\n",
        "           tokens, self.options['text_ctx']\n",
        "       )\n",
        "\n",
        "       # Create the classifier-free guidance tokens (empty)\n",
        "       full_batch_size = self.batch_size * 2\n",
        "       uncond_tokens, uncond_mask = self.model.tokenizer.padded_tokens_and_mask(\n",
        "           [], self.options['text_ctx']\n",
        "       )\n",
        "\n",
        "       # Pack the tokens together into model kwargs.\n",
        "       model_kwargs = dict(\n",
        "           tokens=th.tensor(\n",
        "               [tokens] * self.batch_size + [uncond_tokens] * self.batch_size, device=self.device\n",
        "           ),\n",
        "           mask=th.tensor(\n",
        "               [mask] * self.batch_size + [uncond_mask] * self.batch_size,\n",
        "               dtype=th.bool,\n",
        "               device=self.device,\n",
        "           ),\n",
        "       )\n",
        "\n",
        "       # Create a classifier-free guidance sampling function\n",
        "       def model_fn(x_t, ts, **kwargs):\n",
        "          half = x_t[: len(x_t) // 2]\n",
        "          combined = th.cat([half, half], dim=0)\n",
        "          model_out = self.model(combined, ts, **kwargs)\n",
        "          eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "          cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "          half_eps = uncond_eps + self.guidance_scale * (cond_eps - uncond_eps)\n",
        "          eps = th.cat([half_eps, half_eps], dim=0)\n",
        "          return th.cat([eps, rest], dim=1)\n",
        "\n",
        "         # Sample from the base model.\n",
        "       self.model.del_cache()\n",
        "       samples = self.diffusion.p_sample_loop(\n",
        "          model_fn,\n",
        "          (full_batch_size, 3, self.options[\"image_size\"], self.options[\"image_size\"]),\n",
        "          device=self.device,\n",
        "          clip_denoised=True,\n",
        "          progress=True,\n",
        "          model_kwargs=model_kwargs,\n",
        "          cond_fn=None,\n",
        "      )[:self.batch_size]\n",
        "      self.model.del_cache()\n",
        "\n",
        "      # Show the output\n",
        "      self.show_images(samples)\n",
        "      # Upsample the 64x64 samples\n",
        "      tokens = self.model_up.tokenizer.encode(self.prompt)\n",
        "      tokens, mask = self.model_up.tokenizer.padded_tokens_and_mask(\n",
        "           tokens, self.options_up['text_ctx']\n",
        "       )\n",
        "\n",
        "       # Create the model conditioning dict.\n",
        "       model_kwargs = dict(\n",
        "           # Low-res image to upsample.\n",
        "           low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "           # Text tokens\n",
        "           tokens=th.tensor(\n",
        "               [tokens] * self.batch_size, device=self.device\n",
        "           ),\n",
        "           mask=th.tensor(\n",
        "               [mask] * self.batch_size,\n",
        "               dtype=th.bool,\n",
        "               device=self.device,\n",
        "           ),\n",
        "       )\n",
        "\n",
        "\n",
        "      # Sample from the base model.\n",
        "       self.model_up.del_cache()\n",
        "      up_shape = (self.batch_size, 3, self.options_up[\"image_size\"], self.options_up[\"image_size\"])\n",
        "      up_samples = self.diffusion_up.ddim_sample_loop(\n",
        "          self.model_up,\n",
        "          up_shape,\n",
        "          noise=th.randn(up_shape, device=self.device) * self.upsample_temp,\n",
        "          device=self.device,\n",
        "          clip_denoised=True,\n",
        "          progress=True,\n",
        "          model_kwargs=model_kwargs,\n",
        "            cond_fn=None,\n",
        "      )[:self.batch_size]\n",
        "      self.model_up.del_cache()\n",
        "\n",
        "      # Show the output\n",
        "      self.show_images(up_samples)\n",
        "generator = ImageGenerator(\"Your prompt here\")\n",
        "generator.generate_image()"
      ],
      "metadata": {
        "id": "RHq5Y1OSOFQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "from docx import Document\n",
        "from django.utils.text import slugify\n",
        "\n",
        "def text_to_image(content,prompt,doc,p,save_patch):\n",
        "  image_prompt = prompt # f\"creat image for the : prompt ((({prompt})) which made this content :((({content}))) and consider this is the official repost\"\n",
        "  negative_prompt = \"low quality, bad quality, no text in picture and verbal content\"\n",
        "  #image = pipeline(image_prompt).images[0] #\"An image of a squirrel in Picasso style\").images[0]\n",
        "  image = pipeline(prompt = image_prompt, negative_prompt=negative_prompt,prior_guidance_scale =1.0, height=768, width=768).images[0]\n",
        "  print ('\\n image_prompt is: ',image_prompt)\n",
        "  image\n",
        "  path = save_patch+f\"_\"+str(slugify(prompt[:15]))+\".png\"\n",
        "  image.save(path)\n",
        "  p = doc.add_picture(path)\n",
        "  return doc, image\n",
        "#doc = Document()\n",
        "# Add the generated text to the document\n",
        "#p = doc.add_paragraph(\"test\\n\\n\") #prompt_my)\n",
        "\n",
        "#doc,image = text_to_image(doc,f\"Repost_Type_Title For: \"+\"prompt_my\",doc,p,folder_path)\n",
        "#doc.save(folder_path+'/image.docx')\n",
        "#image"
      ],
      "metadata": {
        "id": "sEkW9FJYFfQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class ImageGenerator:\n",
        "   def __init__(self, prompt=\"\", batch_size=1, guidance_scale=3.0, upsample_temp=0.997):\n",
        "       self.prompt = prompt\n",
        "       self.batch_size = batch_size\n",
        "       self.guidance_scale = guidance_scale\n",
        "       self.upsample_temp = upsample_temp\n",
        "\n",
        "       self.has_cuda = th.cuda.is_available()\n",
        "       self.device = th.device('cpu' if not self.has_cuda else 'cuda')\n",
        "\n",
        "       # Create base model.\n",
        "       self.options = model_and_diffusion_defaults()\n",
        "       self.options['use_fp16'] = self.has_cuda\n",
        "       self.options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n",
        "       self.model, self.diffusion = create_model_and_diffusion(**self.options)\n",
        "       self.model.eval()\n",
        "       if self.has_cuda:\n",
        "           self.model.convert_to_fp16()\n",
        "       self.model.to(self.device)\n",
        "       self.model.load_state_dict(load_checkpoint('base', self.device))\n",
        "\n",
        "       # Create upsampler model.\n",
        "       self.options_up = model_and_diffusion_defaults_upsampler()\n",
        "       self.options_up['use_fp16'] = self.has_cuda\n",
        "       self.options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "       self.model_up, self.diffusion_up = create_model_and_diffusion(**self.options_up)\n",
        "       self.model_up.eval()\n",
        "       if self.has_cuda:\n",
        "           self.model_up.convert_to_fp16()\n",
        "       self.model_up.to(self.device)\n",
        "       self.model_up.load_state_dict(load_checkpoint('upsample', self.device))\n",
        "\n",
        "   def show_images(self, batch: th.Tensor):\n",
        "       \"\"\" Display a batch of images inline. \"\"\"\n",
        "       scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "       reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "       display(Image.fromarray(reshaped.numpy()))\n",
        "      \n",
        "   def generate_image(self):\n",
        "      # Create the text tokens to feed to the model.\n",
        "      tokens = self.model.tokenizer.encode(self.prompt)\n",
        "      tokens, mask = self.model.tokenizer.padded_tokens_and_mask(\n",
        "          tokens, self.options['text_ctx']\n",
        "      )\n",
        "\n",
        "      # Create the classifier-free guidance tokens (empty)\n",
        "      full_batch_size = self.batch_size * 2\n",
        "      uncond_tokens, uncond_mask = self.model.tokenizer.padded_tokens_and_mask(\n",
        "          [], self.options['text_ctx']\n",
        "      )\n",
        "\n",
        "      # Pack the tokens together into model kwargs.\n",
        "      model_kwargs = dict(\n",
        "          tokens=th.tensor(\n",
        "              [tokens] * self.batch_size + [uncond_tokens] * self.batch_size, device=self.device\n",
        "          ),\n",
        "          mask=th.tensor(\n",
        "              [mask] * self.batch_size + [uncond_mask] * self.batch_size,\n",
        "              dtype=th.bool,\n",
        "              device=self.device,\n",
        "          ),\n",
        "      )\n",
        "\n",
        "      # Create a classifier-free guidance sampling function\n",
        "   def model_fn(x_t, ts, **kwargs):\n",
        "         half = x_t[: len(x_t) // 2]\n",
        "         combined = th.cat([half, half], dim=0)\n",
        "         model_out = self.model(combined, ts, **kwargs)\n",
        "         eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "         cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "         half_eps = uncond_eps + self.guidance_scale * (cond_eps - uncond_eps)\n",
        "         eps = th.cat([half_eps, half_eps], dim=0)\n",
        "         return th.cat([eps, rest], dim=1)\n",
        "\n",
        "        # Sample from the base model.\n",
        "   self.model.del_cache()\n",
        "   samples = self.diffusion.p_sample_loop(\n",
        "         model_fn,\n",
        "         (full_batch_size, 3, self.options [\"image_size\"], self.options [\"image_size\"]),\n",
        "         device=self.device,\n",
        "         clip_denoised=True,\n",
        "         progress=True,\n",
        "         model_kwargs=model_kwargs,\n",
        "         cond_fn=None,\n",
        "   )[:self.batch_size]\n",
        "   self.model.del_cache()\n",
        "\n",
        "   # Show the output\n",
        "   self.show_images(samples)\n",
        "   # Upsample the 64x64 samples\n",
        "   tokens = self.model_up.tokenizer.encode(self.prompt)\n",
        "   tokens, mask = self.model_up.tokenizer.padded_tokens_and_mask(\n",
        "          tokens, self.options_up['text_ctx']\n",
        "   )\n",
        "   \n",
        "       # Create the model conditioning dict.\n",
        "      model_kwargs = dict(\n",
        "           # Low-res image to upsample.\n",
        "           low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "           # Text tokens\n",
        "           tokens=th.tensor(\n",
        "               [tokens] * self.batch_size, device=self.device\n",
        "           ),\n",
        "           mask=th.tensor(\n",
        "               [mask] * self.batch_size,\n",
        "               dtype=th.bool,\n",
        "               device=self.device,\n",
        "           ),\n",
        "       )\n",
        "\n",
        "\n",
        "      # Sample from the base model.\n",
        "     self.model_up.del_cache()\n",
        "     up_shape = (self.batch_size, 3, self.options_up[\"image_size\"], self.options_up[\"image_size\"])\n",
        "     up_samples = self.diffusion_up.ddim_sample_loop(\n",
        "          self.model_up,\n",
        "          up_shape,\n",
        "          noise=th.randn(up_shape, device=self.device) * self.upsample_temp,\n",
        "          device=self.device,\n",
        "          clip_denoised=True,\n",
        "          progress=True,\n",
        "          model_kwargs=model_kwargs,\n",
        "            cond_fn=None,\n",
        "     )[:self.batch_size]\n",
        "     self.model_up.del_cache()\n",
        "\n",
        "      # Show the output\n",
        "     self.show_images(up_samples)\n",
        "generator = ImageGenerator(\"Your prompt here\")\n",
        "generator.generate_image()"
      ],
      "metadata": {
        "id": "kwItLBbeON2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "from docx import Document\n",
        "from django.utils.text import slugify\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Set the max_split_size_mb environment variable\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8\"\n",
        "\n",
        "def process_data_0(input, output_size=(512, 512), scale_factors=(2, 2)):\n",
        "  if input.ndimension() == 4 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "  elif input.ndimension() == 5 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "\n",
        "  del input\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  output = output.cpu().numpy()\n",
        "  output = np.transpose(output, (1, 2, 0))\n",
        "  output = output * 255\n",
        "  return output\n",
        "\n",
        "def process_data(input, output_size=(512, 512), scale_factors=(2, 2)):\n",
        "  output = None # Initialize output to None\n",
        "  if input.ndimension() == 4 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "  elif input.ndimension() == 5 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "\n",
        "  del input\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if output is not None:\n",
        "      output = output.cpu().numpy() # Move to CPU and convert to numpy\n",
        "      output = np.transpose(output, (1, 2, 0)) # Change the order of dimensions\n",
        "      output = output * 255 # Scale the values to [0, 255]\n",
        "  return output\n",
        "def text_to_image_4(content, prompt, doc, p, save_path):\n",
        "  image_prompt = prompt\n",
        "  negative_prompt = \"low quality, bad quality\"\n",
        "  image = pipeline(prompt=image_prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, height=768, width=768).images[0]\n",
        "  print('\\n image_prompt is: ', image_prompt)\n",
        "\n",
        "  # Convert the PIL.Image object to a PyTorch tensor\n",
        "  image = ToTensor()(image)\n",
        "\n",
        "  processed_image = process_data(image)\n",
        "\n",
        "  del image\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  path = save_path + \"_\" + str(slugify(prompt[:15])) + \".png\"\n",
        "  cv2.imwrite(path, processed_image)\n",
        "\n",
        "  p = doc.add_picture(path)\n",
        "  return doc, image\n",
        "\n",
        "#response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"])"
      ],
      "metadata": {
        "id": "jN4TDZUMeLDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_data(input, output_size=(512, 512), scale_factors=(2, 2)):\n",
        "  output = None # Initialize output to None\n",
        "  if input.ndimension() == 4 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "  elif input.ndimension() == 5 and mode == \"nearest\":\n",
        "      output = torch.nn.functional.interpolate(input, size=output_size, mode='nearest')\n",
        "\n",
        "  del input\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if output is not None:\n",
        "      output = output.cpu().numpy() # Move to CPU and convert to numpy\n",
        "      output = np.transpose(output, (1, 2, 0)) # Change the order of dimensions\n",
        "      output = output * 255 # Scale the values to [0, 255]\n",
        "  return output\n",
        "\n",
        "\n",
        "def text_to_image_3(content, prompt, doc, p, save_path):\n",
        " image_prompt = prompt\n",
        " negative_prompt = \"low quality, bad quality\"\n",
        " image = pipeline(prompt=image_prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, height=768, width=768).images[0]\n",
        " print('\\n image_prompt is: ', image_prompt)\n",
        "\n",
        " # Convert the PIL.Image object to a PyTorch tensor\n",
        " image = ToTensor()(image)\n",
        "\n",
        " processed_image = process_data(image)\n",
        "\n",
        " # Check if processed_image is not empty\n",
        " if True : #processed_image is not None and not np.all(processed_image == 0):\n",
        "     path = save_path + \"_\" + str(slugify(prompt[:15])) + \".png\"\n",
        "     cv2.imwrite(path, processed_image)\n",
        "\n",
        "     p = doc.add_picture(path)\n",
        " else:\n",
        "     print(\"Warning: processed_image is empty or contains only zeros.\")\n",
        "\n",
        " return doc, image\n",
        "\n",
        "#response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"])"
      ],
      "metadata": {
        "id": "PIFKDG2q2Jq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.transforms import ToTensor\n",
        "import cv2\n",
        "import numpy as np\n",
        "from urllib.parse import quote\n",
        "\n",
        "def slugify(text):\n",
        "   return quote(text, safe='')\n",
        "\n",
        "\n",
        "def text_to_image_1(content, prompt, doc, p, save_path):\n",
        "   image_prompt = prompt\n",
        "   negative_prompt = \"low quality, bad quality\"\n",
        "   image = pipeline(prompt=image_prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, height=768, width=768).images[0]\n",
        "   print('\\n image_prompt is: ', image_prompt)\n",
        "\n",
        "   # Convert the PIL.Image object to a PyTorch tensor\n",
        "   image = ToTensor()(image)\n",
        "\n",
        "   processed_image = process_data(image)\n",
        "\n",
        "   # Check if processed_image is not empty\n",
        "   if processed_image is not None and not np.all(processed_image == 0):\n",
        "       # Ensure the directory exists\n",
        "       os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "       path = save_path + \"_\" + str(slugify(prompt[:15])) + \".png\"\n",
        "       success = cv2.imwrite(path, processed_image)\n",
        "       if not success:\n",
        "           print(\"Failed to save image\")\n",
        "       else:\n",
        "           p = doc.add_picture(path)\n",
        "   else:\n",
        "       print(\"Warning: processed_image is empty or contains only zeros.\")\n",
        "\n",
        "   return doc, image\n",
        "\n",
        "#response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"])"
      ],
      "metadata": {
        "id": "tHtmx8kvE5_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the max_split_size_mb environment variable\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1\"\n",
        "def text_to_image_2(content, prompt, doc, p, save_path):\n",
        " image_prompt = prompt\n",
        " negative_prompt = \"low quality, bad quality\"\n",
        " image = pipeline(prompt=image_prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, height=768, width=768).images[0]\n",
        " print('\\n image_prompt is: ', image_prompt)\n",
        "\n",
        " # Convert the PIL.Image object to a PyTorch tensor\n",
        " image = ToTensor()(image)\n",
        "\n",
        " processed_image = process_data(image)\n",
        "\n",
        " # Check if processed_image is not empty\n",
        " if processed_image is not None and not np.all(processed_image == 0):\n",
        "     path = save_path + \"_\" + str(slugify(prompt[:15])) + \".png\"\n",
        "     cv2.imwrite(path, processed_image)\n",
        "\n",
        "     p = doc.add_picture(path)\n",
        " else:\n",
        "     print(\"Warning: processed_image is empty or contains only zeros.\")\n",
        "\n",
        " return doc, image\n",
        "\n",
        "#response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"])"
      ],
      "metadata": {
        "id": "NdKgzZ4IHwh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "def text_to_image_openai(content, prompt, doc, p, save_path):\n",
        " image_prompt = prompt\n",
        " negative_prompt = \"negative prompts so are low quality, bad quality\"\n",
        " print('\\n image_prompt is: ', image_prompt)\n",
        "\n",
        " # Generate the image using OpenAI API\n",
        " response = client.images.generate(\n",
        "   model=\"dall-e-2\",\n",
        "   prompt= image_prompt+negative_prompt,\n",
        "   size=\"1024x1024\",\n",
        "   quality=\"standard\",\n",
        "   n=1)\n",
        "\n",
        " image_url = response.data[0].url\n",
        "\n",
        " # Download the image from the URL\n",
        " try:\n",
        "   url = urlopen(image_url)\n",
        "   f = Image.open(url)\n",
        "   f.verify() # verify that it is, in fact an image\n",
        " except Exception as e:\n",
        "   print(f\"Failed to open the image: {e}\")\n",
        "   return doc, None\n",
        "\n",
        " # Save the image\n",
        " path = save_path + \"_\" + str(slugify(prompt[:15])) + \".png\"\n",
        " f.save(path)\n",
        "\n",
        " # Display the image\n",
        " f.show()\n",
        "\n",
        " # Add the image to the document\n",
        " p = doc.add_picture(path)\n",
        " return doc, image\n",
        "\n",
        "#response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"])"
      ],
      "metadata": {
        "id": "xkx-k2rkKF_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Funstions for saving as PDF and DOCX :\n",
        "👇🌱"
      ],
      "metadata": {
        "id": "lI5PG0BNPrPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo apt-get update\n",
        "#!sudo apt-get install libreoffice\n",
        "#!sudo apt-get install abiword\n",
        "#!sudo apt install libreoffice-writer"
      ],
      "metadata": {
        "id": "CfWk5ZiyJ1Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2xiQx_camiB"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def convert_docx_to_pdf0(docx_path, pdf_path):\n",
        "  subprocess.call(['abiword', '--to=pdf', docx_path, '--to-dir', pdf_path])\n",
        "\n",
        "def convert_docx_to_pdf(docx_path, pdf_path):\n",
        " command = ['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', pdf_path, docx_path]\n",
        " subprocess.call(command)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "def add_text_with_bold(paragraph, text,p):\n",
        "   parts = text.split('**')\n",
        "   #doc.style('normal')\n",
        "   #p.style = doc.styles['Normal']\n",
        "\n",
        "   for i in range(len(parts)):\n",
        "       if i % 2 == 0:\n",
        "           p.add_run(parts[i])\n",
        "       else:\n",
        "           run = p.add_run(parts[i])\n",
        "           run.bold = True\n",
        "\n",
        "   return paragraph\n",
        "\n",
        "#add_text_with_bold(doc, 'This is some **bold** text',p)"
      ],
      "metadata": {
        "id": "xWvptzlbCPxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "from docx import Document\n",
        "from django.utils.text import slugify\n",
        "\n",
        "\n",
        "def save_academic_paper(topic, prompt_my):\n",
        "  global docx_path,Pdf_Dir\n",
        "\n",
        "  # Define the path to the folder in Google Drive\n",
        "  folder_path = r\"/content/drive/My Drive/ChatGPT_Paper_wrting/Bussiness_Plane/\"\n",
        "\n",
        "  # Mount Google Drive\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "   # If not, mount the drive\n",
        "     drive.mount('/content/drive')\n",
        "  else:\n",
        "     print(\"Drive is already mounted.\")\n",
        "\n",
        "  Pdf_Dir= r\"/content/drive/My Drive/ChatGPT_Paper_wrting/Bussiness_Plane/\"\n",
        "  docx_path= f\"{folder_path}\"+\"FM\"+f\"{topic}.docx\"\n",
        "  # Check if the folder exists\n",
        "  if not os.path.exists(folder_path):\n",
        "  # If the folder doesn't exist, create it\n",
        "    os.mkdir(folder_path)\n",
        "\n",
        "\n",
        "  # Replace spaces in the topic with underscores\n",
        "  #topic = topic[:12].replace(\" \", \"_\")\n",
        "  topic = slugify(topic)\n",
        "\n",
        "  # Define the path to the DOCX file in the folder\n",
        "\n",
        "\n",
        "  # Check if the DOCX file exists\n",
        "  if os.path.isfile(docx_path):\n",
        "    # If the DOCX file exists, open it\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"#+str(random.randint(0,9))+\".docx\"\n",
        "    doc = Document(docx_path)\n",
        "  else:\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"\n",
        "    #print(docx_path)\n",
        "     # If the DOCX file doesn't exist, create a new Document object\n",
        "    doc = Document()\n",
        "\n",
        "  # Add the generated text to the document\n",
        "  #doc.add_paragraph(prompt_my)\n",
        "  doc = add_text_with_bold(doc,0,prompt_my)\n",
        "  # Save the document\n",
        "  doc.save(docx_path)\n",
        "\n",
        "  # Convert the DOCX file to a PDF\n",
        "  convert_docx_to_pdf(docx_path,Pdf_Dir)\n",
        "  return docx_path,Pdf_Dir"
      ],
      "metadata": {
        "id": "wTIqVCsP17gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os , random\n",
        "import subprocess\n",
        "from docx import Document\n",
        "from django.utils.text import slugify\n",
        "from docx.shared import Pt\n",
        "\n",
        "  # Mount Google Drive\n",
        "if not os.path.isdir('/content/drive'):\n",
        "   # If not, mount the drive\n",
        "     drive.mount('/content/drive')\n",
        "else:\n",
        "     print(\"Drive is already mounted.\")\n",
        "\n",
        "\n",
        "def save_academic_paper_with_prompt_Tile(topic, prompt_my,image_prompt,contnet,try_number, category,make_photo,folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"):\n",
        "  global Pdf_Dir,docx_path\n",
        "  Repost_Type_Title = category\n",
        "  Repost_Type = category.replace(' ','_')\n",
        "\n",
        "\n",
        "  # Define the path to the folder in Google Drive\n",
        "  #folder_path = r\"/content/drive/My Drive/ChatGPT_Paper_wrting/\"+f\"{Repost_Type}/Title/\"\n",
        "  #folder_path = r\"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"+f\"{topic}/{Repost_Type}/Title/\"\n",
        "  folder_path=folder_path+f\"{topic}/{Repost_Type}/Title/\"\n",
        "\n",
        "  Pdf_Dir = folder_path# r\"/content/drive/My Drive/ChatGPT_Paper_wrting/{Repost_Type}/Title/\"\n",
        "\n",
        "  topic = slugify(topic[:21])\n",
        "\n",
        "  if try_number == 0 :\n",
        "       docx_path= f\"{folder_path}\"+f\"{Repost_Type}_T_\"+f\"{topic}.docx\"\n",
        "       if os.path.isfile(docx_path):\n",
        "          try:\n",
        "             pass #os.remove (docx_path.replace('docx','pdf'))\n",
        "             pass #os.remove (docx_path)#\n",
        "\n",
        "          except:\n",
        "             print (\"the file for saving Exist and some error happened\")\n",
        "\n",
        "       topic = topic+str(random.randint(0,100))#os.remove(docx_path)\n",
        "       docx_path= f\"{folder_path}\"+f\"{Repost_Type}_T_\"+f\"{topic}.docx\"\n",
        "\n",
        "\n",
        "  # Check if the folder exists\n",
        "  if not os.path.exists(folder_path):\n",
        "  # If the folder doesn't exist, create it\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "  # Replace spaces in the topic with underscores\n",
        "  #topic = topic[:12].replace(\" \", \"_\")\n",
        "  #topic = slugify(topic)\n",
        "\n",
        "  # Define the path to the DOCX file in the folder\n",
        "\n",
        "  # Check if the DOCX file exists\n",
        "  if os.path.isfile(docx_path):\n",
        "\n",
        "    if try_number == 0 :\n",
        "\n",
        "       if os.path.isfile(docx_path):\n",
        "          try:\n",
        "             pass #os.remove (docx_path.replace('docx','pdf'))\n",
        "             pass #os.remove (docx_path)#\n",
        "\n",
        "          except:\n",
        "             print (\"the file for saving Exist and some error happened\")\n",
        "\n",
        "       #topic = topic+str(random.randint(0,100))#os.remove(docx_path)\n",
        "       #docx_path= f\"{folder_path}\"+f\"{Repost_Type}_T_\"+f\"{topic}.docx\"\n",
        "\n",
        "       doc = Document()\n",
        "       # Add the generated text to the document\n",
        "       p = doc.add_paragraph() #prompt_my)\n",
        "       # Add the generated text to the document\n",
        "       # Add the generated text to the document\n",
        "       p.style = doc.styles['Title']\n",
        "\n",
        "       p = add_text_with_bold(doc, f\"{Repost_Type_Title} For: \"+prompt_my,p)\n",
        "       p.style = doc.styles['Title']\n",
        "\n",
        "\n",
        "       # Check if CUDA is available\n",
        "       try:\n",
        "          if torch.cuda.is_available() and make_photo:\n",
        "             doc,image = text_to_image(contnet,f\"{Repost_Type_Title} For: \"+image_prompt,doc,p,folder_path)\n",
        "          elif make_photo:\n",
        "\n",
        "             doc,image = text_to_image_openai(doc,f\"{Repost_Type_Title} For: \"+image_prompt,doc,p,folder_path)\n",
        "       except errors as e:\n",
        "\n",
        "          print ('failed in running the Image creating by this error :',e)\n",
        "    else :\n",
        "\n",
        "    # If the DOCX file exists, open it\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"#+str(random.randint(0,9))+\".docx\"\n",
        "       doc = Document(docx_path)\n",
        "\n",
        "\n",
        "    # Add the generated text to the document\n",
        "    p = doc.add_paragraph() #prompt_my)\n",
        "    # Add the generated text to the document\n",
        "    # Add the generated text to the document\n",
        "    p.style = doc.styles['Subtitle']\n",
        "\n",
        "    font = p.style.font\n",
        "    font.name = 'Arial'\n",
        "    font.size = Pt(15)\n",
        "    p = add_text_with_bold(doc,prompt_my,p)\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    try:\n",
        "          if torch.cuda.is_available() and make_photo:\n",
        "             doc,image = text_to_image(contnet,f\"{Repost_Type_Title} For: \"+image_prompt,doc,p,folder_path)\n",
        "          elif make_photo:\n",
        "\n",
        "             doc,image = text_to_image_openai(doc,f\"{Repost_Type_Title} For: \"+image_prompt,doc,p,folder_path)\n",
        "    except errors as e:\n",
        "\n",
        "          print ('failed in running the Image creating by this error :',e)\n",
        "\n",
        "    #p.style = doc.styles['Subtitle']\n",
        "\n",
        "  else:\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"\n",
        "    #print(docx_path)\n",
        "     # If the DOCX file doesn't exist, create a new Document object\n",
        "    doc = Document()\n",
        "\n",
        "    # Add the generated text to the document\n",
        "    p = doc.add_paragraph() #prompt_my)\n",
        "    # Add the generated text to the document\n",
        "    # Add the generated text to the document\n",
        "    p.style = doc.styles['Title']\n",
        "\n",
        "\n",
        "    p = add_text_with_bold(doc, f\"{Repost_Type_Title} For: \"+prompt_my,p)\n",
        "    p.style = doc.styles['Title']\n",
        "    #doc,image = text_to_image(contnet,f\"{Repost_Type_Title} For: \"+prompt_my,doc,p,folder_path)\n",
        "\n",
        "\n",
        "  # Save the document\n",
        "  doc.save(docx_path)\n",
        "\n",
        "  # Revert the style of the paragraph back to 'Normal'\n",
        "  p = doc.add_paragraph()#contnet)\n",
        "  p.style = doc.styles['Normal']\n",
        "\n",
        "  p = add_text_with_bold(doc,contnet,p)\n",
        "\n",
        "  # Save the document\n",
        "  doc.save(docx_path)\n",
        "\n",
        "  # Convert the DOCX file to a PDF\n",
        "  convert_docx_to_pdf(docx_path,Pdf_Dir)\n",
        "  print ( f'report made for this category of {category} in the folder of {docx_path}')\n",
        "\n",
        "  return docx_path,Pdf_Dir\n",
        "\n",
        "def save_academic_paper_with_prompt(topic, prompt_my,image_prompt,contnet,try_number,category, make_photo,folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"):\n",
        "  global docx_path_prompt,Pdf_Dir\n",
        "\n",
        "  Repost_Type_Title = category\n",
        "  Repost_Type = category.replace(' ','_')\n",
        "\n",
        "  # Define the path to the folder in Google Drive\n",
        "  #folder_path = r\"/content/drive/My Drive/ChatGPT_Paper_wrting/\"+f\"{Repost_Type}/Prompt/\"\n",
        "  #folder_path = r\"/content/drive/My Drive/ChatGPT_Paper_wrting/\"+f\"All_Reports/{topic}/{Repost_Type}/Prompt/\"\n",
        "  folder_path=folder_path+f\"{topic}/{Repost_Type}/Prompt/\"\n",
        "\n",
        "  Pdf_Dir = folder_path# r\"/content/drive/My Drive/ChatGPT_Paper_wrting/\"+f\"{Repost_Type}/Prompt/\"\n",
        "  topic = slugify(topic[:21])\n",
        "  docx_path_prompt= f\"{folder_path}\"+f\"{Repost_Type}_PR_\"+f\"{topic}.docx\"\n",
        "\n",
        "\n",
        "  # Check if the folder exists\n",
        "  if not os.path.exists(folder_path):\n",
        "  # If the folder doesn't exist, create it\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "  # Replace spaces in the topic with underscores\n",
        "  #topic = topic[:12].replace(\" \", \"_\")\n",
        "  #topic = slugify(topic)\n",
        "\n",
        "  # Define the path to the DOCX file in the folder\n",
        "\n",
        "\n",
        "    # If the DOCX file exists, open it\n",
        "  if try_number == 0 :\n",
        "       try:\n",
        "          os.remove(docx_path_prompt)\n",
        "          os.romove (docx_path_prompt.replace('docx','pdf'))\n",
        "       except:\n",
        "          pass\n",
        "\n",
        "  # Check if the DOCX file exists\n",
        "  if os.path.isfile(docx_path_prompt):\n",
        "    # If the DOCX file exists, open it\n",
        "        #docx_path = f\"{folder_path}{topic}.docx\"#+str(random.randint(0,9))+\".docx\"\n",
        "    if try_number == 0 :\n",
        "\n",
        "       topic = topic+str(random.randint(0,100))#os.remove(docx_path)\n",
        "       docx_path_prompt= f\"{folder_path}\"+f\"{Repost_Type}_PR_\"+f\"{topic}.docx\"\n",
        "       if os.path.isfile(docx_path):\n",
        "          try:\n",
        "             os.remove (docx_path_prompt.replace('docx','pdf'))\n",
        "             os.remove (docx_path_prompt)#\n",
        "\n",
        "          except errors as e:\n",
        "\n",
        "             print (\"the file for saving Exist and some error happened\")\n",
        "       doc = Document()\n",
        "       # Add the generated text to the document\n",
        "       p = doc.add_paragraph()#prompt_my)\n",
        "       # Add the generated text to the document\n",
        "       # Add the generated text to the document\n",
        "       p.style = doc.styles['Title']\n",
        "       p = add_text_with_bold(doc,f\"{Repost_Type_Title} For: \"+ prompt_my,p)\n",
        "\n",
        "       p.style = doc.styles['Title']\n",
        "\n",
        "\n",
        "\n",
        "    else :\n",
        "\n",
        "    # If the DOCX file exists, open it\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"#+str(random.randint(0,9))+\".docx\"\n",
        "       doc = Document(docx_path_prompt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Add the generated text to the document\n",
        "    p = doc.add_paragraph(prompt_my)\n",
        "    # Add the generated text to the document\n",
        "    # Add the generated text to the document\n",
        "    p.style = doc.styles['Subtitle']\n",
        "\n",
        "    font = p.style.font\n",
        "    font.name = 'Arial'\n",
        "    font.size = Pt(15)\n",
        "    p = add_text_with_bold(doc,prompt_my,p)\n",
        "    p.style = doc.styles['Subtitle']\n",
        "\n",
        "  else:\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"\n",
        "    #print(docx_path)\n",
        "     # If the DOCX file doesn't exist, create a new Document object\n",
        "    doc = Document()\n",
        "\n",
        "    # Add the generated text to the document\n",
        "    p = doc.add_paragraph()#prompt_my)\n",
        "    # Add the generated text to the document\n",
        "    # Add the generated text to the document\n",
        "    p.style = doc.styles['Title']\n",
        "    p = add_text_with_bold(doc,f\"{Repost_Type_Title} For: \"+ prompt_my,p)\n",
        "\n",
        "    p.style = doc.styles['Title']\n",
        "\n",
        "  #p = add_text_with_bold(doc,prompt_my,p)  # Save the document\n",
        "  doc.save(docx_path_prompt)\n",
        "\n",
        "  # Revert the style of the paragraph back to 'Normal'\n",
        "  p = doc.add_paragraph()#contnet)\n",
        "  p.style = doc.styles['Normal']\n",
        "\n",
        "  p = add_text_with_bold(doc,contnet,p) # Save the document\n",
        "  doc.save(docx_path_prompt)\n",
        "\n",
        "  # Convert the DOCX file to a PDF\n",
        "  #convert_docx_to_pdf(docx_path,Pdf_Dir)\n",
        "  print ( f'report made for this category of {category} in the folder of {docx_path_prompt}')\n",
        "  return docx_path_prompt,Pdf_Dir\n",
        "\n",
        "#save_academic_paper_with_prompt_Tile('title','**prmpt_mt** is ','contetn',0)\n",
        "#save_academic_paper_with_prompt('title','**prmpt_mt**','content ',0)"
      ],
      "metadata": {
        "id": "FXS5P81E2rId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @title\n",
        "# conect to [Mega.z](https://mega.nz/) cloud for storage files based ot this page:\n",
        "\n",
        "https://colab.research.google.com/github/sudo-ken/Mega-to-Google-Drive/blob/master/Transfer_files_from_Mega_to_Google_Drive.ipynb#scrollTo=Vgh1xlQYGZCO"
      ],
      "metadata": {
        "id": "x7XWsP_amNqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %cd '/content/'\n",
        "# !git clone https://github.com/jeroenmeulenaar/python3-mega.git\n",
        "# !(cd python3-mega; pip install urlobject pycrypto)\n",
        "\n",
        "# import os\n",
        "# os.chdir('python3-mega')\n",
        "# from mega import Mega\n",
        "# os.chdir('../')\n",
        "# m = Mega.from_ephemeral()\n",
        "\n",
        "# !git clone https://github.com/odwyersoftware/mega.py\n",
        "# %cd mega.py\n",
        "# !python setup.py install\n",
        "%cd '/content/'\n",
        "\n",
        "!rm -rf '/content/yoloOnGoogleColab'\n",
        "#!git clone https://github.com/CAR-Driving/yoloOnGoogleColab"
      ],
      "metadata": {
        "id": "L436PiVNmQog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# @markdown <center><h3>Input Your Mega ID</h3></center><br>\n",
        "from functools import wraps\n",
        "import errno\n",
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wraps(func)(wrapper)\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "if not os.path.exists(\"/root/.ipython/ocr.py\"):\n",
        "    from subprocess import run\n",
        "    from shlex import split\n",
        "\n",
        "    shellCmd = \"wget -qq https://raw.githubusercontent.com/biplobsd/OneClickRun/master/res/ocr.py \\\n",
        "                    -O /root/.ipython/ocr.py\"\n",
        "    run(split(shellCmd))\n",
        "from ocr import runSh\n",
        "\n",
        "@timeout(10)\n",
        "def runShT(args):\n",
        "    return runSh(args, output=True)\n",
        "\n",
        "\n",
        "# MEGAcmd installing\n",
        "if not os.path.exists(\"/usr/bin/mega-cmd\"):\n",
        "    print(\"Installing MEGA ...\")\n",
        "    runSh('sudo apt-get -y update')\n",
        "    runSh('sudo apt-get -y install libmms0 libc-ares2 libc6 libcrypto++6 libgcc1 libmediainfo0v5 libpcre3 libpcrecpp0v5 libssl1.1 libstdc++6 libzen0v5 zlib1g apt-transport-https')\n",
        "    runSh('sudo curl -sL -o /var/cache/apt/archives/MEGAcmd.deb https://mega.nz/linux/MEGAsync/Debian_9.0/amd64/megacmd-Debian_9.0_amd64.deb', output=True)\n",
        "    runSh('sudo dpkg -i /var/cache/apt/archives/MEGAcmd.deb', output=True)\n",
        "    print(\"MEGA is installed.\")\n",
        "else:\n",
        "    !pkill mega-cmd\n",
        "\n",
        "# INPUT YOUR MEGA ID\n",
        "\n",
        "USERNAME = 'kayeyoc231@llubed.com'  # @param {type:\"string\"}\n",
        "PASSWORD = \"ss123456\"  # @param {type:\"string\"}\n",
        "if not (USERNAME == \"\" or PASSWORD == \"\"):\n",
        "    try:\n",
        "        runShT(f\"mega-login {USERNAME} {PASSWORD}\")\n",
        "        print (\"megazn login has done successfully\")\n",
        "    except TimeoutError:\n",
        "        runSh('mega-whoami', output=True)\n",
        "else:\n",
        "    print(\"Please Input your Mega IDs.\")\n",
        "# @markdown *After signing in, use the above cell(Main Cell) to transfer files from Mega to GDrive using the transfer quota of your Mega Pro Account*"
      ],
      "metadata": {
        "id": "uqYuzvYFoMIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#!rm -rf '/content/ChatGPT_academic_paper'\n",
        "!mkdir '/content/ChatGPT_academic_paper'\n",
        "%cd '/content/ChatGPT_academic_paper'\n",
        "!echo 't' >> t.txt\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "import shlex\n",
        "import errno, os\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "import shlex\n",
        "from ocr import runSh\n",
        "import re\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "# copy subdirectory example\n",
        "fromDirectory = '/content/ChatGPT_academic_paper/'\n",
        "toDirectory = '/content/gdrive/My Drive/yolo_car_database/'\n",
        "\n",
        "runSh('mega-cd /' )\n",
        "runSh(' mega-mkdir ChatGPT_academic_paper ')\n",
        "runSh(' mega-ls ')\n",
        "runSh('mega-cd ChatGPT_academic_paper ')\n",
        "\n",
        "# copy_tree(fromDirectory, toDirectory)\n",
        "def uplaod_mega_nz(root,cloude='mega',title1=''):\n",
        "  if (cloude=='mega'):\n",
        "  #  from mega import Mega\n",
        "  #  mega = Mega()\n",
        "  #  email = 'woraqofa@ivyandmarj.com'\n",
        "  #  password = 'feripas800@wgraj.com'\n",
        "  #  m = mega.login(email, password)\n",
        "  #  # login using a temporary anonymous account\n",
        "  #  m = mega.login()\n",
        "  #  quota = m.get_quota()\n",
        "  #  # specify unit output kilo, mega, gig, else bytes will output\n",
        "  #  space = m.get_storage_space(kilo=True)\n",
        "  #  m.create_folder('yolo_car_database/'+title1+'/')\n",
        "  #  folder = m.find('yolo_car_database/'+title1+'/')\n",
        "   # m.upload('myfile.doc', folder[0])\n",
        "  #  run_command('mega-cd ~ && mega-mkdir Yolo_car_database && mega-cd Yolo_car_database && mega-put '+ root+'/* . ')\n",
        "\n",
        "   runSh('mega-cd ChatGPT_academic_paper' + title1 + '/')\n",
        "   runSh('mega-put '+ root+' . ')\n",
        "   runSh(' mega-ls')\n",
        "\n",
        "   for path, subdirs, files in os.walk( root):\n",
        "      for name in files:\n",
        "        print (files)\n",
        "          # m.upload(os.path.join(path, name), folder[0])\n",
        "   # public_exported_web_link = m.export('myfile.doc')\n",
        "   # public_exported_web_link = m.export('my_mega_folder/my_sub_folder_to_share')\n",
        "\n",
        "  if (cloude=='google_drive'):\n",
        "     run_command('mkdir '+root+'/')\n",
        "     for path, subdirs, files in os.walk( root):\n",
        "      for name in files:\n",
        "        # print ('cp -r \"'+os.path.join(path, name)+'\" '+'\"/content/gdrive/My Drive/ChatGPT_academic_paper/' + title1 + '/\"')\n",
        "        # print (os.path.join(path, change_name(name)))\n",
        "        # os.rename(os.path.join(path, name), os.path.join(path, change_name(name)))\n",
        "        run_command('cp -r  \"'+os.path.join(path, name)+'\" '+'\"/content/gdrive/My Drive/ChatGPT_academic_paper/' + title1 + '/\"')\n",
        "     try:\n",
        "        copy_tree(root,'/content/gdrive/My Drive/ChatGPT_academic_paper/' + title1 + '/')\n",
        "     except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
        "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
        "            # raise #\n",
        "            i=0\n",
        "def change_file_name(root):\n",
        "  for path, subdirs, files in os.walk( root):\n",
        "      for name in files:\n",
        "        print ('filename is: \"'+os.path.join(path, name))\n",
        "        print (os.path.join(path, change_name(name)))\n",
        "        os.rename(os.path.join(path, name), os.path.join(path, change_name(name)))\n",
        "\n",
        "def run_command(cmd):\n",
        "    with Popen(shlex.split(cmd), stdout=PIPE, bufsize=1, universal_newlines=False) as p:\n",
        "        while True:\n",
        "            line = p.stdout.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line)\n",
        "        exit_code = p.poll()\n",
        "    return exit_code\n",
        "def change_name(name):\n",
        "    name_file = ''.join(name)\n",
        "    name_file = name_file.replace(\" \", \"_\")\n",
        "    name_file = name_file.replace(\"'\", \"_\")\n",
        "    name_file = name_file.replace(\"\\n\", \"\")\n",
        "    name_file = name_file.replace(\"\\t\", \"\")\n",
        "    name_file = name_file.replace(\"!\", \"\")\n",
        "    name_file = name_file.replace(\"-\", \"\")\n",
        "    name_file = name_file.replace(\"$\", \"\")\n",
        "    name_file = re.sub(r'[\\\\/*?:\"<>|]',\"\",name_file)\n",
        "    return name_file\n",
        "def MegaZN_upload_file(command):\n",
        "    # command_title=re.split('([^a-zA-Z0-9])',command)\n",
        "    command_title_1 = re.split('([^a-zA-Z0-9])', command)\n",
        "    command_title = ''.join(command_title_1)\n",
        "    command_title=change_name(command_title)\n",
        "\n",
        "\n",
        "    # call_args = ' youtube-dl --match-title ' + ' \"'+command_title_1[0]+ '\" ' + '\"ytsearch10\":'+'\"'+command+ '\" '+\"--external-downloader-args   '-ss 00:00:03 -t 00:03:08' --write-info-json --write-annotation --write-thumbnail --write-sub -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4'\"\n",
        "    #call_args = ' youtube-dl --match-title ' + ' \"'+command_title_1[0]+ '\" ' + '\"ytsearch100\":'+'\"'+command+ '\" '+\"--playlist-end 3  --external-downloader-args   '-ss 00:00:03 -t 00:03:08' --write-info-json --write-annotation --write-thumbnail --write-sub -f 'best[ext=mp4,height<=1080]+best[filesize<100M]' \"\n",
        "\n",
        "    # call_args = call_args.split() # because call takes a list of strings\n",
        "    # print (call_args)\n",
        "    # call(call_args)\n",
        "    # os.chdir(\"/content/yolo_car_database\")\n",
        "    # run_command('cd \"/content/yolo_car_database\"')\n",
        "    # run_command(call_args)\n",
        "    # os.system(\"cp --recursive '/content/yolo_car_database' '/content/gdrive/My Drive/yolo_car_databe\")\n",
        "    # os.system('pwd && ls ')\n",
        "    # copy_tree(fromDirectory, toDirectory)\n",
        "    # p = Popen(shlex.split(call_args), bufsize=1, universal_newlines=True)\n",
        "    run_command('mkdir /content/ChatGPT_academic_paper/'+command_title)\n",
        "\n",
        "    os.chdir( '/content/ChatGPT_academic_paper/'+command_title+'/')\n",
        "    #run_command(call_args)\n",
        "    # copy_tree('/content/yolo_car_database/'+command_title+'/', '/content/gdrive/My Drive/yolo_car_database/'+command_title+'/')\n",
        "    change_file_name('/content/ChatGPT_academic_paper/'+command_title+'/')\n",
        "    uplaod_mega_nz('/content/ChatGPT_academic_paper/'+command_title+'/','mega',command_title)\n",
        "    try:\n",
        "        # call(call_args)\n",
        "        # run_command('mkdir /content/yolo_car_database/'+command_title)\n",
        "        # os.chdir( '/content/yolo_car_database/'+command_title+'/')\n",
        "        # run_command(call_args)\n",
        "        # copy_tree('/content/yolo_car_database/'+command_title+'/', toDirectory)\n",
        "\n",
        "        # print (call_args)\n",
        "        pp=0\n",
        "\n",
        "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
        "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
        "            raise # re-raise exception if a different error occurred\n",
        "\n",
        "    # return p.poll()\n",
        "\n",
        "# !wget https://github.com/spectrico/car-make-model-classifier-yolo3-python/blob/master/labels.txt\n",
        "\n",
        "i=0\n",
        "#for line in open('/content/yoloOnGoogleColab/car_type_label.txt','r').readlines():\n",
        "    #print (line)\n",
        "    #try:\n",
        "      #Youtube_download_video(line)\n",
        "\n",
        "    #except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
        "        #if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
        "            #i=1 # re-raise exception if a different error occurred\n",
        "    #i=i+1\n",
        "# !cp '/content/yolo_car_database' \"gdrive/My Drive/yolo_car_databe\""
      ],
      "metadata": {
        "id": "8Fx5M6zXmTAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pwd\n",
        "%cd '/content/'\n",
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "from docx import Document\n",
        "from django.utils.text import slugify\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "def save_academic_paper_with_prompt_megazn(topic, prompt_my,contnet, category):\n",
        "  # Define the path to the folder in Google Drive\n",
        "  folder_path = r\"/content/ChatGPT_academic_paper/\"\n",
        "\n",
        "  Pdf_Dir= r\"/content/ChatGPT_academic_paper/\"\n",
        "  docx_path= f\"{folder_path}{topic}.docx\"\n",
        "  # Check if the folder exists\n",
        "  if not os.path.exists(folder_path):\n",
        "  # If the folder doesn't exist, create it\n",
        "    os.mkdir(folder_path)\n",
        "\n",
        "\n",
        "  # Replace spaces in the topic with underscores\n",
        "  #topic = topic[:12].replace(\" \", \"_\")\n",
        "  topic = slugify(topic)\n",
        "\n",
        "  # Define the path to the DOCX file in the folder\n",
        "\n",
        "\n",
        "  # Check if the DOCX file exists\n",
        "  if os.path.isfile(docx_path):\n",
        "    # If the DOCX file exists, open it\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"#+str(random.randint(0,9))+\".docx\"\n",
        "    doc = Document(docx_path)\n",
        "  else:\n",
        "\n",
        "    #docx_path = f\"{folder_path}{topic}.docx\"\n",
        "    #print(docx_path)\n",
        "     # If the DOCX file doesn't exist, create a new Document object\n",
        "    doc = Document()\n",
        "\n",
        "  # Add the generated text to the document\n",
        "  p = doc.add_paragraph(prompt_my)\n",
        "  # Add the generated text to the document\n",
        "  # Add the generated text to the document\n",
        "  p.style = doc.styles['Heading 2']\n",
        "\n",
        "  # Save the document\n",
        "  doc.save(docx_path)\n",
        "\n",
        "  # Revert the style of the paragraph back to 'Normal'\n",
        "  p = doc.add_paragraph(contnet)\n",
        "  p.style = doc.styles['Normal']\n",
        "\n",
        "  # Save the document\n",
        "  doc.save(docx_path)\n",
        "\n",
        "  # Convert the DOCX file to a PDF\n",
        "  convert_docx_to_pdf(docx_path,Pdf_Dir)\n",
        "\n",
        "  os.chdir(folder_path)\n",
        "\n",
        "    # copy_tree('/content/yolo_car_database/'+command_title+'/', '/content/gdrive/My Drive/yolo_car_database/'+command_title+'/')\n",
        "\n",
        "  uplaod_mega_nz('/content/ChatGPT_Paper_wrting/')\n",
        "  uplaod_mega_nz('/content/ChatGPT_Paper_wrting/','google_drive')\n",
        "  return docx_path,Pdf_Dir"
      ],
      "metadata": {
        "id": "nv2vAOr8sskq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#save_academic_paper(\"Persian_\"+'topic','\\n**'+'choice_translated_prompt'+'**\\n'+\"choice_translated\")\n",
        "topic = TOPIC[:20].replace(\" \", \"_\")\n",
        "topic = slugify(TOPIC[:-5])\n",
        "docx_path = f\"{topic}.docx\"\n",
        "\n",
        "print(docx_path)"
      ],
      "metadata": {
        "id": "a76qNIRHs_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running the ChatDev codes based of prompt: 👇🌹🔥🐢🌱"
      ],
      "metadata": {
        "id": "jzbhUax1mMPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import pkg_resources\n",
        "from pkg_resources import DistributionNotFound, VersionConflict\n",
        "import pip\n",
        "import os\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def runSh_(command):\n",
        "   result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n",
        "\n",
        "   if result.returncode != 0:\n",
        "       print(f\"Error: {result.stderr}\")\n",
        "   else:\n",
        "       print(f\"Output: {result.stdout}\")\n",
        "\n",
        "   return result.stdout\n",
        "\n",
        "def runSh(command):\n",
        "  result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n",
        "\n",
        "  if result.returncode != 0:\n",
        "      print(f\"Error: {result.stderr}\")\n",
        "  else:\n",
        "      print(f\"Output: {result.stdout}\")\n",
        "\n",
        "  return result.stdout, result.stderr\n",
        "def check_and_install_requirements(requirements_file_path='', additional_modules=[]):\n",
        "   # If a requirements file path is provided, check and install the requirements\n",
        "   if requirements_file_path:\n",
        "     # Ensure the file exists\n",
        "     if not os.path.isfile(requirements_file_path):\n",
        "         print(f\"The file {requirements_file_path} does not exist.\")\n",
        "         return\n",
        "\n",
        "     # Parse the requirements file\n",
        "     with open(requirements_file_path, 'r') as file:\n",
        "         requirements = file.readlines()\n",
        "\n",
        "     # Iterate over each requirement\n",
        "     for requirement in requirements:\n",
        "         package_name = requirement.strip() # remove leading/trailing white spaces\n",
        "\n",
        "         # Check if the package is installed\n",
        "         package_name_1 = package_name.split('=')\n",
        "         print( '\\n Modules are :',package_name, 'and module_1 is :',f'{package_name_1[0]}' )\n",
        "\n",
        "         spec = importlib.util.find_spec(package_name_1[0])\n",
        "         if spec is None:\n",
        "             print(f\"{package_name} is not installed. Installing...\")\n",
        "\n",
        "             # Install the package\n",
        "             pip.main(['install', package_name])\n",
        "         else:\n",
        "             print(f\"{package_name} is installed.\")\n",
        "\n",
        "   # Iterate over each additional module\n",
        "   for module in additional_modules:\n",
        "     # Check if the module is installed\n",
        "     print( '\\n Modules are :',module)\n",
        "\n",
        "   # Iterate over each additional module\n",
        "   for module in additional_modules:\n",
        "     # Check if the module is installed\n",
        "     module_1 = module.split('=')\n",
        "     print( '\\n Modules are :',module, 'and module_1 is :',f'{module_1[0]}' )\n",
        "     spec = importlib.util.find_spec(f'{module_1[0]} ')\n",
        "     if spec is None:\n",
        "         print(f\"{module} is not installed. Installing...\")\n",
        "\n",
        "         # Install the module\n",
        "         #pip.main(['install', f\"{module}\"])\n",
        "         runSh(f'python3 -m pip install {module} ')\n",
        "     else:\n",
        "         print(f\"{module} is installed.\")\n",
        "\n",
        "#check_and_install_requirements('', ['module1', 'module2'])\n",
        "\n",
        "#check_and_install_requirements('path/to/requirements.txt', ['module1', 'module2'])"
      ],
      "metadata": {
        "id": "uN6Q-cCr8ESc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import Popen, PIPE\n",
        "import os, glob, shutil\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "class ChatDev_Run():\n",
        "\n",
        "\n",
        "   def copy_folder_to_destination(self,src_folder, dst_folder,folder_name):\n",
        "      src_folder = os.path.abspath(src_folder)\n",
        "      dst_folder = os.path.abspath(dst_folder)\n",
        "      for folder in glob.glob(f'{src_folder}/*{folder_name}*/**/**/**', recursive=True):\n",
        "         if os.path.isdir(folder):\n",
        "           dst_folder_path = os.path.join(dst_folder, os.path.relpath(folder, src_folder))\n",
        "           shutil.copytree(folder, dst_folder_path,dirs_exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "   def ChatDev_Doit (self,Prompt,Topic_Name,dst,save_results_Dir):\n",
        "\n",
        "       #Current_Dir = os.getcwd()\n",
        "       #dst = '/content/drive/MyDrive/ChatGPT_Paper_wrting/ChatDev_projec/'\n",
        "\n",
        "       #Check Chat dev is installed or not\n",
        "       if not os.path.exists(dst):\n",
        "          os.makedirs(dst)\n",
        "\n",
        "       os.chdir(dst)\n",
        "       new_destination = dst+'/ChatDev/'\n",
        "       if not os.path.exists(new_destination):\n",
        "          runSh('git clone https://github.com/OpenBMB/ChatDev.git')\n",
        "          #os.makedirs(new_destination)\n",
        "       os.chdir(new_destination)\n",
        "\n",
        "       #check_and_install_requirements('',[ 'colorama==0.1.6',\"openai==0.28.0\" ,'tiktoken', 'tenacity'])\n",
        "\n",
        "       #check_and_install_requirements(f'{new_destination}/requirements.txt')\n",
        "\n",
        "\n",
        "       runSh('python3 -m pip install colorama==0.1.6 openai==0.28.0 tiktoken tenacity')\n",
        "\n",
        "       runSh('python3 -m pip install -r requirements.txt')\n",
        "\n",
        "       os.environ['OPENAI_API_KEY'] = openai_api\n",
        "       #Prompt = Prompt.replace('\\n', ' ')\n",
        "       #Topic_Name = Topic_Name.replace('\\n',' ')\n",
        "\n",
        "       os.chdir(new_destination)\n",
        "       Prompt = Prompt.replace('\"\"\"',\"'''\").replace('\\n', '\\\\n')\n",
        "       Topic_Name = Topic_Name.replace('\"\"\"',\"'''\").replace('\\n', '\\\\n')\n",
        "\n",
        "       print(\"python3 run.py --task \"+ f\"\"\" {Prompt}\"\"\"+\" --name \"+ f'\"\"\"{Topic_Name}\"\"\"')\n",
        "\n",
        "       #runSh(\"python3 run.py --task \"+ f'\"{Prompt}\"'+\" --name \"+ f'\"{Topic_Name}\"')\n",
        "       Result, Error = runSh(\"python3 run.py --task \"+ f\"\"\"{Prompt}'\"\"\"+\" --name \"+ f\"\"\"'{Topic_Name}'\"\"\" ) #+ ' --path '+ f'{new_destination}' )\n",
        "       print(f\"Result: {Result}\")\n",
        "       print(f\"Error: {Error}\")\n",
        "\n",
        "       root = f'{new_destination}/WareHouse/'\n",
        "\n",
        "       self.copy_folder_to_destination(root,save_results_Dir,Topic_Name)\n",
        "\n",
        "\n",
        "       #runSh(f\"cp -r '{new_destination}/WareHouse/' '{save_results_Dir}'\")\n",
        "\n",
        "       return Result, Error\n",
        "\n",
        "\n",
        "\n",
        "   def run_command_1(self,cmd):\n",
        "      with Popen(shlex.split(cmd), stdout=PIPE, bufsize=1, universal_newlines=False) as p:\n",
        "        while True:\n",
        "            line = p.stdout.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            print(line)\n",
        "        exit_code = p.poll()\n",
        "      return exit_code\n",
        "\n",
        "#Current_Dir = os.getcwd()\n",
        "#dst = '/content/drive/MyDrive/ChatGPT_Paper_wrting/ChatDev_projec/'\n",
        "\n",
        "#Results , Error = ChatDev_Run().ChatDev_Doit(f'as a programer do action {ACTION} for this project example for {PROJECT_EXAMPLE} which is por this topic {TOPIC} ',f'{TOPIC}', dst,Current_Dir)\n",
        "#print ( 'Results', Results)"
      ],
      "metadata": {
        "id": "i_cJPo7LmLGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download and upload vidoe form the internet and put it on YouTube 🦋🌹🌀🙏🌺 :"
      ],
      "metadata": {
        "id": "mEIybqas9PXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pillar-youtube-upload"
      ],
      "metadata": {
        "id": "WRtPulC69PDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/pillargg/youtube-upload\n",
        "\n",
        "\n",
        "# Video options\n",
        "options = {\n",
        "    \"title\" : \"Example title\", # The video title\n",
        "    \"description\" : \"Example description\", # The video description\n",
        "    \"tags\" : [\"tag1\", \"tag2\", \"tag3\"],\n",
        "    \"categoryId\" : \"22\",\n",
        "    \"privacyStatus\" : \"private\", # Video privacy. Can either be \"public\", \"private\", or \"unlisted\"\n",
        "    \"kids\" : False, # Specifies if the Video if for kids or not. Defaults to False.\n",
        "    \"thumbnailLink\" : \"https://cdn.havecamerawilltravel.com/photographer/files/2020/01/youtube-logo-new-1068x510.jpg\" # Optional. Specifies video thumbnail.\n",
        "}\n",
        "\n",
        "# upload video\n",
        "#uploader.upload(file_path, options)"
      ],
      "metadata": {
        "id": "SMlH8Ll59vMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#I have tried to update the prompt by ChatGPT itself 👇👇"
      ],
      "metadata": {
        "id": "wxqM4h1fw4fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_main_variables(prompts, variable):\n",
        "   main_variables= main_variables_0\n",
        "   for prompt in prompts:\n",
        "       variables = re.findall(r'\\{(\\w+)\\}', prompt)\n",
        "       for var in variables:\n",
        "           if var == 'TOPIC':\n",
        "               main_variables[var] = variable\n",
        "           else:\n",
        "               main_variables[var] = None\n",
        "\n",
        "   return main_variables"
      ],
      "metadata": {
        "id": "Jd711_MyBgje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_main_variables(prompts, variable_para, var_content):\n",
        "    main_variables = main_variables_0\n",
        "\n",
        "    new_prompt = prompts[:]  # Create a deep copy of prompts\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        for var in variable_para:\n",
        "            if var == variable_para :\n",
        "                main_variables[var] = var_content\n",
        "                new_prompt[i] = prompt.replace(var, var_content)\n",
        "        for var, value in main_variables.items():\n",
        "             if var == variable_para:\n",
        "                main_variables[var] = var_content\n",
        "                new_prompt[i] = new_prompt[i].replace('{'+var+'}', \"{\"+f\"{var}\"+'}'+f\" which is :{var_content}\")#f\"{{{variable}}}\")\n",
        "\n",
        "\n",
        "    return main_variables, new_prompt\n",
        "#TOPIC='my test topic'\n",
        "main_variables, prompt_new = extract_main_variables(prompts, 'TOPIC', TOPIC)\n",
        "print (\" topic is :\", TOPIC)\n",
        "print(\"main variable is :\", main_variables)\n",
        "print(\"new_prompt is :\", prompt_new)\n",
        "prompts= prompt_new\n",
        "print(\"prompt is :\", prompts)"
      ],
      "metadata": {
        "id": "VLFUpVSA2Pbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def generate_prompt_from_response(previous_response, main_variables):\n",
        "   # Customize this logic based on your needs\n",
        "   # For simplicity, let's use the last 50 characters of the response as the new prompt\n",
        "   new_prompt = previous_response[-50:]\n",
        "\n",
        "   # Replace the main variables in the new prompt\n",
        "   for var, value in main_variables.items():\n",
        "       if value is not None:\n",
        "           new_prompt = new_prompt.replace(value, f\"{{{var}}}\")\n",
        "\n",
        "   return new_prompt\n",
        "\n",
        "# Example usage\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"  #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "#TOPIC = f\"Understanding the Cycle of Domestic Violence against iranian women: The Influence of Economic Power\"\n",
        "\n",
        "prompts_1 = [\n",
        "f\"Find a research topic for a PhD in the area of '{TOPIC}'\",\n",
        "\n",
        "]\n",
        "#prompts_2 = [\n",
        "#   \"I need to find information on renewable energy for a research paper that will be 1,500 words and must include eight sources.\",\n",
        "#   \"The research paper should cover the latest advancements in renewable energy technology.\"\n",
        "#]\n",
        "\n",
        "#main_variables,prompt_new = extract_main_variables(prompts,'TOPIC',TOPIC)\n",
        "#print(main_variables)\n",
        "\n",
        "#previous_response = \"Previous GPT response\"\n",
        "#new_prompt = generate_prompt_from_response(previous_response, main_variables)\n",
        "#print(new_prompt)\n",
        "#print ( prompts)"
      ],
      "metadata": {
        "id": "NYegFb6aq_h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Updating the prompt by ChatGPT:👇👇"
      ],
      "metadata": {
        "id": "UgzzXkG9wjbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
      ],
      "metadata": {
        "id": "oJ1QCUr62rtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_update_0(prompt, previous_content, main_variables):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "\n",
        "   # Replace the TOPIC variable in the prompt\n",
        "   for var, value in main_variables.items():\n",
        "       if var == 'TOPIC' and value is not None:\n",
        "           prompt = prompt.replace(f\"{{{var}}}\", value)\n",
        "\n",
        "   # Construct the instruction for updating the prompt\n",
        "   instruction = f\"Given the previous content:\\n\\n{previous_content}\\n\\nUpdate the prompt: {prompt}\"\n",
        "\n",
        "   # Create the completion with the instruction\n",
        "   completion = client.completions.create(\n",
        "       model=model_engine,\n",
        "       prompt=instruction,\n",
        "       max_tokens=max_tokens,\n",
        "       temperature=0.3,\n",
        "       n=1,\n",
        "       frequency_penalty=0,\n",
        "   )\n",
        "\n",
        "   # Extract and return the updated prompt from the response\n",
        "   updated_prompt = completion.choices[0].text.strip()\n",
        "   return updated_prompt"
      ],
      "metadata": {
        "id": "QbOCQqCDB8xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=20, max_time=600)\n",
        "def generate_prompt_update_a1(prompt_my):\n",
        "  model_engine = \"text-davinci-003\"\n",
        "  max_tokens = 2048\n",
        "\n",
        "  completion = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      #model=\"curie\"\n",
        "      prompt=prompt_my,\n",
        "      max_tokens=2048,\n",
        "      temperature=0.3,\n",
        "      n=1,\n",
        "      frequency_penalty=0,\n",
        "  )\n",
        "  return completion"
      ],
      "metadata": {
        "id": "WPRw6qJOuaS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import openai\n",
        "from openai import RateLimitError\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "   # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "   api_key = openai_api# \"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\",\n",
        ")\n",
        "def retry_with_exponential_backoff(\n",
        "   func,\n",
        "   initial_delay: float = 1,\n",
        "   exponential_base: float = 2,\n",
        "   jitter: bool = True,\n",
        "   max_retries: int = 15,\n",
        "   errors: tuple = (RateLimitError,),\n",
        "):\n",
        "   \"\"\"Retry a function with exponential backoff.\"\"\"\n",
        "   def wrapper(*args, **kwargs):\n",
        "       # Initialize variables\n",
        "       num_retries = 0\n",
        "       delay = initial_delay\n",
        "       # Loop until a successful response or max_retries is hit or an exception is raised\n",
        "       while True:\n",
        "           try:\n",
        "               return func(*args, **kwargs)\n",
        "           # Retry on specified errors\n",
        "           except errors as e:\n",
        "               # Increment retries\n",
        "               num_retries += 1\n",
        "               # Check if max retries has been reached\n",
        "               if num_retries > max_retries:\n",
        "                  raise Exception(\n",
        "                      f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
        "                  )\n",
        "               # Increment the delay\n",
        "               delay *= exponential_base * (1 + jitter * random.random())\n",
        "               # Sleep for the delay\n",
        "               time.sleep(delay)\n",
        "           # Raise exceptions for any errors not specified\n",
        "           except Exception as e:\n",
        "               raise e\n",
        "   return wrapper\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=20, max_time=600)\n",
        "\n",
        "@retry_with_exponential_backoff\n",
        "def generate_prompt_update(prompt_my):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "   print(f\"{num_tokens_from_string(prompt_my,model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "\n",
        "   completion = client.completions.create(\n",
        "     model=\"gpt-3.5-turbo-instruct\",\n",
        "     prompt=prompt_my,\n",
        "     max_tokens=500,\n",
        "     temperature=0.1,\n",
        "     n=1,\n",
        "     frequency_penalty=0,\n",
        "   )\n",
        "   return completion\n",
        "\n",
        "\n",
        "@retry_with_exponential_backoff\n",
        "def generate_academic_paper(prompt_my):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "   print(f\"{num_tokens_from_string(prompt_my,model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "\n",
        "\n",
        "   completion = client.completion.create(\n",
        "     #model=\"gpt-3.5-turbo-instruct\",\n",
        "     model=\"gpt-3.5-turbo-1106\",\n",
        "     prompt=prompt_my,\n",
        "     max_tokens=2048,\n",
        "     temperature=0.3,\n",
        "     n=1,\n",
        "     frequency_penalty=0,\n",
        "   )\n",
        "   return completion"
      ],
      "metadata": {
        "id": "O2TNMQ8eFl8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def generate_content(prompts, variables, TOPIC, perviuse_content=\"\"):\n",
        "  # Generate content for each variable\n",
        "  variable_contents = {}\n",
        "\n",
        "\n",
        "  main_variables = main_variables_0\n",
        "  for prompt in prompts:\n",
        "      for var in variables:\n",
        "          if var in prompt:\n",
        "              print(f\"\\n---prompt is ----\\n{prompt}\")\n",
        "              #prompt_my = f\"Generate content for the variable '{var}' in the context of the topic '{TOPIC}'. Please consider the result must be in less than 10 words. The prompt is '{prompt}'. Also the more information for understanding better content is '{perviuse_content}'\"\n",
        "\n",
        "              prompt_my = f\"Generate content for the variable '{var}' in the context of the topic '{TOPIC}'. Please consider the result must be sumerized and as variable for chatgpt academic phd writer assistance. The prompt is '{prompt}'. Also the more information for understanding better content is '{perviuse_content}'\"\n",
        "              content = generate_prompt_update_a1(prompt_my)\n",
        "              for choice in content.choices:\n",
        "                print(\"\\n Result is :\"+choice.text)\n",
        "                variable_contents[var] = choice.text\n",
        "              print(f\"variable_contents[{var}] is: {variable_contents[var]}\")\n",
        "\n",
        "              #time.sleep(random.randint(22, 40))\n",
        "\n",
        "  # Replace the variables in the prompts with the generated content\n",
        "  prompt_new = []\n",
        "  for prompt in prompts:\n",
        "      for var, content in variable_contents.items():\n",
        "          prompt = prompt.replace(f\"{{{var}}}\", variable_contents[var])\n",
        "      prompt_new.append(prompt)\n",
        "      print(\"\\n --- Updated prompt is :\\n\"+str(prompt_new))\n",
        "  return prompt_new\n",
        "\n",
        "# Define main variables\n",
        "#TOPIC = \"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"\n",
        "\n",
        "# List of variables\n",
        "variables = [\"RESEARCH_DOMAIN\", \"PARAGRAPH\", \"PARAGRAPHS\", \"TOPIC_SENTENCE\", \"LANGUAGE\", \"ABSTRACT_PARAGRAPH\", \"BIBLIOGRAPHY\", \"THEORY1\", \"THEORY2\", \"RESEARCH_QUESTIONS\", \"ACTION\", \"RESULT_PARAGRAPHS\", \"DATE\", \"NUMBER_OF_DAYS_MONTHS_YEARS\"]\n",
        "\n",
        "# List of prompts\n",
        "#prompts = [\"prompt1\", \"prompt2\", \"prompt3\"] # replace with your actual prompts\n",
        "#perviuse_content= \"test\"\n",
        "# Call the function and store the returned prompts\n",
        "#updated_prompts = generate_content(prompts, variables, TOPIC, perviuse_content)\n",
        "#print(\"Updated Prompts:\", updated_prompts)"
      ],
      "metadata": {
        "id": "uI21enmkhb0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def generate_content_2(prompts, variables, TOPIC, perviuse_content=\"\"):\n",
        "  # Generate content for each variable\n",
        "  variable_contents = {}\n",
        "  main_variables = main_variables_0\n",
        "  for prompt in prompts:\n",
        "      for var in variables:\n",
        "          if var in prompt:\n",
        "              print(f\"\\n---prompt is ----\\n{prompt}\")\n",
        "              prompt_my = f\"Generate content for the variable '{var}' in the context of the topic '{TOPIC}'. Please consider the result must be in less than 10 words. The prompt is '{prompt}'. Also the more information for understanding better content is '{perviuse_content}'\"\n",
        "              content = generate_prompt_update_a1(prompt_my)\n",
        "              for choice in content.choices:\n",
        "                print(\"\\n Result is :\"+choice.text)\n",
        "                variable_contents[var] = choice.text\n",
        "              print(f\"variable_contents[{var}] is: {variable_contents[var]}\")\n",
        "\n",
        "              #time.sleep(random.randint(22, 40))\n",
        "\n",
        "  main_variables, prompt_new = extract_main_variables(prompts, var, variable_contents[var])\n",
        "\n",
        "  print(\"main variable is :\", main_variables)\n",
        "  print(\"new_prompt is :\", prompt_new)\n",
        "  prompts= prompt_new\n",
        "\n",
        "  # Replace the variables in the prompts with the generated content\n",
        "  prompt_new = []\n",
        "  for prompt in prompts:\n",
        "      for var, content in variable_contents.items():\n",
        "          prompt = prompt.replace(f\"{{{var}}}\", variable_contents[var])\n",
        "      prompt_new.append(prompt)\n",
        "      print(\"\\n --- Updated prompt is :\\n\"+str(prompt_new))\n",
        "  return prompt_new,main_variables\n",
        "\n",
        "# Define main variables\n",
        "#TOPIC = \"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"\n",
        "\n",
        "# List of variables\n",
        "variables = [\"RESEARCH_DOMAIN\", \"PARAGRAPH\", \"PARAGRAPHS\", \"TOPIC_SENTENCE\", \"LANGUAGE\", \"ABSTRACT_PARAGRAPH\", \"BIBLIOGRAPHY\", \"THEORY1\", \"THEORY2\", \"RESEARCH_QUESTIONS\", \"ACTION\", \"RESULT_PARAGRAPHS\", \"DATE\", \"NUMBER_OF_DAYS_MONTHS_YEARS\"]\n",
        "\n",
        "# List of prompts\n",
        "#prompts = [\"prompt1\", \"prompt2\", \"prompt3\"] # replace with your actual prompts\n",
        "#perviuse_content= \"test\"\n",
        "# Call the function and s uptore the returned prompts\n",
        "#updated_prompts = generate_content(prompts, variables, TOPIC, perviuse_content)\n",
        "#print(\"Updated Prompts:\", updated_prompts)"
      ],
      "metadata": {
        "id": "-tAXnnAOkagH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install groq\n",
        "!pip install exa_py\n",
        "\n",
        "from groq import Groq\n",
        "from exa_py import Exa\n",
        "\n",
        "import os\n",
        "\n",
        "# Set the max_split_size_mb environment variable\n",
        "os.environ[\"EXA_API_KEY\"] = \"870ed4c9-8932-4f86-ae3f-ead03a16167c\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Declare the exa search API\n",
        "exa = Exa(api_key=os.getenv(\"EXA_API_KEY\"))\n",
        "\n",
        "# Define your API Model and key\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "utilized_model = \"llama3-70b-8192\"\n",
        "\n",
        "highlights_options = {\n",
        "    \"num_sentences\": 7,  # Length of highlights\n",
        "    \"highlights_per_url\": 1,  # Get the best highlight for each URL\n",
        "}\n",
        "\n",
        "def call_llm(prompt):\n",
        "    search_response = exa.search_and_contents(query=prompt, highlights=highlights_options, num_results=3, use_autoprompt=True)\n",
        "    info = [sr.highlights[0] for sr in search_response.results]\n",
        "\n",
        "    system_prompt = \"You are a Business proposal generator. Read the provided contexts and, if relevant, use them to answer the user's question.\"\n",
        "    user_prompt = f\"Sources: {info}\\nQuestion: {prompt}\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=utilized_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "PhATJsIH--0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import the necessary libraries and set up the API key\n",
        "#!pip install openai\n",
        "import json\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\" #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "#TOPIC = f\"Understanding the Cycle of Domestic Violence against iranian women: The Influence of Economic Power\"\n",
        "\n",
        "\n",
        "#topic = f\"The Importance of Focusing on Waste Collection in a Waste-Filled World\"\n",
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "#client = OpenAI(\n",
        "   # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "   #api_key = \"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\",\n",
        "#)\n",
        "#openai.api_key = os.getenv('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')\n",
        "# Define the prompts\n",
        "prompts3 = [\n",
        "f\"Find a research topic for a PhD in the area of '{topic}'\",\n",
        "f\"Write a detailed proposal on the following research '{topic}'. Make Sure it is free from plagiarism. \",\n",
        "f\"Identify gaps in the literature on '{topic}'\",\n",
        "\"Generate 10 academic research questions about Perviuse action\",\n",
        "f\"Generate a list of research hypotheses related to '{topic}'\"\n",
        "]\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "# Create a Translator object\n",
        "translator = Translator()\n",
        "\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=20, max_time=600)\n",
        "def generate_academic_paper_a0(prompt_my):\n",
        "  model_engine = \"text-davinci-003\"\n",
        "  max_tokens = 2048\n",
        "\n",
        "  completion = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt_my,\n",
        "      max_tokens=2048,\n",
        "      temperature=0.3,\n",
        "      n=1,\n",
        "      frequency_penalty=0,\n",
        "  )\n",
        "  return completion"
      ],
      "metadata": {
        "id": "44bkCJOHtmuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time,random\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "   # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "   api_key = openai_api#\"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\",\n",
        ")\n",
        "\n",
        "\n",
        "global k\n",
        "k=0\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "#ChatGPT image prompt creating\n",
        "def image_prompting(contnet,prompt):\n",
        "    # Generate text with Sumy\n",
        "    parser = PlaintextParser.from_string(contnet, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary_sumy = summarizer(parser.document, 3)\n",
        "    print(\"\\nSumy Summary and remove the html content from this content :\\n\", summary_sumy)\n",
        "    prompt_my = f\"As report logo designer suggest the text to image prompt in the context of the topic '{TOPIC}', in less one paragraph, for the this part of the report :({summary_sumy})\"\n",
        "    image_prompt = generate_prompt_update_a1(prompt_my)\n",
        "    image_prompt = image_prompt.choices[0].text.strip()\n",
        "    print(\"\\n Image Prompt Result is :\",image_prompt)\n",
        "    return image_prompt\n",
        "\n",
        "def update_prompt (prompt, main_variables):\n",
        "        # Replace variables in the prompt with corresponding values from main_variables\n",
        "        for variable, value in main_variables.items():\n",
        "          if variable in prompt:\n",
        "            print ( '\\n variable is :', variable,' \\n and value is :', value)\n",
        "            prompt = prompt.replace(\"{\"+variable+\"}\", value)\n",
        "            prompt = prompt.replace(f\"{{{variable}}}\", value)\n",
        "\n",
        "            print ( '\\n Prompt new us :', prompt)\n",
        "        return prompt\n",
        "\n",
        "\n",
        "def generate_papers(prompts, perviuse_content, perviuse_try_numner,prompt_Word_Topic_0, category='First',main_variables= main_variables_0, make_photo = False):\n",
        "  choice_text_all=[]\n",
        "  global prompt_Word_Topic,k\n",
        "  prompt_Word_Topic_1 = prompt_Word_Topic_0\n",
        "  print('prompt_Word_Topic_1 is :',prompt_Word_Topic_1)\n",
        "  k = perviuse_try_numner\n",
        "  for i in range(perviuse_try_numner, len(prompts), 20):\n",
        "     # Slice the prompts list to get the next 20 prompts\n",
        "     print(\"I is \",i,\" Len of prompt Is:\", len(prompts))\n",
        "\n",
        "     batch = prompts[i:i+20]\n",
        "     print(\"batch is \",batch)\n",
        "\n",
        "\n",
        "     for prompt in batch:\n",
        "\n",
        "         # Print the prompt\n",
        "         print(\"prompt is \",list({prompt}))\n",
        "\n",
        "         #updated_prompts = generate_content(list({prompt}), variables, TOPIC, perviuse_content)\n",
        "\n",
        "         updated_prompts = update_prompt(prompt, main_variables)\n",
        "         print(\"Updated Prompts:\", updated_prompts)\n",
        "\n",
        "         t = 0\n",
        "         while (t == 0):\n",
        "\n",
        "           time.sleep(random.randint(22, 40))\n",
        "           #response = generate_academic_paper_a0(updated_prompts)\n",
        "           response= call_llm(updated_prompts)\n",
        "           print(f\"\\n Generated Content in the Field of {category} is: \")\n",
        "           print(\"========================\\n\")\n",
        "\n",
        "           if not response:\n",
        "             print(\"The response generate_academic_paper_a0 is empty.\")\n",
        "           else:\n",
        "             print(\"\\n The response generate_academic_paper_a0 is not empty and is :\\n\")\n",
        "           t= 1\n",
        "\n",
        "         for choice in response.choices:\n",
        "           print(choice.text)\n",
        "           choice_in_loop = choice.text\n",
        "           choice_text_all.append(choice.text)\n",
        "\n",
        "\n",
        "         #save_academic_paper(topic,'\\n--------**\\n'+updated_prompts+'/n-------**\\n'+choice.text)\n",
        "         if make_photo:\n",
        "            image_prompt = image_prompting(choice.text, updated_prompts)\n",
        "         else:\n",
        "            image_prompt = ''\n",
        "         if k == 0 :\n",
        "            # using string concatenation\n",
        "            new_string = choice.text #+ prompt_Word_Topic_1[1:]\n",
        "            print ('\\n new_string is:',new_string)\n",
        "            prompt_Word_Topic_1[0] = new_string\n",
        "            #prompt_Word_Topic_1[0] = choice.text\n",
        "            prompt_Word_Topic_1[0] = prompt_Word_Topic_1[0].replace ( '\"','')\n",
        "            print ('\\n Prompt for topic is',prompt_Word_Topic_1[0]  )\n",
        "\n",
        "            #ChatGPT image prompt creating\n",
        "            save_academic_paper_with_prompt(TOPIC[:15]+\"_Pr\",prompt_Word_Topic_1[k],image_prompt,\"\",perviuse_try_numner,category,make_photo)\n",
        "            save_academic_paper_with_prompt_Tile(TOPIC[:15]+\"_T\",prompt_Word_Topic_1[k],image_prompt,\"\",perviuse_try_numner,category,make_photo)\n",
        "\n",
        "         else :\n",
        "            save_academic_paper_with_prompt(TOPIC[:15]+\"_Pr\",'\\n**<<<  ChatGPT Prompt is:\\n'+''.join(updated_prompts)+'\\n>>>**\\n',image_prompt,choice.text,perviuse_try_numner,category,make_photo)\n",
        "            save_academic_paper_with_prompt_Tile(TOPIC[:15]+'_T',prompt_Word_Topic_1[k],image_prompt,choice.text,perviuse_try_numner,category,make_photo)\n",
        "\n",
        "         #save_academic_paper_with_prompt_megazn(TOPIC[:15],'\\n**<<<  ChatGPT Prompt is:\\n'+''.join(updated_prompts)+'\\n>>>**\\n',choice.text,category,make_photo)\n",
        "         #save_academic_paper_with_prompt_megazn(TOPIC[:15],'\\n**<<<  ChatGPT Prompt is:\\n'+''.join(updated_prompts)+'\\n>>>**\\n',choice.text, 'google_drive',category,make_photo)\n",
        "         if hasattr(choice, 'choices'):\n",
        "             extract_text(choice)\n",
        "\n",
        "         perviuse_content = choice_in_loop\n",
        "         print(\"\\n end of loop\")\n",
        "         print(\"========================\\n\")\n",
        "\n",
        "         time.sleep(random.randint(22, 40))\n",
        "         perviuse_try_numner = perviuse_try_numner+1\n",
        "         k=k+1\n",
        "  return choice.text,perviuse_try_numner,response\n",
        "\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\" #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "#TOPIC = f\"Understanding the Cycle of Domestic Violence against iranian women: The Influence of Economic Power\""
      ],
      "metadata": {
        "id": "Uf6RCPBz3aqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Results 👇🌹🌱"
      ],
      "metadata": {
        "id": "xHPMNDjfToIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from django.utils.text import slugify\n",
        "import os\n",
        "prompts_business_plan_test = [\n",
        "    f\"suggest one Business Plans repost Title in less than 15 word, based of This Topic :({TOPIC}) and the description:({PARAGRAPH}.\",\n",
        "  ]\n",
        "class Auto_Main():\n",
        "  def __init__(self):\n",
        "        pass\n",
        "  def prompts_all(self): #, topic, description, main_variables_0):\n",
        "        prompts = {\n",
        "            \"Save Variables\" : prompt_save_var,\n",
        "            \"General Course Designing\":prompt_course,\n",
        "            \"Academic Proposal\" : prompts_Academic,\n",
        "            \"Cancel Culture report\" : Prompt_cancel_culture,\n",
        "            \"Psychology 7 Step\" : prompts_Psychology,\n",
        "            \"6 Hat Brainstorming\" : prompt_6Hat_Brainstorm,\n",
        "            \"Code For It\" : prompts_chatdev,\n",
        "            \"Financial Model\" : prompts_finantial,\n",
        "            \"Academic Critique Paper\" : prompts_Academic_proposal_critique,\n",
        "            \"Political Party Constitution\" : prompt_party_chart,\n",
        "            \"Novel Structure\": prompts_story,\n",
        "            \"Radio Script\" : prompt_radio_script,\n",
        "            \"Pitch Deck\": prompts_pitch,\n",
        "            \"The meeting content\" : prompt_meeting,\n",
        "           #\"Course Designing\" : prompts_manual,\n",
        "            \"Game Theory\" : prompts_game_theory_1,\n",
        "            \"Business Plan\" : prompts_business_plan,\n",
        "            \"Project Managment Report\" :prompts_project_management,\n",
        "\n",
        "        }\n",
        "        prompts_topic = {\n",
        "            #\"Course Designing\": prompts_manual_Title,\n",
        "            \"Save Variables\" : prompt_save_var_title,\n",
        "            \"Radio Script\" : prompt_radio_script_title,\n",
        "            \"General Course Designing\":prompt_course_title,\n",
        "            \"Project Managment Report\" : prompts_project_management_title,\n",
        "            \"Pitch Deck\": prompt_Word_Topic_pitch,\n",
        "            \"Psychology 7 Step\" : prompt_Word_Topic_Psychology,\n",
        "            \"Novel Structure\" : prompts_story_topic,\n",
        "            \"Code For It\":prompts_Topic_chatdev,\n",
        "            \"Financial Model\" : prompt_Word_Topic_finantial,\n",
        "            \"Academic Proposal\" : prompt_Word_Topic_Academic,\n",
        "            \"Academic Critique Paper\" : prompt_Word_Topic_Academic_proposal_critique,\n",
        "            \"Game Theory\" : prompt_Word_Topic_game_theory_1,\n",
        "            #\"Pitch Deck\": prompt_Word_Topic_pitch,\n",
        "            \"Business Plan\":prompt_Word_Topic_business_plan,\n",
        "            \"6 Hat Brainstorming\" : prompt_Word_Topic_6Hat_Brainstorm,\n",
        "            \"The meeting content\" : prompt_meeting_title,\n",
        "            \"Cancel Culture report\" : Prompt_cancel_culture_title,\n",
        "            \"Political Party Constitution\" : prompt_party_chart_title,\n",
        "\n",
        "        }\n",
        "\n",
        "        return prompts,prompts_topic\n",
        "\n",
        "  def for_each_category(self, Main_Variable, prompts = None,prompts_topic= None):\n",
        "\n",
        "     if prompts is None:\n",
        "        prompts,prompts_topic = self.prompts_all()\n",
        "\n",
        "     results = {}\n",
        "     for category, category_prompts in prompts.items():\n",
        "        for category_topic, category_prompts_topic in prompts_topic.items():\n",
        "            if (category==category_topic):\n",
        "            #for prompt in category_prompts:\n",
        "                perviuse_try_numner = 0\n",
        "                # Update the prompt with variables\n",
        "                print ( '\\n category is :',category,'\\n category_prompts is:',category_prompts)\n",
        "                #updated_prompt = self.update_prompt(prompt, main_variables_0)\n",
        "                if category == 'Save Variables' :\n",
        "                   topic = TOPIC_CLASS()\n",
        "                   topic.category['main_variables']['category']=category\n",
        "                   for variable, value in topic.category['main_variables'].items():\n",
        "                     prompt =prompt_save_var[1]\n",
        "\n",
        "                     if variable in prompt:\n",
        "                        print ( '\\n variable is :', variable,' \\n and value is :', value)\n",
        "                        prompt = prompt.replace(f\"{{{variable}}}\", value)\n",
        "                        print ( '\\n Prompt new us :', prompt)\n",
        "                   #prompt_save_var[1] == prompt\n",
        "                   #prompt_save_var[2] == topic.category['main_variables'].items()\n",
        "                   save_academic_paper_with_prompt_Tile(TOPIC[:15]+\"_Pr\",TOPIC,\"\",prompt_save_var_title[0],0,category,False)\n",
        "                   save_academic_paper_with_prompt_Tile(TOPIC[:15]+\"_Pr\",prompt_save_var[2],\"\",prompt_save_var_title[1],11,category,False)\n",
        "\n",
        "                   save_academic_paper_with_prompt(TOPIC[:15]+\"_Pr\",TOPIC,\"\",prompt_save_var_title[0],0,category,False)\n",
        "                   save_academic_paper_with_prompt(TOPIC[:15]+\"_Pr\",prompt,\"\",prompt_save_var_title[1],11,category,False)\n",
        "                   category_prompts = category_prompts + prompt_presentation\n",
        "                   category_prompts_topic = category_prompts_topic + prompt_presentation_topic\n",
        "                   #response = self.main(perviuse_try_numner,category_prompts,category_prompts_topic ,category)\n",
        "                   #print ('\\n response is :',response)\n",
        "\n",
        "                elif category == 'Code For It' :\n",
        "                   #response= 'test'\n",
        "\n",
        "                   last_step_result,perviuse_try_numner,response = self.main(perviuse_try_numner,category_prompts,category_prompts_topic ,category)\n",
        "                   print ('\\n response is :',response)\n",
        "\n",
        "                   Topic_Name = Main_Variable['TOPIC']\n",
        "                   Topic_Name_abr = Topic_Name[:5]\n",
        "                   #Current_Dir = os.getcwd()+'/'+f'{topic}/{category}\"\n",
        "                   #save_folder_dest = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"+f\"{Topic_Name_abr}/{category}/\"\n",
        "\n",
        "                   save_folder_dest = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"+f\"{Topic_Name_abr}\"+\"_T/\"+category.replace(' ','_')\n",
        "                   print ( 'save_folder_dest dir is :', save_folder_dest)\n",
        "\n",
        "                   print ( 'save_folder_dest dir is :', save_folder_dest)\n",
        "                   source = '/content/drive/MyDrive/ChatGPT_Paper_wrting/ChatDev_projec/'\n",
        "                   Results = ChatDev_Run().ChatDev_Doit(last_step_result,Topic_Name[:10], source,save_folder_dest)\n",
        "                else:\n",
        "\n",
        "                   perviuse_try_numner = 0\n",
        "                   category_prompts = category_prompts + prompt_presentation\n",
        "                   category_prompts_topic = category_prompts_topic + prompt_presentation_topic\n",
        "                   response = self.main(perviuse_try_numner,category_prompts,category_prompts_topic ,category)\n",
        "                   print ('\\n response is :',response)\n",
        "\n",
        "     return response\n",
        "\n",
        "  def main_generate_papers(self,TOPIC, prompts, perviuse_content, perviuse_try_numner,prompts_topic,category,main_variables):\n",
        "\n",
        "    if not perviuse_try_numner:\n",
        "        perviuse_try_numner = 0\n",
        "        perviuse_content = ['fist step']\n",
        "\n",
        "    elif (perviuse_try_numner == len(prompts)):\n",
        "        perviuse_try_numner = 0\n",
        "        perviuse_content = ['fist step']\n",
        "    #Main_var= topic.category['main_variables']\n",
        "    print('main_var_category is:',main_variables['category'])\n",
        "    last_step_result,perviuse_try_numner_1,results = generate_papers(prompts, perviuse_content, perviuse_try_numner,prompts_topic,category,main_variables)\n",
        "\n",
        "    return last_step_result, perviuse_try_numner_1,results\n",
        "\n",
        "  def main(self,perviuse_try_numner,prompts,prompts_topic,category):\n",
        "     topic = TOPIC_CLASS()\n",
        "     topic.category[\"perviuse_try_numner\"] = perviuse_try_numner\n",
        "     topic.category[\"name\"] = category\n",
        "\n",
        "     if not topic.category[\"perviuse_try_numner\"]:\n",
        "       topic.category[\"perviuse_try_numner\"] = 0\n",
        "       #topic.category[\"perviuse_content\"] = ['fist step']\n",
        "\n",
        "     elif (topic.category[\"perviuse_try_numner\"] == len(prompts)):\n",
        "       topic.category[\"perviuse_try_numner\"] = 0\n",
        "       #topic.category[\"perviuse_content\"] = ['fist step']\n",
        "\n",
        "     topic.topic = TOPIC\n",
        "     topic.category['main_variables']['category']=category\n",
        "     last_step_result,topic.category[\"perviuse_try_numner\"],topic.category[\"results\"] = self.main_generate_papers(topic.topic, prompts, topic.category[\"perviuse_content\"], topic.category[\"perviuse_try_numner\"],prompts_topic,category,topic.category[\"main_variables\"])\n",
        "     #TOPIC_CLASS() == topic\n",
        "     return last_step_result,topic.category[\"perviuse_try_numner\"],topic.category[\"results\"]\n",
        "\n",
        "response = Auto_Main().for_each_category(TOPIC_CLASS().category[\"main_variables\"]) #prompts,prompts_topic,"
      ],
      "metadata": {
        "id": "-YbWDEd4WzDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the `linkedin-auto-post` library in your Python script, you first need to install it using pip:\n",
        "\n",
        "```python\n",
        "pip install linkedin-auto-post\n",
        "```\n",
        "\n",
        "Then, you can import the necessary functions from the library and use them in your script. Here's how you can modify your `main()` function to use `linkedin-auto-post`:\n",
        "\n",
        "```python\n",
        "from linkedin_auto_post import *\n",
        "\n",
        "def main():\n",
        "   file_path = \"linkedin_topic.txt\" # Replace with the actual file path\n",
        "   current_line = 0\n",
        "   while True:\n",
        "       next_line, current_line = text_getter.get_next_line_from_file(file_path, current_line)\n",
        "\n",
        "       if next_line is not None:\n",
        "           selected_text = next_line\n",
        "           print(selected_text)\n",
        "           print(\"\")\n",
        "           prompt = linkedin_prompts.prompt.format(role_and_target_audience = selected_text)\n",
        "           text_content = text_content_generator.openai_generate(prompt)\n",
        "           print(text_content)\n",
        "           print(\"\")\n",
        "           linkedin.login(username=\"linkedin username\",password=\"linkedin password\")\n",
        "           linkedin.upload_content(content=text_content)\n",
        "           time.sleep(21600)\n",
        "       else:\n",
        "           print(\"All Linkedin posts have been Published.\")\n",
        "           break\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        "```\n",
        "\n",
        "In this modified script, `linkedin.login(username=\"linkedin username\",password=\"linkedin password\")` logs into LinkedIn using your LinkedIn username and password, and `linkedin.upload_content(content=text_content)` uploads the generated content to LinkedIn.\n",
        "\n",
        "Please note that you need to replace `\"linkedin username\"` and `\"linkedin password\"` with your actual LinkedIn username and password. Also, remember to handle your credentials securely, as they will be used for logging into LinkedIn [Source 1](https://pypi.org/project/linkedin-auto-post/)."
      ],
      "metadata": {
        "id": "djlP71rbo1Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from linkedin_auto_post import *\n",
        "\n",
        "def main():\n",
        "   file_path = \"linkedin_topic.txt\" # Replace with the actual file path\n",
        "   current_line = 0\n",
        "   while True:\n",
        "       next_line, current_line = text_getter.get_next_line_from_file(file_path, current_line)\n",
        "\n",
        "       if next_line is not None:\n",
        "           selected_text = next_line\n",
        "           print(selected_text)\n",
        "           print(\"\")\n",
        "           prompt = linkedin_prompts.prompt.format(role_and_target_audience = selected_text)\n",
        "           text_content = text_content_generator.openai_generate(prompt)\n",
        "           print(text_content)\n",
        "           print(\"\")\n",
        "           linkedin.login(username=\"linkedin username\",password=\"linkedin password\")\n",
        "           linkedin.upload_content(content=text_content)\n",
        "           time.sleep(21600)\n",
        "       else:\n",
        "           print(\"All Linkedin posts have been Published.\")\n",
        "           break\n",
        "if __name__ == \"__main__\":\n",
        "   main()"
      ],
      "metadata": {
        "id": "kY8SELMNpAy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.phind.com/search?cache=xqbhxgilysl4b8wqweih04eg\n",
        "Sure, here's how you can integrate the `linkedin-content-uploader` library into your existing Python code:\n",
        "\n",
        "First, install the library using pip:\n",
        "\n",
        "```python\n",
        "pip install linkedin-content-uploader\n",
        "```\n",
        "\n",
        "Next, import the library and use the `login` function to log into LinkedIn. You can either use your email and password or your cookies. Then, use the `upload_content` function to post content on LinkedIn.\n",
        "\n",
        "Here's an updated version of your `main` function:\n",
        "\n",
        "```python\n",
        "from linkedin_content_uploader import *\n",
        "import linkedin_json_data\n",
        "import text_content_generator\n",
        "import linkedin_prompts\n",
        "import text_getter\n",
        "import time\n",
        "\n",
        "def main():\n",
        "   file_path = \"linkedin_topic.txt\" # Replace with the actual file path\n",
        "   current_line = 0\n",
        "\n",
        "   # Log in to LinkedIn\n",
        "   linkedin.login(email=\"<Your Email>\", password=\"<Your Password>\")\n",
        "   # Or use cookies if you prefer\n",
        "   # linkedin.login_cookie(cookies=<Your Cookies>)\n",
        "\n",
        "   while True:\n",
        "       next_line, current_line = text_getter.get_next_line_from_file(file_path, current_line)\n",
        "\n",
        "       if next_line is not None:\n",
        "           selected_text = next_line\n",
        "           print(selected_text)\n",
        "           print(\"\")\n",
        "           prompt = linkedin_prompts.prompt.format(role_and_target_audience = selected_text)\n",
        "           text_content = text_content_generator.openai_generate(prompt)\n",
        "           print(text_content)\n",
        "           print(\"\")\n",
        "           \n",
        "           # Upload the content to LinkedIn\n",
        "           linkedin.upload_content(content=text_content)\n",
        "           \n",
        "           time.sleep(21600)\n",
        "       else:\n",
        "           print(\"All Linkedin posts have been Published.\")\n",
        "           break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        "```\n",
        "\n",
        "In this code, replace `<Your Email>` and `<Your Password>` with your actual LinkedIn email and password. If you choose to use cookies, replace `<Your Cookies>` with your actual cookies [Source 0](https://github.com/datakund/linkedin-post-content-python).\n",
        "\n",
        "Please note that automating LinkedIn posts might violate LinkedIn's terms of service. Always ensure that your actions comply with LinkedIn's rules and regulations."
      ],
      "metadata": {
        "id": "Q3lW7_UWnRJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install linkedin-content-uploader"
      ],
      "metadata": {
        "id": "xQBOBADSnamD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/datakund/linkedin-post-content-python.git\n",
        "\n",
        "!cd linkedin-post-content-python\n",
        "\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "hycjlHzLoFG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from linkedin_content_uploader import *\n",
        "import linkedin_json_data\n",
        "import text_content_generator\n",
        "import linkedin_prompts\n",
        "import text_getter\n",
        "import time\n",
        "\n",
        "def main():\n",
        "   file_path = \"linkedin_topic.txt\" # Replace with the actual file path\n",
        "   current_line = 0\n",
        "\n",
        "   # Log in to LinkedIn\n",
        "   linkedin.login(email=\"<Your Email>\", password=\"<Your Password>\")\n",
        "   # Or use cookies if you prefer\n",
        "   # linkedin.login_cookie(cookies=<Your Cookies>)\n",
        "\n",
        "   while True:\n",
        "       next_line, current_line = text_getter.get_next_line_from_file(file_path, current_line)\n",
        "\n",
        "       if next_line is not None:\n",
        "           selected_text = next_line\n",
        "           print(selected_text)\n",
        "           print(\"\")\n",
        "           prompt = linkedin_prompts.prompt.format(role_and_target_audience = selected_text)\n",
        "           text_content = text_content_generator.openai_generate(prompt)\n",
        "           print(text_content)\n",
        "           print(\"\")\n",
        "\n",
        "           # Upload the content to LinkedIn\n",
        "           linkedin.upload_content(content=text_content)\n",
        "\n",
        "           time.sleep(21600)\n",
        "       else:\n",
        "           print(\"All Linkedin posts have been Published.\")\n",
        "           break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main()"
      ],
      "metadata": {
        "id": "gOeeFh1gnU8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the sound by OpenAI: 👇👇\n",
        "\n",
        "https://platform.openai.com/docs/guides/text-to-speech"
      ],
      "metadata": {
        "id": "2ctHZTF6wj0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/ChatGPT_academic_paper\"\n",
        "from openai import OpenAI\n",
        "from django.utils.text import slugify\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "   # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "   api_key = openai_api#\"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\",\n",
        ")\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "    voice=\"alloy\",\n",
        "    model=\"Kamtera/persian-tts-female-glow_tts\", #model=\"tts-1\",\n",
        "    input=TOPIC #\"Hello world! This is a streaming test.\",\n",
        ")\n",
        "TOPIC_S = slugify(TOPIC)\n",
        "\n",
        "\n",
        "Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        "Sound_File = Sound_Folder +str(random.randint (0,1000))+\".mp3\"\n",
        "\n",
        "if not os.path.exists(Sound_Folder):\n",
        "   os.makedirs(Sound_Folder)\n",
        "\n",
        "print (\"save folder is: \",Sound_File)#/content/drive/MyDrive/ChatGPT_Paper_wrting/\"+f\"{TOPIC_S}\"+str(random.randint (0,1000))+\".mp3\") #\"/\"+\"output.mp3\"\n",
        "#response.stream_to_file(\"/content/drive/My Drive/ChatGPT_Paper_wrting/\"+f\"{TOPIC_S}\"+str(random.randint (0,1000))+\".mp3\")#\"/\"+\"output.mp3\")\n",
        "\n",
        "response.stream_to_file(Sound_File)\n",
        "print ( 'topic is:',TOPIC)\n",
        "#print (\"save folder is: /content/drive/MyDrive/ChatGPT_Paper_wrting/\"+f\"{TOPIC[:40]}\"+str(random.randint (0,1000))+\".mp3\") #\"/\"+\"output.mp3\""
      ],
      "metadata": {
        "id": "d2YvDx5DwS7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pydub import AudioSegment\n",
        "from slugify import slugify\n",
        "\n",
        "def extract_course_information(course_design_variables):\n",
        "   course_data = {}\n",
        "\n",
        "   # Retrieve course information from the .docx file\n",
        "   filename = course_design_variables[\"filename\"]\n",
        "   doc = Document(filename)\n",
        "   full_text = []\n",
        "   for para in doc.paragraphs:\n",
        "       full_text.append(para.text)\n",
        "   course_description = '\\n'.join(full_text) if full_text else \"Description not found in the file\"\n",
        "   course_data['course_description'] = course_description\n",
        "\n",
        "   # Split the description into parts\n",
        "   course_parts = course_description.split('\\n')\n",
        "\n",
        "   return course_parts, doc\n",
        "\n",
        "def generate_voice(course_parts, TOPIC):\n",
        "   TOPIC_S = slugify(TOPIC)\n",
        "   Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        "\n",
        "   if not os.path.exists(Sound_Folder):\n",
        "       os.makedirs(Sound_Folder)\n",
        "\n",
        "   audio_files = []\n",
        "   @retry_with_exponential_backoff\n",
        "   for i, part in enumerate(course_parts):\n",
        "       # Generate voice from the part\n",
        "       response = client.audio.speech.create(    voice=\"alloy\",\n",
        "         #model=\"Kamtera/persian-tts-female-glow_tts\", #model=\"tts-1\",\n",
        "         model=\"tts-1\",\n",
        "         input=TOPIC #\"Hello world! This is a streaming test.\",\n",
        "       )\n",
        "       # Save the response content (the audio file) to a local file\n",
        "       Sound_File = Sound_Folder +str(i)+\".mp3\"\n",
        "       response.stream_to_file(Sound_File)\n",
        "\n",
        "       # Add the audio file to the list of audio files\n",
        "       audio_files.append(Sound_File)\n",
        "\n",
        "   # Combine all audio files into a single audio file\n",
        "   combined_audio = sum([AudioSegment.from_file(af) for af in audio_files])\n",
        "\n",
        "   # Estimate the duration of the text-to-speech audio\n",
        "   avg_speed = 150 # average words per minute\n",
        "   total_words = len(' '.join(course_parts).split())\n",
        "   est_duration = total_words / avg_speed # in minutes\n",
        "\n",
        "   # If the estimated duration is less than n minutes, extend the audio\n",
        "   n = 5 # desired duration in minutes\n",
        "   if est_duration < n:\n",
        "       extended_audio = combined_audio * int((n / est_duration) + 1)\n",
        "       extended_audio.export(Sound_Folder+\"extended.mp3\", format='mp3')\n",
        "\n",
        "   combined_audio.export(Sound_Folder+\"combined.mp3\", format='mp3')\n",
        "\n",
        "# Example Usage\n",
        "course_filename = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/Title/Pitch_Deck_T_climate-change-_t.docx\" # \"path_to_your_file.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "TOPIC = \"Your Topic\"\n",
        "\n",
        "# Extract course information\n",
        "course_parts, doc = extract_course_information(course_design_variables)\n",
        "\n",
        "# Generate voice for each part of the course\n",
        "generate_voice(course_parts, TOPIC)"
      ],
      "metadata": {
        "id": "pPoqYzDHHi57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from google.cloud import texttospeech\n",
        "import os\n",
        "import random\n",
        "import slugify\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/ChatGPT_Paper_wrting/All_Reports/\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_voice(course_data, TOPIC):\n",
        "# Convert the Sentence object to a string\n",
        " summary_sumy_str = course_data['course_description']\n",
        "\n",
        "# Initialize the Text-to-Speech client\n",
        " client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "# Set the text input to be synthesized\n",
        " synthesis_input = texttospeech.SynthesisInput(text=summary_sumy_str)\n",
        "\n",
        "# Build the voice request\n",
        " voice = texttospeech.VoiceSelectionParams(\n",
        "    language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
        " )\n",
        "\n",
        "# Select the type of audio file you want returned\n",
        " audio_config = texttospeech.AudioConfig(\n",
        "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
        " )\n",
        "\n",
        "# Perform the text-to-speech request\n",
        " response = client.synthesize_speech(\n",
        "    input=synthesis_input, voice=voice, audio_config=audio_config\n",
        " )\n",
        "\n",
        "# Write the response to the output file.\n",
        " TOPIC_S = slugify(TOPIC)\n",
        " Sound_Folder = folder_path+f\"{TOPIC[:15]}_T/sound_/\"\n",
        " Sound_File = Sound_Folder +str(random.randint (0,1000))+\".mp3\"\n",
        "\n",
        "if not os.path.exists(Sound_Folder):\n",
        "    os.makedirs(Sound_Folder)\n",
        "\n",
        "with open(Sound_File, \"wb\") as out:\n",
        "    out.write(response.audio_content)\n",
        "\n",
        "# Example Usage\n",
        "course_filename = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/Title/Pitch_Deck_T_climate-change-_t.docx\" # \"path_to_your_file.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "TOPIC = \"Your Topic\"\n",
        "\n",
        "# Extract course information\n",
        "course_data, doc = extract_course_information(course_design_variables)\n",
        "\n",
        "# Generate voice for the course\n",
        "generate_voice(course_data, TOPIC)"
      ],
      "metadata": {
        "id": "MDJ4IOWCHBx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install decencies:\n",
        "\n",
        "https://github.com/karim23657/Persian-tts-coqui/blob/5fffc180b65e4aea9dc3afc370feb5b07c7a6690/recepies/glowtts/test-glowtts-model.ipynb#L4"
      ],
      "metadata": {
        "id": "Y7o-50bDQ4IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q git+https://github.com/colefranks/coqui-without-japanese\n",
        "!sudo apt-get -y install espeak-ng"
      ],
      "metadata": {
        "id": "zGgTzSnKRDh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS\n",
        "!sudo apt-get -y install espeak-ng"
      ],
      "metadata": {
        "id": "fIeGK4_-KDyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "# @ Download female model\n",
        "!git clone https://huggingface.co/Kamtera/persian-tts-female-glow_tts\n",
        "\n",
        "# Or also you can download male model\n",
        "#!git clone https://huggingface.co/Kamtera/persian-tts-male-glow_tts\n",
        "\n",
        "\n",
        "!wget \"https://huggingface.co/Kamtera/persian-tts-female-Hifigan/resolve/main/config-3.json\" -O \"config.json\"\n",
        "\n",
        "\n",
        "!wget \"https://huggingface.co/Kamtera/persian-tts-female-Hifigan/resolve/main/checkpoint_378000.pth\" -O \"checkpoint_378000.pth\""
      ],
      "metadata": {
        "id": "JGXGql_eSXe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "def extract_course_information_for_farsi_voice(course_design_variables):\n",
        "  course_data = {}\n",
        "\n",
        "# Retrieve course information from the .docx file\n",
        "  filename = course_design_variables[\"filename\"]\n",
        "  doc = Document(filename)\n",
        "  full_text = []\n",
        "  for para in doc.paragraphs:\n",
        "    full_text.append(para.text)\n",
        "    course_description = '\\n'.join(full_text) if full_text else \"Description not found in the file\"\n",
        "    course_data['course_description'] = course_description\n",
        "\n",
        "  # Parse the text and generate a summary\n",
        "  parser = PlaintextParser.from_string(course_description, Tokenizer(\"english\"))\n",
        "  summarizer = LsaSummarizer()\n",
        "\n",
        "  # Estimate the number of sentences needed for a 2-minute summary\n",
        "  avg_speed = 150 # average words per minute\n",
        "  est_num_sentences = int(2 * avg_speed) # 2 minutes in words\n",
        "  summary = summarizer(parser.document, est_num_sentences)\n",
        "\n",
        " # Convert the Sentence object to a string\n",
        "  summary_str = ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        " # Split the summary into parts\n",
        "  course_parts = summary_str.split('\\n')\n",
        "\n",
        "  return summary_str,course_parts, doc"
      ],
      "metadata": {
        "id": "toH7OEjn_6AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from TTS.config import load_config\n",
        "from TTS.utils.manage import ModelManager\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "import IPython\n",
        "\n",
        "basepath=\"/content/persian-tts-female-glow_tts\"\n",
        "vbasepath=\"/content\"\n",
        "model_path =basepath+\"/best_model.pth\" # Absolute path to the model checkpoint.pth\n",
        "config_path =basepath+\"/config.json\" # Absolute path to the model config.json\n",
        "# speakers_file_path = # Absolute path to speakers.pth file\n",
        "vocoder_path=\"/content/checkpoint_378000.pth\"#vbasepath+\"/checkpoint_127000.pth\"\n",
        "vocoder_config_path=\"/content/config.json\"\n",
        "synthesizer = Synthesizer(\n",
        "        model_path,\n",
        "        config_path,\n",
        "        None ,#speakers_file_path,\n",
        "        None ,#language_ids_file_path,\n",
        "        vocoder_path ,#vocoder_path,\n",
        "        vocoder_config_path ,#vocoder_config_path,\n",
        "        None ,#encoder_path,\n",
        "        None ,#encoder_config_path,\n",
        "        None ,#args.use_cuda,\n",
        "    )\n",
        "\n",
        "# Example Usage\n",
        "course_filename = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/Title/Pitch_Deck_T_climate-change-_t.docx\" # \"path_to_your_file.docx\"\n",
        "course_design_variables = {\"filename\": course_filename}\n",
        "TOPIC = \"Your Topic\"\n",
        "\n",
        "\n",
        "# Extract course information\n",
        "summary_str,course_data, doc = extract_course_information_for_farsi_voice(course_design_variables)\n",
        "save_folder_dest = \"/content/drive/MyDrive/ChatGPT_Paper_wrting/All_Reports/Climate Change _T/Pitch_Deck/\" # \"path_to_your_file.docx\"\n",
        "text = course_data\n",
        "text=\".زندگی فقط یک بار است؛ از آن به خوبی استفاده کن\"\n",
        "#text = \"test\"\n",
        "print( text)\n",
        "wavs = synthesizer.tts(text)\n",
        "synthesizer.save_wav(wavs, save_folder_dest + 'sp.wav')\n",
        "IPython.display.Audio(save_folder_dest + 'sp.wav')\n",
        "\n",
        "if False : #for category, save_folder_dest in TOPIC_CLASS.items():\n",
        "\n",
        "  text = category[\"results\"]\n",
        "  save_folder=save_folder_dest(category[\"TOPIC\"],category[\"name\"])\n",
        "  #text=\".زندگی فقط یک بار است؛ از آن به خوبی استفاده کن\"\n",
        "\n",
        "  wavs = synthesizer.tts(text)\n",
        "  synthesizer.save_wav(wavs,save_folder_dest+ 'sp.wav')\n",
        "\n",
        "  IPython.display.Audio(save_folder_dest+'sp.wav')\n",
        "\n",
        "IPython.display.Audio(save_folder_dest + 'sp.wav')"
      ],
      "metadata": {
        "id": "3zxpTUwURNj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from TTS.config import load_config\n",
        "from TTS.utils.manage import ModelManager\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "\n",
        "config=\"config.json\"\n",
        "model=\"best_model_30824.pth\"\n",
        "\n",
        "model_path =model # Absolute path to the model checkpoint.pth\n",
        "config_path =config # Absolute path to the model config.json\n",
        "\n",
        "for category, save_folder_dest in TOPIC_CLASS.items():\n",
        "\n",
        "  text = category[\"results\"]\n",
        "  save_folder=save_folder_dest(category[\"TOPIC\"],category[\"name\"])\n",
        "  #text=\".زندگی فقط یک بار است؛ از آن به خوبی استفاده کن\"\n",
        "\n",
        "  synthesizer = Synthesizer(\n",
        "    model_path, config_path\n",
        "  )\n",
        "  wavs = synthesizer.tts(text)\n",
        "  synthesizer.save_wav(wavs,save_folder_dest+ 'Describtion.wav')\n",
        "\n",
        "  IPython.display.Audio('sp.wav')"
      ],
      "metadata": {
        "id": "qp_JB8kcKGp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import HTML\n",
        "\n",
        "def upload_files_to_transfer_sh(file_paths):\n",
        "  urls = []\n",
        "  html_content = \"<form>\"\n",
        "  for file_path in file_paths:\n",
        "      with open(file_path, 'rb') as file:\n",
        "          response = requests.post('https://transfer.sh/', files={'file': file})\n",
        "          response.raise_for_status()\n",
        "          urls.append(response.text)\n",
        "          html_content += f\"<p>File: {file_path} <br> And Upload URL is: <a href='{response.text}'>{response.text}</a></p>\"\n",
        "  html_content += \"</form>\"\n",
        "  return urls, html_content\n",
        "\n",
        "file_paths = [Sound_File, docx_path, docx_path.replace('docx','pdf')]\n",
        "urls, html_content = upload_files_to_transfer_sh(file_paths)\n",
        "for url in urls:\n",
        "  print(url)\n",
        "HTML(html_content)"
      ],
      "metadata": {
        "id": "JQVNA0T95rgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Emailing the content 👇🐢🌸"
      ],
      "metadata": {
        "id": "bIYfHcTCp9Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi-mail"
      ],
      "metadata": {
        "id": "Pku-t3QT_AsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi_mail import FastMail, MessageSchema, ConnectionConfig\n",
        "import os\n",
        "import base64\n",
        "import zipfile\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "conf = ConnectionConfig(\n",
        "  MAIL_USERNAME = \"Your_Email\",\n",
        "  MAIL_PASSWORD = \"Your_Email_Password\",\n",
        "  MAIL_FROM = \"Your_Email\",\n",
        "  MAIL_PORT = 587,\n",
        "  MAIL_SERVER = \"smtp.gmail.com\",\n",
        "  MAIL_TLS = True,\n",
        "  MAIL_SSL = False,\n",
        "  USE_CREDENTIALS = True\n",
        ")\n",
        "\n",
        "@app.post(\"/send_email_summary/\")\n",
        "async def send_email_summary(news_content_summary: str, news_data_with_text_df_1_html: str, attachments: list):\n",
        "  # Create a zip file from the attachments\n",
        "  with zipfile.ZipFile('attachments.zip', 'w') as zipf:\n",
        "      for attachment in attachments:\n",
        "          zipf.write(attachment)\n",
        "\n",
        "  # Prepare attachments\n",
        "  with open('attachments.zip', 'rb') as f:\n",
        "      content = f.read()\n",
        "      encoded_content = base64.b64encode(content).decode()\n",
        "      attachments_data = [\n",
        "          {\n",
        "              'ContentType': 'application/zip',\n",
        "              'Filename': 'attachments.zip',\n",
        "              'Base64Content': encoded_content\n",
        "          }\n",
        "      ]\n",
        "\n",
        "  message = MessageSchema(\n",
        "      subject=\"GPT News Summary of Today\",\n",
        "      recipients=[\"Your_Email\"],\n",
        "      body=\"<h3>Here is the result of our code for today.</h3>{}<br><br> \\\n",
        "            <h3>GPT Gent Summary Sources</h3>{}<br><br> \\\n",
        "            <h4> The PDF,DICX and MP3 file has been added at attachment</h4>\".format(news_content_summary, news_data_with_text_df_1_html),\n",
        "      attachments=attachments_data\n",
        "  )\n",
        "\n",
        "  fm = FastMail(conf)\n",
        "  await fm.send_message(message)\n",
        "  return {\"message\": \"Email Sent\"}"
      ],
      "metadata": {
        "id": "n_r7b-Ud_FKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perviuse_content=''\n",
        "news_content_summary = 'Summary of the news'\n",
        "news_data_with_text_df_1_html = 'HTML table of news sources'\n",
        "attachments = [Sound_File, docx_path, docx_path.replace('docx','pdf')]\n",
        "\n",
        "send_email_summary(news_content_summary, news_data_with_text_df_1_html, attachments)"
      ],
      "metadata": {
        "id": "i4QMJkj8AF6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install mailjet_rest\n",
        "\n",
        "\n",
        "from mailjet_rest import Client\n",
        "import os\n",
        "import base64\n",
        "import zipfile\n",
        "\n",
        "def send_email_summary(api_key, api_secret, news_content_summary, news_data_with_text_df_1_html , attachments):\n",
        "  mailjet = Client(auth=(api_key, api_secret), version='v3.1')\n",
        "\n",
        "  # Create a zip file from the attachments\n",
        "  with zipfile.ZipFile('attachments.zip', 'w') as zipf:\n",
        "      for attachment in attachments:\n",
        "          zipf.write(attachment)\n",
        "\n",
        "  # Prepare attachments\n",
        "  attachments_data = []\n",
        "  with open('attachments.zip', 'rb') as f:\n",
        "      content = f.read()\n",
        "      encoded_content = base64.b64encode(content).decode()\n",
        "      attachments_data.append({\n",
        "          'ContentType': 'application/zip',\n",
        "          'Filename': 'attachments.zip',\n",
        "          'Base64Content': encoded_content\n",
        "      })\n",
        "\n",
        "  data = {\n",
        "    'Messages': [\n",
        "      {\n",
        "        \"From\": {\n",
        "          \"Email\": \"easonlai888@gmail.com\",\n",
        "          \"Name\": \"Eason\"\n",
        "        },\n",
        "        \"To\": [\n",
        "          {\n",
        "            \"Email\": \"Your_Email\",\n",
        "            \"Name\": \"Eason\"\n",
        "          }\n",
        "        ],\n",
        "        \"Subject\": \"GPT News Summary of Today\",\n",
        "        \"HTMLPart\": \"<h3>Here is the result of our code for today.</h3>{}<br><br> \\\n",
        "                  <h3>GPT Gent Summary Sources</h3>{}<br><br> \\\n",
        "                  <h4> The PDF,DICX and MP3 file has been added at attachment</h4>\",#.format(news_content_summary, news_data_with_text_df_1_html),\n",
        "        \"Attachments\": attachments_data\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "  result = mailjet.send.create(data=data)\n",
        "  print(result.status_code)\n",
        "  print(result.json())"
      ],
      "metadata": {
        "id": "lmNpBP9vrrSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = openai_api\n",
        "api_secret = 'PLEASE_ENTER_YOUR_OWNED_MAILJET_API_KEY_SECRET'\n",
        "perviuse_content=''\n",
        "news_content_summary = 'Summary of the news'\n",
        "news_data_with_text_df_1_html = 'HTML table of news sources'\n",
        "attachments = [Sound_File, docx_path, docx_path.replace('docx','pdf')]\n",
        "\n",
        "send_email_summary(api_key, api_secret, perviuse_content, news_data_with_text_df_1_html, attachments)"
      ],
      "metadata": {
        "id": "OZMmRxJyr1QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Only update the Prompt by the help of ChatGPT:👇👇"
      ],
      "metadata": {
        "id": "BAiQz23MlpUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define main variables\n",
        "#TOPIC = \"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"\n",
        "\n",
        "# List of variables\n",
        "variables = [\"RESEARCH_DOMAIN\", \"PARAGRAPH\", \"PARAGRAPHS\", \"TOPIC_SENTENCE\", \"LANGUAGE\", \"ABSTRACT_PARAGRAPH\", \"BIBLIOGRAPHY\", \"THEORY1\", \"THEORY2\", \"RESEARCH_QUESTIONS\", \"ACTION\", \"RESULT_PARAGRAPHS\", \"DATE\", \"NUMBER_OF_DAYS_MONTHS_YEARS\"]\n",
        "\n",
        "# Generate content for each variable\n",
        "variable_contents = {}\n",
        "for prompt in prompts:\n",
        "   for var in variables:\n",
        "       if var in prompt:\n",
        "           prompt = f\"Generate content for the variable '{var}' based on the topic '{TOPIC}'. For this prompt '{prompt}'\"\n",
        "           content= generate_academic_paper(prompt)\n",
        "           for choice in content.choices:\n",
        "               print(choice.text)\n",
        "               variable_contents[var] = choice.text\n",
        "           print(\"variable_contents[var] is: \",variable_contents[var])\n",
        "\n",
        "# Replace the variables in the prompts with the generated content\n",
        "for prompt in prompts:\n",
        "   for var, content in variable_contents.items():\n",
        "       prompt = prompt.replace(f\"{{{var}}}\", content)\n",
        "print(\"Updated Prompt:\", prompt)"
      ],
      "metadata": {
        "id": "kH3LygdiXsPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_update_a3(prompt, previous_content):\n",
        "    model_engine = \"text-davinci-003\"\n",
        "    max_tokens = 2048\n",
        "\n",
        "    # Construct the instruction for updating the prompt\n",
        "    instruction = f\"Given the previous content:\\n\\n{previous_content}\\n\\nUpdate the prompt: {prompt}\"\n",
        "\n",
        "    # Create the completion with the instruction\n",
        "    completion = client.completions.create(\n",
        "        model=model_engine,\n",
        "        prompt=instruction,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3,\n",
        "        n=1,\n",
        "        frequency_penalty=0,\n",
        "    )\n",
        "\n",
        "    # Extract and return the updated prompt from the response\n",
        "    updated_prompt = completion.choices[0].text.strip()\n",
        "    return updated_prompt\n",
        "# Inside your loop\n",
        "for prompt in prompts:\n",
        "    print(\"Original Prompt:\", prompt)\n",
        "    response = generate_academic_paper_a2(prompt)\n",
        "    previous_content = response.choices[0].text.strip()\n",
        "    updated_prompt = generate_prompt_update_2(prompt, previous_content)\n",
        "    print(\"Updated Prompt:\", updated_prompt)\n",
        "\n",
        "    # Continue with the rest of your processing\n",
        "    # ..."
      ],
      "metadata": {
        "id": "KviSdMwzwqAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with no translation for become faster answer:👇👇🙏"
      ],
      "metadata": {
        "id": "iuTPp4YYBg2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries and set up the API key\n",
        "#!pip install openai\n",
        "import json\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"  #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "#TOPIC = f\"Understanding the Cycle of Domestic Violence against iranian women: The Influence of Economic Power\"\n",
        "#topic = f\"The Importance of Focusing on Waste Collection in a Waste-Filled World\"\n",
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key = openai_api#\"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\",\n",
        ")\n",
        "#openai.api_key = os.getenv('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')\n",
        "# Define the prompts\n",
        "prompts3 = [\n",
        "f\"Find a research topic for a PhD in the area of '{topic}'\",\n",
        "f\"Write a detailed proposal on the following research '{topic}'. Make Sure it is free from plagiarism. \",\n",
        "f\"Identify gaps in the literature on '{topic}'\",\n",
        "\"Generate 10 academic research questions about Perviuse action\",\n",
        "f\"Generate a list of research hypotheses related to '{topic}'\"\n",
        "]\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "# Create a Translator object\n",
        "translator = Translator()\n",
        "\n",
        "\n",
        "# Function to generate an academic paper\n",
        "\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=20, max_time=600)\n",
        "def generate_academic_paper_a2(prompt_my):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "\n",
        "   completion = client.completions.create(\n",
        "       model=\"gpt-3.5-turbo-instruct\",\n",
        "       prompt=prompt_my,\n",
        "       max_tokens=2048,\n",
        "       temperature=0.3,\n",
        "       n=1,\n",
        "       frequency_penalty=0,\n",
        "   )\n",
        "   return completion\n",
        "\n",
        "choice_text_all=[]\n",
        "\n",
        "import time\n",
        "# Function to generate academic papers for given prompts\n",
        "# Function to generate academic papers for given prompts\n",
        "def generate_papers(prompts):\n",
        "   for i in range(0, len(prompts), 20):\n",
        "       # Slice the prompts list to get the next 20 prompts\n",
        "       print(\"I is \",i,\" Len of prompt Is:\", len(prompts))\n",
        "\n",
        "       batch = prompts[i:i+20]\n",
        "       print(\"batch is \",batch)\n",
        "   #for j in range ( 0, Len(batch)):\n",
        "       # Generate papers for the next 20 prompts\n",
        "       for prompt in batch:\n",
        "      #   for prompt in prompt1:\n",
        "           # Print the prompt\n",
        "           print(\"prompt is \", prompt)\n",
        "           response = generate_academic_paper_a2(prompt)\n",
        "           print(\"\\nGenerated Academic Paper:\")\n",
        "           print(\"========================\\n\")\n",
        "           #print(response)\n",
        "           for choice in response.choices:\n",
        "             print(choice.text)\n",
        "             choice_text_all.append(choice.text)\n",
        "\n",
        "             # Translate the generated text to Persian\n",
        "         #  choice_translated = translate_to_persian(choice.text) #translator.translate(choice.text, dest='fa')\n",
        "         # choice_translated_prompt = translate_to_persian(prompt)#translator.translate(prompt, dest='fa')\n",
        "\n",
        "         #    print(\"\\n-----\\n Translated is \",choice_translated)\n",
        "         #    save_academic_paper(\"Persian_\"+topic,'\\n**'+choice_translated_prompt+'**\\n'+choice_translated)\n",
        "             save_academic_paper(topic,'\\n--------**\\n'+prompt+'/n-------**\\n'+choice.text)\n",
        "             # Recursively call the function for the nested Completion objects\n",
        "             if hasattr(choice, 'choices'):\n",
        "               extract_text(choice)\n",
        "\n",
        "           #generated_text = response.choices[0].text\n",
        "           # Print the generated text\n",
        "           #print(generated_text)\n",
        "           print(\"\\n end of loop\")\n",
        "           print(\"========================\\n\")\n",
        "\n",
        "           #print(\"loop\")\n",
        "       # Wait for a short period of time before sending the next batch of prompts\n",
        "           time.sleep(40)\n",
        "   return choice_text_all#,choice.translated\n",
        "\n",
        "# Generate academic papers for the given prompts\n",
        "generate_papers(prompts)"
      ],
      "metadata": {
        "id": "rl16DSYABo9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP COLAB for run Streamlit\n",
        "!npm install localtunnel\n",
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "GRn7OkOUjcdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL Translate\n",
        "A deep learning-based translation library built on Huggingface transformers and Facebook's mBART-Large\n",
        "\n",
        "https://colab.research.google.com/github/xhluca/dl-translate/blob/main/demos/colab_demo.ipynb#scrollTo=qdefSjR_YIiG"
      ],
      "metadata": {
        "id": "_I5yn6CCZdMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q dl-translate"
      ],
      "metadata": {
        "id": "c7-q341UWsr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dl_translate as dlt\n",
        "mt = dlt.TranslationModel('mbart50')"
      ],
      "metadata": {
        "id": "gtQcnXabalaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dl_translate as dlt\n",
        "\n",
        "def translate_to_persian(text):\n",
        "  # Initialize the translation model\n",
        "  #mt = dlt.TranslationModel('mbart50')\n",
        "\n",
        "  # Translate the text\n",
        "  translated = mt.translate(text, source=dlt.lang.ENGLISH, target=dlt.lang.PERSIAN)\n",
        "\n",
        "  return translated"
      ],
      "metadata": {
        "id": "iOX6FGFWaFqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries and set up the API key\n",
        "#!pip install openai\n",
        "import json\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"  #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "#topic = f\"The Importance of Focusing on Waste Collection in a Waste-Filled World\"\n",
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key = openai_api#\"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\",\n",
        ")\n",
        "#openai.api_key = os.getenv('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')\n",
        "# Define the prompts\n",
        "prompts3 = [\n",
        "f\"Find a research topic for a PhD in the area of '{topic}'\",\n",
        "f\"Write a detailed proposal on the following research '{topic}'. Make Sure it is free from plagiarism. \",\n",
        "f\"Identify gaps in the literature on '{topic}'\",\n",
        "\"Generate 10 academic research questions about Perviuse action\",\n",
        "f\"Generate a list of research hypotheses related to '{topic}'\"\n",
        "]\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "# Create a Translator object\n",
        "translator = Translator()\n",
        "\n",
        "\n",
        "# Function to generate an academic paper\n",
        "\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=10, max_time=600)\n",
        "def generate_academic_paper_5(prompt_my):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "\n",
        "   completion = client.completions.create(\n",
        "       model=\"gpt-3.5-turbo-instruct\",\n",
        "       prompt=prompt_my,\n",
        "       max_tokens=2048,\n",
        "       temperature=0.3,\n",
        "       n=1,\n",
        "       frequency_penalty=0,\n",
        "   )\n",
        "   return completion\n",
        "\n",
        "choice_text_all=[]\n",
        "\n",
        "import time\n",
        "# Function to generate academic papers for given prompts\n",
        "# Function to generate academic papers for given prompts\n",
        "def generate_papers(prompts):\n",
        "   for i in range(0, len(prompts), 20):\n",
        "       # Slice the prompts list to get the next 20 prompts\n",
        "       print(\"I is \",i,\" Len of prompt Is:\", len(prompts))\n",
        "       batch = prompts[i:i+10]\n",
        "       print(\"batch is \",batch)\n",
        "       # Generate papers for the next 20 prompts\n",
        "       for prompt in batch:\n",
        "      #   for prompt in prompt1:\n",
        "           # Print the prompt\n",
        "           print(\"prompt is \", prompt)\n",
        "           response = generate_academic_paper(prompt)\n",
        "           print(\"\\nGenerated Academic Paper:\")\n",
        "           print(\"========================\\n\")\n",
        "           #print(response)\n",
        "           for choice in response.choices:\n",
        "             print(choice.text)\n",
        "             choice_text_all.append(choice.text)\n",
        "              # Translate the generated text to Persian\n",
        "             choice_translated = translate_to_persian(choice.text) #translator.translate(choice.text, dest='fa')\n",
        "             choice_translated_prompt = translate_to_persian(prompt)#translator.translate(prompt, dest='fa')\n",
        "\n",
        "             print(\"\\n-----\\n Translated is \",choice_translated)\n",
        "             save_academic_paper(\"Persian_\"+topic,'\\n**'+choice_translated_prompt+'**\\n'+choice_translated)\n",
        "             save_academic_paper(topic,'\\n**'+prompt+'**\\n'+choice.text)\n",
        "             # Recursively call the function for the nested Completion objects\n",
        "             if hasattr(choice, 'choices'):\n",
        "               extract_text(choice)\n",
        "\n",
        "           #generated_text = response.choices[0].text\n",
        "           # Print the generated text\n",
        "           #print(generated_text)\n",
        "           print(\"\\n end of loop\")\n",
        "           print(\"========================\\n\")\n",
        "\n",
        "           #print(\"loop\")\n",
        "       # Wait for a short period of time before sending the next batch of prompts\n",
        "           time.sleep(2)\n",
        "   return choice_text_all,choice.translated\n",
        "\n",
        "# Generate academic papers for the given prompts\n",
        "generate_papers(prompts3)"
      ],
      "metadata": {
        "id": "oVldhSpgEVZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For solving queta prompt of Openai API;\n",
        "👇👇🌱"
      ],
      "metadata": {
        "id": "GdTvLhAkKLEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "#import openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key = openai_api#\"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\",\n",
        ")\n",
        "\n",
        "def retry_with_exponential_backoff(\n",
        "   func,\n",
        "   initial_delay: float = 1,\n",
        "   exponential_base: float = 2,\n",
        "   jitter: bool = True,\n",
        "   max_retries: int = 10,\n",
        "   errors: tuple = (openai.error.RateLimitError,),\n",
        "):\n",
        "   \"\"\"Retry a function with exponential backoff.\"\"\"\n",
        "   def wrapper(*args, **kwargs):\n",
        "       # Initialize variables\n",
        "       num_retries = 0\n",
        "       delay = initial_delay\n",
        "       # Loop until a successful response or max_retries is hit or an exception is raised\n",
        "       while True:\n",
        "           try:\n",
        "               return func(*args, **kwargs)\n",
        "           # Retry on specified errors\n",
        "           except errors as e:\n",
        "               # Increment retries\n",
        "               num_retries += 1\n",
        "               # Check if max retries has been reached\n",
        "               if num_retries > max_retries:\n",
        "                  raise Exception(\n",
        "                      f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
        "                  )\n",
        "               # Increment the delay\n",
        "               delay *= exponential_base * (1 + jitter * random.random())\n",
        "               # Sleep for the delay\n",
        "               time.sleep(delay)\n",
        "           # Raise exceptions for any errors not specified\n",
        "           except Exception as e:\n",
        "               raise e\n",
        "   return wrapper\n",
        "\n",
        "@retry_with_exponential_backoff\n",
        "def generate_academic_paper_0(prompt_my):\n",
        "  model_engine = \"text-davinci-003\"\n",
        "  max_tokens = 2048\n",
        "\n",
        "  completion = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt_my,\n",
        "      max_tokens=2048,\n",
        "      temperature=0.3,\n",
        "      n=1,\n",
        "      frequency_penalty=0,\n",
        "  )\n",
        "  return completion"
      ],
      "metadata": {
        "id": "zLWA8i87KXks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH0Z7EL5LnQc"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries and set up the API key\n",
        "#!pip install openai\n",
        "import json\n",
        "#TOPIC =f\"strategies for increase the compassion in iranian Women movment and ist relationship with the fatigue of compassion in light triad personality\"  #f\" the dark triad in psychology and it's relation with {goal}\"\n",
        "topic = f\"The Importance of Focusing on Waste Collection in a Waste-Filled World\"\n",
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "openai.api_key = openai_api#\"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key= openai_api#\"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\",\n",
        ")\n",
        "#openai.api_key = os.getenv('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')\n",
        "# Define the prompts\n",
        "prompts3 = [\n",
        "f\"Find a research topic for a PhD in the area of '{topic}'\",\n",
        "f\"Write a detailed proposal on the following research '{topic}'. Make Sure it is free from plagiarism. \",\n",
        "f\"Identify gaps in the literature on '{topic}'\",\n",
        "\"Generate 10 academic research questions about Perviuse action\",\n",
        "f\"Generate a list of research hypotheses related to '{topic}'\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Function to generate an academic paper\n",
        "\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "\n",
        "@on_exception(expo, RateLimitError, max_tries=10, max_time=600)\n",
        "def generate_academic_paper_6(prompt_my):\n",
        "   model_engine = \"text-davinci-003\"\n",
        "   max_tokens = 2048\n",
        "\n",
        "   completion = client.completions.create(\n",
        "       model=\"gpt-3.5-turbo-instruct\",\n",
        "       prompt=prompt_my,\n",
        "       max_tokens=2048,\n",
        "       temperature=0.3,\n",
        "       n=1,\n",
        "       frequency_penalty=0,\n",
        "   )\n",
        "   return completion\n",
        "\n",
        "choice_text_all=[]\n",
        "\n",
        "import time\n",
        "# Function to generate academic papers for given prompts\n",
        "# Function to generate academic papers for given prompts\n",
        "def generate_papers(prompts):\n",
        "   for i in range(0, len(prompts), 20):\n",
        "       # Slice the prompts list to get the next 20 prompts\n",
        "       print(\"I is \",i,\" Len of prompt Is:\", len(prompts))\n",
        "       batch = prompts[i:i+10]\n",
        "       print(\"batch is \",batch)\n",
        "       # Generate papers for the next 20 prompts\n",
        "       for prompt in batch:\n",
        "      #   for prompt in prompt1:\n",
        "           # Print the prompt\n",
        "           print(\"prompt is \", prompt)\n",
        "           response = generate_academic_paper(prompt)\n",
        "           print(\"\\nGenerated Academic Paper:\")\n",
        "           print(\"========================\\n\")\n",
        "           #print(response)\n",
        "           for choice in response.choices:\n",
        "             print(choice.text)\n",
        "             choice_text_all.append(choice.text)\n",
        "             save_academic_paper(TOPIC,'\\n**'+prompt+'**\\n'+choice.text)\n",
        "             # Recursively call the function for the nested Completion objects\n",
        "             if hasattr(choice, 'choices'):\n",
        "               extract_text(choice)\n",
        "\n",
        "           #generated_text = response.choices[0].text\n",
        "           # Print the generated text\n",
        "           #print(generated_text)\n",
        "           print(\"\\n end of loop\")\n",
        "           print(\"========================\\n\")\n",
        "\n",
        "           #print(\"loop\")\n",
        "       # Wait for a short period of time before sending the next batch of prompts\n",
        "           time.sleep(2)\n",
        "   return choice.text_all\n",
        "\n",
        "# Generate academic papers for the given prompts\n",
        "generate_papers(prompts3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJTgcojthnm7"
      },
      "outputs": [],
      "source": [
        "def render_index_page():\n",
        "   html_content = \"\"\"\n",
        "   <!DOCTYPE html>\n",
        "   <html>\n",
        "   <head>\n",
        "      <title>Generate Academic Papers</title>\n",
        "   </head>\n",
        "   <body>\n",
        "      <h1>Generate Academic Papers</h1>\n",
        "      <form method=\"POST\">\n",
        "          <label for=\"topic\">Topic:</label>\n",
        "          <input type=\"text\" id=\"topic\" name=\"topic\" required>\n",
        "          <input type=\"submit\" value=\"Generate\">\n",
        "      </form>\n",
        "   </body>\n",
        "   </html>\n",
        "   \"\"\"\n",
        "   return render_template_string(html_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKhSXALkhpL7"
      },
      "outputs": [],
      "source": [
        "def render_result_page(topic, docx_file, pdf_file, choice_text_all):\n",
        "   html_content = f\"\"\"\n",
        "   <!DOCTYPE html>\n",
        "   <html>\n",
        "   <head>\n",
        "       <title>Result</title>\n",
        "   </head>\n",
        "   <body>\n",
        "       <h1>Result</h1>\n",
        "       <p>Academic papers have been generated for the topic: {topic}</p>\n",
        "       <a href=\"/download/{docx_file}\">Download DOCX</a>\n",
        "       <a href=\"/download/{pdf_file}\">Download PDF</a>\n",
        "       <h2>Generated Text:</h2>\n",
        "       <p>{choice_text_all}</p>\n",
        "   </body>\n",
        "   </html>\n",
        "   \"\"\"\n",
        "   return render_template_string(html_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuTJlSSGgh4E"
      },
      "outputs": [],
      "source": [
        "# app.py\n",
        "\n",
        "from flask import Flask, request, render_template\n",
        "from academic_paper_generator import generate_papers\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "  if request.method == 'POST':\n",
        "      topic = request.form.get('topic')\n",
        "      prompts3 = [\n",
        "          f\"Find a research topic for a PhD in the area of '{topic}'\",\n",
        "          f\"Write a detailed proposal on the following research '{topic}'. Make Sure it is free from plagiarism. \",\n",
        "          f\"Identify gaps in the literature on '{topic}'\",\n",
        "          \"Generate 10 academic research questions about Perviuse action\",\n",
        "          f\"Generate a list of research hypotheses related to '{topic}'\"\n",
        "      ]\n",
        "      generate_papers(prompts3)\n",
        "      return render_template('result.html', topic=topic)\n",
        "  else:\n",
        "      return render_template('index.html')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  app.run(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making prompt for Docx documents:👇👇🌹🙏🌸"
      ],
      "metadata": {
        "id": "1FkLc6wN9My7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "from docx import Document\n",
        "from ipywidgets import Text, Textarea\n",
        "\n",
        "# Function to extract text from a Word document\n",
        "def read_word_document(file_path):\n",
        "   doc = Document(file_path)\n",
        "   text_content = []\n",
        "   for paragraph in doc.paragraphs:\n",
        "       text_content.append(paragraph.text)\n",
        "   return text_content\n",
        "\n",
        "\n",
        "# Input parameters\n",
        "course_designer = Text(value='chatgpt cource designer', description='Course Designer:')\n",
        "topic = Text(value='Civil Conflict Resolution', description='TOPIC:')\n",
        "paragraph = Textarea(value='Provide a brief description or guideline for the course content.', description='Paragraph:')\n",
        "\n",
        "# Display the input form\n",
        "display(course_designer, topic, paragraph)\n",
        "\n",
        "# Read content from the uploaded Word document\n",
        "document_content = read_word_document(uploaded_file_name)\n",
        "\n",
        "\n",
        "# Upload Word document\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Get the first uploaded file name\n",
        "uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "# Create prompts_var with the specified structure\n",
        "prompts_var = [\n",
        "   f\"As {course_designer.value} {line} for the This TOPIC: ({topic.value}) and this guideline description: ({paragraph.value}).\"\n",
        "   for line in document_content\n",
        "]\n",
        "\n",
        "# Print the generated prompts\n",
        "for prompt in prompts_var:\n",
        "   print(prompt)"
      ],
      "metadata": {
        "id": "N00QrPZH9Oir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming course_designer, TOPIC, PARAGRAPH are defined\n",
        "course_designer = \"ChatGPT Course Designer\"\n",
        "TOPIC = \"Civil Conflict Resolution and Transformative Scenario Planning for the Iranian Opposition\"\n",
        "PARAGRAPH = \"'Civil Conflict Resolution and Transformative Scenario Planning for the Iranian Opposition' is a comprehensive course designed to teach learners about conflict resolution and transformative planning within the Iranian political landscape.\"\n",
        "\n",
        "prompts_manual_Title = [\n",
        "  \"suggest one Course Title in less than 15 words\",\n",
        "  \"Civil Conflict Resolution and Transformative Scenario Planning for the Iranian Opposition\",\n",
        "  \"Section 1: Understanding Conflict Dynamics\",\n",
        "  \"1.1 Introduction to Conflict Resolution\",\n",
        "  \"1.2 Understanding the Iranian Political Landscape\",\n",
        "  \"Conflict Resolution Approaches\",\n",
        "  \"Section 2: Communication and Dialogue\",\n",
        "  \"2.1 Effective Communication Skills\",\n",
        "  \"2.2 Constructive Dialogue Techniques\",\n",
        "  \"2.3 Cultural Sensitivity in Communication\",\n",
        "  \"Section 3: Mediation and Negotiation\",\n",
        "  \"3.1 Mediation Process\",\n",
        "  \"3.2 Negotiation Strategies\",\n",
        "  \"3.3 Case Studies in Mediation and Negotiation\",\n",
        "  \"Section 4: Advocacy and Civil Engagement\",\n",
        "  \"4.1 Advocacy for Conflict Resolution\",\n",
        "  \"4.2 Leveraging International Support\",\n",
        "  \"4.3 Community Engagement\",\n",
        "  \"Section 5: Ethical Considerations\",\n",
        "  \"5.1 Ethics in Conflict Resolution\",\n",
        "  \"5.2 Navigating Legal Frameworks\",\n",
        "  \"Section 6: Building Resilience and Sustainable Peace\",\n",
        "  \"6.1 Psychosocial Support\",\n",
        "  \"6.2 Sustainable Peacebuilding\",\n",
        "  \"Section 7: Transformative Scenario Planning\",\n",
        "  \"7.1 Introduction to Transformative Scenario Planning\",\n",
        "  \"7.2 Understanding the Iranian Political Landscape for Scenario Planning\",\n",
        "  \"7.3 Analysis of Current Conflict Dynamics for Scenario Planning\",\n",
        "  \"Section 8: Tools and Techniques of Transformative Scenario Planning\",\n",
        "  \"8.1 Scenario Building Techniques\",\n",
        "  \"8.2 Systems Thinking and Complexity in Conflict\",\n",
        "  \"8.3 Data Collection and Analysis for Scenario Planning\",\n",
        "  \"Section 9: Visioning and Goal Setting for Scenario Planning\",\n",
        "  \"9.1 Envisioning a Desired Future\",\n",
        "  \"9.2 Setting Transformative Goals for Scenario Planning\",\n",
        "  \"Section 10: Stakeholder Engagement and Collaboration for Scenario Planning\",\n",
        "  \"10.1 Identifying and Mapping Stakeholders for Scenario Planning\",\n",
        "  \"10.2 Facilitating Collaborative Processes for Scenario Planning\",\n",
        "  \"Section 11: Scenario Testing and Rehearsal\",\n",
        "  \"11.1 Testing Scenarios Through Simulation\",\n",
        "  \"11.2 Adaptive Strategy Development for Scenario Planning\",\n",
        "  \"Section 12: Communication and Advocacy for Scenario Planning\",\n",
        "  \"12.1 Communicating Transformative Visions for Scenario Planning\",\n",
        "  \"12.2 Advocacy for Conflict Transformation in Scenario Planning\",\n",
        "  \"Section 13: Implementation and Monitoring\",\n",
        "  \"13.1 Action Planning and Implementation for Scenario Planning\",\n",
        "  \"13.2 Monitoring and Adaptive Management for Scenario Planning\",\n",
        "  \"Section 14: Reflection and Future Directions\",\n",
        "  \"14.1 Course Reflection and Evaluation\",\n",
        "  \"14.2 Developing a Sustainable Path Forward\",\n",
        "  \"Assessment:\"\n",
        "]\n",
        "\n",
        "prompts_manual = [\n",
        "f\"As {course_designer}, you are asked to creat this part of course designing as ('{line}') for the topic: ({TOPIC}) and this guideline description: ({PARAGRAPH}).\"\n",
        "for line in prompts_word\n",
        "]\n",
        "prompts_manual[0] = f\" As ChatGPT expert in the role of course designing as {role}, suggest one Course Title in less than 15 words, based on This Topic :({TOPIC}) and the description:({PARAGRAPH}).\"\n",
        "\n",
        "\n",
        "# Update the last line for the assessment part\n",
        "assessment_items = [\n",
        "  \"Active participation in discussions and activities\",\n",
        "  \"Case study analyses\",\n",
        "  \"Mediation and negotiation simulations\",\n",
        "  \"Advocacy and community engagement project\",\n",
        "  \"Scenario-building exercises\",\n",
        "  \"Stakeholder engagement and collaboration project\",\n",
        "  \"Action plan development and presentation\",\n",
        "  \"Final reflection and vision for the future\"\n",
        "]\n",
        "\n",
        "last_index = len(prompts_manual) - 1\n",
        "prompts_manual[-1] = f\"As {course_designer}, you are asked to assess {', '.join(assessment_items)} for the topic: ({TOPIC}) and this guideline description: ({PARAGRAPH}).\"\n",
        "\n",
        "# Print all the lines\n",
        "for i, prompt in enumerate(prompts_manual, start=1):\n",
        "   print(f\"Line {i}: {prompt}\")"
      ],
      "metadata": {
        "id": "RoD_spsH9Qn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import openai\n",
        "from openai import RateLimitError\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "#openai.api_key = \"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\"# \"your_openai_api_key\"\n",
        "\n",
        "client = OpenAI(\n",
        "   # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "   api_key = openai_api# \"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\",\n",
        ")\n",
        "def retry_with_exponential_backoff_1(\n",
        "   func,\n",
        "   initial_delay: float = 1,\n",
        "   exponential_base: float = 2,\n",
        "   jitter: bool = True,\n",
        "   max_retries: int = 5,\n",
        "   errors: tuple = (RateLimitError,),\n",
        "):\n",
        "   \"\"\"Retry a function with exponential backoff.\"\"\"\n",
        "   def wrapper(*args, **kwargs):\n",
        "       # Initialize variables\n",
        "       num_retries = 0\n",
        "       delay = initial_delay\n",
        "       # Loop until a successful response or max_retries is hit or an exception is raised\n",
        "       while True:\n",
        "           try:\n",
        "               return func(*args, **kwargs)\n",
        "           # Retry on specified errors\n",
        "           except errors as e:\n",
        "               # Increment retries\n",
        "               num_retries += 1\n",
        "               # Check if max retries has been reached\n",
        "               if num_retries > max_retries:\n",
        "                  raise Exception(\n",
        "                      f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
        "                  )\n",
        "               # Increment the delay\n",
        "               delay *= exponential_base * (1 + jitter * random.random())\n",
        "               # Sleep for the delay\n",
        "               time.sleep(delay)\n",
        "           # Raise exceptions for any errors not specified\n",
        "           except Exception as e:\n",
        "               raise e\n",
        "   return wrapper\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo"
      ],
      "metadata": {
        "id": "feLJB1v99ppa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "from docx import Document\n",
        "\n",
        "# Function to extract text from a Word document\n",
        "def read_word_document(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text_content = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text_content.append(paragraph.text)\n",
        "    return text_content\n",
        "\n",
        "# Upload Word document\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Get the first uploaded file name\n",
        "uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "\n",
        "# Read content from the uploaded Word document\n",
        "document_content = read_word_document(uploaded_file_name)\n",
        "\n",
        "# Replace 'Your Name' with the actual course designer's name\n",
        "course_designer = \"Your Name\"\n",
        "\n",
        "# Create prompts_var with the specified structure\n",
        "prompts_var = [\n",
        "    f\"As {course_designer} {line} for the This TOPIC: ({TOPIC}) and this guideline description: ({PARAGRAPH}).\"\n",
        "    for line in document_content\n",
        "]\n",
        "\n",
        "# Print the generated prompts\n",
        "for prompt in prompts_var:\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "rBC3R1EU9mga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from docx import Document\n",
        "from newspaper import Article\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "def generate_question_gpt2(content, variable, question_template, model, tokenizer, max_response_length=None):\n",
        "    # Generate a question for a specific variable\n",
        "    #prompt = f\"{question_template}\\nContent {content}\"\n",
        "    prompt = f\"Answer the this question :{question_template} For the {variable} with this content:\\n Content: {content}\"\n",
        "    print ( \"\\n----New The Prompt is :\" , prompt )\n",
        "    #content = content.replace(\"\\n\", '')\n",
        "\n",
        "    # Ensure attention_mask is set during tokenization\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Set max_length to the provided value or use None if not provided\n",
        "    if max_response_length is not None:\n",
        "        max_length = min(max_response_length, model.config.max_position_embeddings)\n",
        "    else:\n",
        "        max_length = None\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    print(f\"GPT2####Generated question for {variable}: {generated_text}####\")\n",
        "    return generated_text\n",
        "\n",
        "def generate_question(content,variable, question_template, model, tokenizer):\n",
        "    # Generate a question for a specific variable\n",
        "    prompt = f\"{question_template}\\nContent: {content}\"\n",
        "\n",
        "    # Ensure attention_mask is set during tokenization\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=100, temperature=0.7, num_return_sequences=1, attention_mask=attention_mask, pad_token_id=model.config.eos_token_id)\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Extract the generated text without the prompt\n",
        "    generated_text_without_prompt = generated_text.split('\\n', 1)[1]  # Extract everything after the first newline character\n",
        "\n",
        "    return generated_text_without_prompt\n",
        "def generate_questions(content, dvar_questions, model, tokenizer, topic, description):\n",
        "    # Generate questions for each variable\n",
        "    questions = {}\n",
        "    for variable, question_template in dvar_questions.items():\n",
        "        # Exclude topic and description from the content\n",
        "        #content_without_topic_desc = content.replace(f\"Topic: {topic}\", \"\").replace(f\"Description: {description}\\n\", \"\")\n",
        "        question = generate_question_gpt2(content, variable, question_template, model, tokenizer,1024)\n",
        "\n",
        "        # Replace the topic and description in the generated answer\n",
        "        question = question.replace(f\"Content: \",''). replace (f\"Topic: {topic}\\n\", \"\").replace(f\"Description: \",\"\").replace(f\"{description}\", \"\")\n",
        "        question = question.replace(topic, \"\").replace(description, \"\")\n",
        "\n",
        "        questions[variable] = question\n",
        "        print(f\"Generated question for {variable}: {question}\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "def main():\n",
        "    # Set the values for the topic and description directly in the script\n",
        "    topic = \"Addressing Iran's Crises: Exploring Social Change Methodologies and Creating a Counter Profile\"\n",
        "    description = \"Iran's societal challenges are complex and require immediate attention. Engaging in research and working on solutions in this field can help address these issues. This course aims to equip students with the necessary tools and knowledge to navigate these challenges.\"\n",
        "\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model_name = \"distilgpt2\"  # You can replace this with other models available on Hugging Face\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Set pad_token to EOS token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Define your variable questions\n",
        "    dvar_questions_1 = {\n",
        "        \"TOPIC\": \"What is the main topic or focus of the content?\",\n",
        "        \"RESEARCH_DOMAIN\": \"Which field or domain does the research focus on?\",\n",
        "        # Add more variables with question templates as needed\n",
        "    }\n",
        "\n",
        "    # Combine topic and description into content\n",
        "    content = f\"{topic}\\n{description}\"\n",
        "\n",
        "    # Generate questions and fill variable values\n",
        "    filled_variables = generate_questions(content, dvar_questions, model, tokenizer, topic, description)\n",
        "    print(\"Filled Variables:\", filled_variables)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "G5PiBi_A9ruT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from docx import Document\n",
        "from newspaper import Article\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import requests\n",
        "\n",
        "from docx import Document\n",
        "from newspaper import Article\n",
        "from openai import RateLimitError\n",
        "from backoff import on_exception, expo\n",
        "from docx import Document\n",
        "\n",
        "from langdetect import detect\n",
        "import time ,langdetect\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def sumerizing_post(content):\n",
        "\n",
        "\n",
        "   # Detect the language of the content\n",
        "   try:\n",
        "      language = detect(content)\n",
        "      print(f\"Detected language: {language}\")\n",
        "   except langdetect.lang_detect_exception.LangDetectException:\n",
        "      print(\"Could not detect language.\")\n",
        "\n",
        "   parser = PlaintextParser.from_string(content , Tokenizer(language))\n",
        "   summarizer = LsaSummarizer()\n",
        "   summary_sumy = summarizer(parser.document, 3)\n",
        "   sumerized_content = \" \".join(str(sentence) for sentence in summary_sumy)\n",
        "   print(\"\\nSumy Summary and remove the html content from this content :\\n\", sumerized_content)\n",
        "   return sumerized_content\n",
        "\n",
        "# Function to generate questions using the OpenAI API\n",
        "@retry_with_exponential_backoff_1\n",
        "def generate_question_gpt3(content,variable, question_template):\n",
        "    # Generate questions for each variable\n",
        "    #questions = {}\n",
        "    if True: #for variable, question_template in dvar_questions.items():\n",
        "        prompt = f\"Answer the this question :{question_template} For the {variable} with this content:\\n Content: {content}\"\n",
        "        response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=2048,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            temperature=0.3,\n",
        "            frequency_penalty=0\n",
        "        )\n",
        "\n",
        "        question = response.choices[0].text.strip()\n",
        "\n",
        "        #questions[variable] = question\n",
        "        print(f\"+++GPT3---Generated question for {variable}: \\n {question} ---GPT3---\\n\")\n",
        "\n",
        "    return question\n",
        "\n",
        "def generate_question_gpt2(content, variable, question_template, model, tokenizer, max_response_length=None):\n",
        "    # Generate a question for a specific variable\n",
        "    #prompt = f\"{question_template}\\nContent {content}\"\n",
        "    prompt = f\"{question_template}\\n {content}\"\n",
        "    print ( \"\\n -----The Prompt is :\" , prompt )\n",
        "    #content = content.replace(\"\\n\", '')\n",
        "\n",
        "    # Ensure attention_mask is set during tokenization\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Set max_length to the provided value or use None if not provided\n",
        "    if max_response_length is not None:\n",
        "        max_length = min(max_response_length, model.config.max_position_embeddings)\n",
        "    else:\n",
        "        max_length = None\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    print(f\"GPT2####Generated question for {variable}: {generated_text}####\")\n",
        "\n",
        "    #ChatGPT image prompt creating\n",
        "    prompt_Word_Topic_1 =[]\n",
        "    category= 'Creating Variables'; image_prompt ='';k=0\n",
        "    prompt_Word_Topic_1.append('MAKING VARIABELS')\n",
        "    make_photo=\"\"\n",
        "    perviuse_try_numner = 0\n",
        "    if True:\n",
        "            save_academic_paper_with_prompt(topic[:15]+\"_Pr\",prompt_Word_Topic_1[k],image_prompt,\"\",perviuse_try_numner,category,make_photo)\n",
        "            save_academic_paper_with_prompt_Tile(topic[:15]+\"_T\",prompt_Word_Topic_1[k],image_prompt,\"\",perviuse_try_numner,category,make_photo)\n",
        "\n",
        "    else :\n",
        "            save_academic_paper_with_prompt(topic[:15]+\"_Pr\",'\\n**<<< ChatGPT Prompt is:\\n'+''.join(prompt)+'\\n>>>**\\n',\"\",choice.text,perviuse_try_numner,category,make_photo)\n",
        "            save_academic_paper_with_prompt_Tile(topic[:15]+'_T',prompt_Word_Topic_1[k],image_prompt,choice.text,perviuse_try_numner,category,make_photo)\n",
        "    perviuse_try_numner =perviuse_try_numner +1\n",
        "\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "def generate_questions(content_data, content, dvar_questions, model, tokenizer, model_type, max_length=1096):\n",
        "    # Generate questions for each variable\n",
        "    questions = {}\n",
        "\n",
        "    for variable, question_template in dvar_questions.items():\n",
        "        print ('\\nFor the variable of :',variable ,' The question is :',question_template)\n",
        "        if model_type == \"gpt2\":\n",
        "           question = generate_question_gpt2(content, variable, question_template, model, tokenizer, max_response_length=max_length)\n",
        "           question = question.replace(f\"Topic: \",\"\").replace(f\"{content_data['topic']}\", \"\").replace(f\"Description: \",\"\").replace(f\"{content_data['description']}\", \"\")\n",
        "           #question = question.replace(topic, \"\").replace(description, \"\")\n",
        "\n",
        "\n",
        "        elif model_type == \"gpt3\":\n",
        "           question = generate_question_gpt3(content,variable, question_template)\n",
        "           print( f'\\n @@@@ question for var {variable} is :', question)\n",
        "           #question = question.replace(f\"{content_data['topic']}\", \"\").replace(f\"{content_data['description']}\", \"\")\n",
        "\n",
        "        #question = question.replace(f\"{dvar_questions[variable]}\",\"\") #[variable]}\",'')\n",
        "        #question = question.replace(f\"{question_template}\", \"\").replace(\"\\nContent\",'')\n",
        "\n",
        "        questions[variable] = question\n",
        "        print(f\"\\n xxxx Generated question for {variable}:is  {question}\\n\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "\n",
        "def extract_content_from_url(url):\n",
        "    # Retrieve content from the web using newspaper library\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "\n",
        "    course_data_1=[]\n",
        "    # Extract course title course_title = article.title if article.title else \"Title not found on the page\"\n",
        "    course_title = article.title if article.title else \"Title not found on the page\"\n",
        "    #course_data_1['course_title'] = course_title\n",
        "\n",
        "    # Extract course description\n",
        "    course_description = article.text if article.text else \"Description not found on the page\"\n",
        "    #course_data_1['course_description'] = course_description\n",
        "    course_data = []\n",
        "    course_data.append({'topic': course_title, 'description': course_description})\n",
        "\n",
        "    return course_data\n",
        "# Rest of your code...\n",
        "def fill_variables_from_docx(docx_path, dvar_questions):\n",
        "    # Read content from DOCX file\n",
        "    doc = Document(docx_path)\n",
        "    content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "    # Generate questions and fill variable values\n",
        "    variable_values = generate_questions(content, dvar_questions)\n",
        "\n",
        "    return variable_values\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set the values for the topic and description directly in the script\n",
        "    topic = \"Addressing Iran's Crises: Exploring Social Change Methodologies and Creating a Counter Profile\"\n",
        "    description = \"Iran's societal challenges are complex and require immediate attention. Engaging in research and working on solutions in this field can help address these issues. This course aims to equip students with the necessary tools and knowledge to navigate these challenges.\"\n",
        "\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model_name = \"distilgpt2\"  # You can replace this with other models available on Hugging Face\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Set pad_token to EOS token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Define your variable questions\n",
        "    dvar_questions_url = {\n",
        "        \"topic\": \"What is the main topic or focus of the content?\",\n",
        "         \"description\": \"Can you provide a summary of the content of that internet post?\",\n",
        "     # Add more variables with question templates as needed\n",
        "    }\n",
        "\n",
        "    # Combine topic and description into content\n",
        "    #content = f\"Topic: {topic}\\nDescription: {description}\"\n",
        "\n",
        "    # Generate questions and fill variable values\n",
        "    #filled_variables = generate_questions(content, dvar_questions, model, tokenizer)\n",
        "    #print(\"Filled Variables:\", filled_variables)\n",
        "\n",
        "    # Extract content from a URL\n",
        "    course_url = \"https://telegra.ph/OpenAIs-Democratic-Rules-and-Light-Triad-Personality-Traits-for-Iran-01-19\"\n",
        "    extracted_content = extract_content_from_url(course_url)\n",
        "    extracted_content = extracted_content[0]\n",
        "    content = extracted_content['description'] # f\"Topic: {extracted_content['topic']}\\nDescription: {extracted_content['description']}\"\n",
        "\n",
        "    summarize_post = sumerizing_post(content)\n",
        "    extracted_content['description'] = summarize_post # extracted_content[0]  # Accessing the first (and only) dictionary in the list\n",
        "\n",
        "    #content = f\"Topic: {extracted_content['topic']}\\nDescription: {extracted_content['description']}\"\n",
        "    content = summarize_post #f\"{extracted_content['topic']}\\n{extracted_content['description']}\"\n",
        "\n",
        "    print('\\n Content is :', content)\n",
        "    url_content = generate_questions(extracted_content,content, dvar_questions, model, tokenizer, 'gpt3')\n",
        "\n",
        "    content = f\"Topic: {url_content['topic']}\\nDescription: {url_content['description']}\"\n",
        "    filled_variables = generate_questions(url_content,content, dvar_questions, model, tokenizer, 'gpt3')\n",
        "\n",
        "    print(\"Extracted Content from URL:\", filled_variables)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "AetaJy59-BGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}