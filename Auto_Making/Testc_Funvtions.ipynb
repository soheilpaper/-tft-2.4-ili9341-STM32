{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/Auto_Making/Testc_Funvtions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f8xPv8AfezZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang):\n",
        "  model_name = \"Helsinki-NLP/opus-mt-{}\".format(src_lang+\"-\"+trg_lang)\n",
        "\n",
        "  # Load the tokenizer\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  # Load the model\n",
        "  model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "  # Encode the text\n",
        "  inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "  # Generate the translation\n",
        "  outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "\n",
        "  # Decode the translation\n",
        "  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return translation\n",
        "\n",
        "txt = \"Hello, world!\"\n",
        "translated_text = translate_text(txt, \"en\", \"fr\")\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "lfILV9Gxezo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang, token):\n",
        " model_name = f\"facebook/hf-seamless-m4t-{src_lang}_to_{trg_lang}\"\n",
        "\n",
        "# Load the tokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Load the model\n",
        " model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Encode the text\n",
        " inputs = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "# Generate the translation\n",
        " outputs = model.generate(inputs)\n",
        "\n",
        "# Decode the translation\n",
        " translation = tokenizer.decode(outputs[0])\n",
        "\n",
        "return translation\n",
        "\n",
        "Huggingface_Token = \"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\"\n",
        "txt = \"Hello, world!\"\n",
        "#translated_text = translate_text(txt, \"en\", \"fr\",Huggingface_Token)\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "Ih0Yyj91e0r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")"
      ],
      "metadata": {
        "id": "zPccZlMpe6Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read an audio file and resample to 16kHz:\n",
        "#audio, orig_freq =  torchaudio.load(\"https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav\")\n",
        "#audio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\n",
        "#audio_inputs = processor(audios=audio, return_tensors=\"pt\")\n",
        "# Process some input text as well:\n",
        "#text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "QaRbuDWae96N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mycroft-mimic3-tts[all]"
      ],
      "metadata": {
        "id": "EcGknPPwfBJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "ebihB-aPfHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/spaces/saillab/persian-tts-playground/blob/main/app.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
        "# Define constants\n",
        "MODEL_INFO = [\n",
        "    [\"vits-espeak-57000\", \"checkpoint_57000.pth\", \"config.json\", \"mhrahmani/persian-tts-vits-0\"],\n",
        "]\n",
        "\n",
        "# # Extract model names from MODEL_INFO\n",
        "# MODEL_NAMES = [info[0] for info in MODEL_INFO]\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"vits checkpoint 57000\",\n",
        "    # Add other model names similarly...\n",
        "]\n",
        "\n",
        "MAX_TXT_LEN = 400\n",
        "TOKEN = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
        "\n",
        "# # Download models\n",
        "# for model_name, model_file, config_file, repo_name in MODEL_INFO:\n",
        "#     os.makedirs(model_name, exist_ok=True)\n",
        "#     print(f\"|> Downloading: {model_name}\")\n",
        "\n",
        "#     # Use hf_hub_download to download models from private Hugging Face repositories\n",
        "#     hf_hub_download(repo_id=repo_name, filename=model_file, use_auth_token=TOKEN)\n",
        "#     hf_hub_download(repo_id=repo_name, filename=config_file, use_auth_token=TOKEN)\n",
        "\n",
        "repo_name = \"mhrahmani/persian-tts-vits-0\"\n",
        "filename = \"checkpoint_57000.pth\"\n",
        "\n",
        "model_file = hf_hub_download(repo_name, filename, use_auth_token=TOKEN)\n",
        "config_file = hf_hub_download(repo_name, \"config.json\", use_auth_token=TOKEN)\n",
        "\n",
        "\n",
        "def synthesize(text: str, model_name: str) -> str:\n",
        "    \"\"\"Synthesize speech using the selected model.\"\"\"\n",
        "    if len(text) > MAX_TXT_LEN:\n",
        "        text = text[:MAX_TXT_LEN]\n",
        "        print(f\"Input text was cut off as it exceeded the {MAX_TXT_LEN} character limit.\")\n",
        "\n",
        "    synthesizer = Synthesizer(model_file, config_file)\n",
        "    if synthesizer is None:\n",
        "        raise NameError(\"Model not found\")\n",
        "\n",
        "    wavs = synthesizer.tts(text)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n",
        "        synthesizer.save_wav(wavs, fp)\n",
        "        return fp.name\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=synthesize,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Text to Synthesize:\", value=\"زین همرهان سست عناصر، دلم گرفت.\"),\n",
        "        gr.Radio(label=\"Pick a Model\", choices=MODEL_NAMES, value=MODEL_NAMES[0]),\n",
        "    ],\n",
        "    outputs=gr.Audio(label=\"Output\", type='filepath'),\n",
        "    examples=[[\"زین همرهان سست عناصر، دلم گرفت.\", MODEL_NAMES[0]]],\n",
        "    title='Persian TTS Playground',\n",
        "    description=\"Persian text to speech model demo\",\n",
        "    article=\"\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "#iface.launch()"
      ],
      "metadata": {
        "id": "9dujQYeAfOtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}