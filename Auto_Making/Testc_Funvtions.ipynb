{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/Auto_Making/Testc_Funvtions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f8xPv8AfezZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang):\n",
        "  model_name = \"Helsinki-NLP/opus-mt-{}\".format(src_lang+\"-\"+trg_lang)\n",
        "\n",
        "  # Load the tokenizer\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  # Load the model\n",
        "  model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "  # Encode the text\n",
        "  inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "  # Generate the translation\n",
        "  outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "\n",
        "  # Decode the translation\n",
        "  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return translation\n",
        "\n",
        "txt = \"Hello, world!\"\n",
        "translated_text = translate_text(txt, \"en\", \"fr\")\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "lfILV9Gxezo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang, token):\n",
        " model_name = f\"facebook/hf-seamless-m4t-{src_lang}_to_{trg_lang}\"\n",
        "\n",
        "# Load the tokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Load the model\n",
        " model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Encode the text\n",
        " inputs = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "# Generate the translation\n",
        " outputs = model.generate(inputs)\n",
        "\n",
        "# Decode the translation\n",
        " translation = tokenizer.decode(outputs[0])\n",
        "\n",
        "return translation\n",
        "\n",
        "Huggingface_Token = \"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\"\n",
        "txt = \"Hello, world!\"\n",
        "#translated_text = translate_text(txt, \"en\", \"fr\",Huggingface_Token)\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "Ih0Yyj91e0r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")"
      ],
      "metadata": {
        "id": "zPccZlMpe6Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read an audio file and resample to 16kHz:\n",
        "#audio, orig_freq =  torchaudio.load(\"https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav\")\n",
        "#audio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\n",
        "#audio_inputs = processor(audios=audio, return_tensors=\"pt\")\n",
        "# Process some input text as well:\n",
        "#text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "QaRbuDWae96N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mycroft-mimic3-tts[all]"
      ],
      "metadata": {
        "id": "EcGknPPwfBJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "ebihB-aPfHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/spaces/saillab/persian-tts-playground/blob/main/app.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
        "# Define constants\n",
        "MODEL_INFO = [\n",
        "    [\"vits-espeak-57000\", \"checkpoint_57000.pth\", \"config.json\", \"mhrahmani/persian-tts-vits-0\"],\n",
        "]\n",
        "\n",
        "# # Extract model names from MODEL_INFO\n",
        "# MODEL_NAMES = [info[0] for info in MODEL_INFO]\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"vits checkpoint 57000\",\n",
        "    # Add other model names similarly...\n",
        "]\n",
        "\n",
        "MAX_TXT_LEN = 400\n",
        "TOKEN = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
        "\n",
        "# # Download models\n",
        "# for model_name, model_file, config_file, repo_name in MODEL_INFO:\n",
        "#     os.makedirs(model_name, exist_ok=True)\n",
        "#     print(f\"|> Downloading: {model_name}\")\n",
        "\n",
        "#     # Use hf_hub_download to download models from private Hugging Face repositories\n",
        "#     hf_hub_download(repo_id=repo_name, filename=model_file, use_auth_token=TOKEN)\n",
        "#     hf_hub_download(repo_id=repo_name, filename=config_file, use_auth_token=TOKEN)\n",
        "\n",
        "repo_name = \"mhrahmani/persian-tts-vits-0\"\n",
        "filename = \"checkpoint_57000.pth\"\n",
        "\n",
        "model_file = hf_hub_download(repo_name, filename, use_auth_token=TOKEN)\n",
        "config_file = hf_hub_download(repo_name, \"config.json\", use_auth_token=TOKEN)\n",
        "\n",
        "\n",
        "def synthesize(text: str, model_name: str) -> str:\n",
        "    \"\"\"Synthesize speech using the selected model.\"\"\"\n",
        "    if len(text) > MAX_TXT_LEN:\n",
        "        text = text[:MAX_TXT_LEN]\n",
        "        print(f\"Input text was cut off as it exceeded the {MAX_TXT_LEN} character limit.\")\n",
        "\n",
        "    synthesizer = Synthesizer(model_file, config_file)\n",
        "    if synthesizer is None:\n",
        "        raise NameError(\"Model not found\")\n",
        "\n",
        "    wavs = synthesizer.tts(text)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n",
        "        synthesizer.save_wav(wavs, fp)\n",
        "        return fp.name\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=synthesize,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Text to Synthesize:\", value=\"زین همرهان سست عناصر، دلم گرفت.\"),\n",
        "        gr.Radio(label=\"Pick a Model\", choices=MODEL_NAMES, value=MODEL_NAMES[0]),\n",
        "    ],\n",
        "    outputs=gr.Audio(label=\"Output\", type='filepath'),\n",
        "    examples=[[\"زین همرهان سست عناصر، دلم گرفت.\", MODEL_NAMES[0]]],\n",
        "    title='Persian TTS Playground',\n",
        "    description=\"Persian text to speech model demo\",\n",
        "    article=\"\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "#iface.launch()"
      ],
      "metadata": {
        "id": "9dujQYeAfOtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://www.phind.com/search?cache=c0v40teuf4g1lng6rq0n2zeh\n",
        "\n",
        "Sure, here's a README file written in markdown for the Python script. It explains what the script does, how to run it, and some potential improvements.\n",
        "\n",
        "```markdown\n",
        "# Text Classification Model\n",
        "\n",
        "This repository contains a Python script that trains a text classification model using Keras. The model is trained on the IMDB movie review sentiment classification dataset, which is readily available in Keras and does not require downloading any external files.\n",
        "\n",
        "## How to Run\n",
        "\n",
        "To run the script, clone the repository and navigate to the directory containing the script. Then, run the following command in your terminal:\n",
        "\n",
        "```bash\n",
        "python script_name.py\n",
        "```\n",
        "\n",
        "Replace `script_name.py` with the name of the Python script.\n",
        "\n",
        "## What the Script Does\n",
        "\n",
        "The script loads the IMDB movie review sentiment classification dataset, preprocesses the data, builds a model, trains the model, and evaluates its performance on the test set.\n",
        "\n",
        "## Potential Improvements\n",
        "\n",
        "1. **Custom Dataset**: Currently, the script uses the IMDB movie review sentiment classification dataset. You could replace this with a custom dataset of your choice. Make sure to preprocess the data appropriately and adjust the model architecture as needed.\n",
        "\n",
        "2. **More Complex Model Architecture**: The current model architecture might not be optimal for your specific task. You could experiment with adding more layers to the model or changing the type of layers used.\n",
        "\n",
        "3. **Hyperparameter Tuning**: The current script uses default hyperparameters for the model and training process. You could perform hyperparameter tuning to potentially improve the model's performance.\n",
        "\n",
        "Remember, building a model to detect psychological disorders from text requires a large labeled dataset. You might need to manually label your data or find a suitable pre-labeled dataset. Also, keep in mind that this is a complex task and the performance of the model depends heavily on the quality and quantity of the training data.\n",
        "\n",
        "## References\n",
        "\n",
        "- [Keras Text Classification From Scratch](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
        "- [StackAbuse Python for NLP Multi Label Text Classification with Keras](https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/)\n",
        "```\n",
        "\n",
        "Please replace `\"script_name.py\"` with the name of your Python script. Also, make sure to update the references with the correct URLs for the sources you used [Source 0](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks), [Source 1](https://markdown.land/markdown-code-block), [Source 3](https://readme.so/), [Source 4](https://code.visualstudio.com/Docs/languages/markdown)."
      ],
      "metadata": {
        "id": "avN8jUFz56Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the data\n",
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "   \"aclImdb/train\", batch_size=batch_size, validation_split=0.2, subset=\"training\", seed=1337\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "   \"aclImdb/train\", batch_size=batch_size, validation_split=0.2, subset=\"validation\", seed=1337\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "# Print number of batches in each dataset\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n",
        "\n",
        "# Print some examples from the training set\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "   for i in range(5):\n",
        "       print(text_batch.numpy()[i])\n",
        "       print(label_batch.numpy()[i])\n",
        "\n",
        "# Vectorize the data\n",
        "def vectorize_text(text, label):\n",
        "   text = tf.expand_dims(text, -1)\n",
        "   return vectorize_layer(text), label\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Cache and prefetch the data for best performance on GPU\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
        "\n",
        "# Build the model\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "indices = vectorize_layer(inputs)\n",
        "outputs = model(indices)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = end_to_end_model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = end_to_end_model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_accz}\")"
      ],
      "metadata": {
        "id": "HNGi3bAf6BT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}