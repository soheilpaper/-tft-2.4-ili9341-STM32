{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheilpaper/-tft-2.4-ili9341-STM32/blob/master/Auto_Making/Testc_Funvtions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f8xPv8AfezZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang):\n",
        "  model_name = \"Helsinki-NLP/opus-mt-{}\".format(src_lang+\"-\"+trg_lang)\n",
        "\n",
        "  # Load the tokenizer\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  # Load the model\n",
        "  model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "  # Encode the text\n",
        "  inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "  # Generate the translation\n",
        "  outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
        "\n",
        "  # Decode the translation\n",
        "  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return translation\n",
        "\n",
        "txt = \"Hello, world!\"\n",
        "translated_text = translate_text(txt, \"en\", \"fr\")\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "lfILV9Gxezo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def translate_text(text, src_lang, trg_lang, token):\n",
        " model_name = f\"facebook/hf-seamless-m4t-{src_lang}_to_{trg_lang}\"\n",
        "\n",
        "# Load the tokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Load the model\n",
        " model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=token)\n",
        "\n",
        "# Encode the text\n",
        " inputs = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "# Generate the translation\n",
        " outputs = model.generate(inputs)\n",
        "\n",
        "# Decode the translation\n",
        " translation = tokenizer.decode(outputs[0])\n",
        "\n",
        "return translation\n",
        "\n",
        "Huggingface_Token = \"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\"\n",
        "txt = \"Hello, world!\"\n",
        "#translated_text = translate_text(txt, \"en\", \"fr\",Huggingface_Token)\n",
        "print(f'{txt} translated is is:',f'Translated is :{translated_text} ')"
      ],
      "metadata": {
        "id": "Ih0Yyj91e0r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")"
      ],
      "metadata": {
        "id": "zPccZlMpe6Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read an audio file and resample to 16kHz:\n",
        "#audio, orig_freq =  torchaudio.load(\"https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav\")\n",
        "#audio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\n",
        "#audio_inputs = processor(audios=audio, return_tensors=\"pt\")\n",
        "# Process some input text as well:\n",
        "#text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "QaRbuDWae96N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mycroft-mimic3-tts[all]"
      ],
      "metadata": {
        "id": "EcGknPPwfBJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "ebihB-aPfHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/spaces/saillab/persian-tts-playground/blob/main/app.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
        "# Define constants\n",
        "MODEL_INFO = [\n",
        "    [\"vits-espeak-57000\", \"checkpoint_57000.pth\", \"config.json\", \"mhrahmani/persian-tts-vits-0\"],\n",
        "]\n",
        "\n",
        "# # Extract model names from MODEL_INFO\n",
        "# MODEL_NAMES = [info[0] for info in MODEL_INFO]\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"vits checkpoint 57000\",\n",
        "    # Add other model names similarly...\n",
        "]\n",
        "\n",
        "MAX_TXT_LEN = 400\n",
        "TOKEN = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
        "\n",
        "# # Download models\n",
        "# for model_name, model_file, config_file, repo_name in MODEL_INFO:\n",
        "#     os.makedirs(model_name, exist_ok=True)\n",
        "#     print(f\"|> Downloading: {model_name}\")\n",
        "\n",
        "#     # Use hf_hub_download to download models from private Hugging Face repositories\n",
        "#     hf_hub_download(repo_id=repo_name, filename=model_file, use_auth_token=TOKEN)\n",
        "#     hf_hub_download(repo_id=repo_name, filename=config_file, use_auth_token=TOKEN)\n",
        "\n",
        "repo_name = \"mhrahmani/persian-tts-vits-0\"\n",
        "filename = \"checkpoint_57000.pth\"\n",
        "\n",
        "model_file = hf_hub_download(repo_name, filename, use_auth_token=TOKEN)\n",
        "config_file = hf_hub_download(repo_name, \"config.json\", use_auth_token=TOKEN)\n",
        "\n",
        "\n",
        "def synthesize(text: str, model_name: str) -> str:\n",
        "    \"\"\"Synthesize speech using the selected model.\"\"\"\n",
        "    if len(text) > MAX_TXT_LEN:\n",
        "        text = text[:MAX_TXT_LEN]\n",
        "        print(f\"Input text was cut off as it exceeded the {MAX_TXT_LEN} character limit.\")\n",
        "\n",
        "    synthesizer = Synthesizer(model_file, config_file)\n",
        "    if synthesizer is None:\n",
        "        raise NameError(\"Model not found\")\n",
        "\n",
        "    wavs = synthesizer.tts(text)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n",
        "        synthesizer.save_wav(wavs, fp)\n",
        "        return fp.name\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=synthesize,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Text to Synthesize:\", value=\"زین همرهان سست عناصر، دلم گرفت.\"),\n",
        "        gr.Radio(label=\"Pick a Model\", choices=MODEL_NAMES, value=MODEL_NAMES[0]),\n",
        "    ],\n",
        "    outputs=gr.Audio(label=\"Output\", type='filepath'),\n",
        "    examples=[[\"زین همرهان سست عناصر، دلم گرفت.\", MODEL_NAMES[0]]],\n",
        "    title='Persian TTS Playground',\n",
        "    description=\"Persian text to speech model demo\",\n",
        "    article=\"\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "#iface.launch()"
      ],
      "metadata": {
        "id": "9dujQYeAfOtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://www.phind.com/search?cache=c0v40teuf4g1lng6rq0n2zeh\n",
        "\n",
        "Sure, here's a README file written in markdown for the Python script. It explains what the script does, how to run it, and some potential improvements.\n",
        "\n",
        "```markdown\n",
        "# Text Classification Model\n",
        "\n",
        "This repository contains a Python script that trains a text classification model using Keras. The model is trained on the IMDB movie review sentiment classification dataset, which is readily available in Keras and does not require downloading any external files.\n",
        "\n",
        "## How to Run\n",
        "\n",
        "To run the script, clone the repository and navigate to the directory containing the script. Then, run the following command in your terminal:\n",
        "\n",
        "```bash\n",
        "python script_name.py\n",
        "```\n",
        "\n",
        "Replace `script_name.py` with the name of the Python script.\n",
        "\n",
        "## What the Script Does\n",
        "\n",
        "The script loads the IMDB movie review sentiment classification dataset, preprocesses the data, builds a model, trains the model, and evaluates its performance on the test set.\n",
        "\n",
        "## Potential Improvements\n",
        "\n",
        "1. **Custom Dataset**: Currently, the script uses the IMDB movie review sentiment classification dataset. You could replace this with a custom dataset of your choice. Make sure to preprocess the data appropriately and adjust the model architecture as needed.\n",
        "\n",
        "2. **More Complex Model Architecture**: The current model architecture might not be optimal for your specific task. You could experiment with adding more layers to the model or changing the type of layers used.\n",
        "\n",
        "3. **Hyperparameter Tuning**: The current script uses default hyperparameters for the model and training process. You could perform hyperparameter tuning to potentially improve the model's performance.\n",
        "\n",
        "Remember, building a model to detect psychological disorders from text requires a large labeled dataset. You might need to manually label your data or find a suitable pre-labeled dataset. Also, keep in mind that this is a complex task and the performance of the model depends heavily on the quality and quantity of the training data.\n",
        "\n",
        "## References\n",
        "\n",
        "- [Keras Text Classification From Scratch](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
        "- [StackAbuse Python for NLP Multi Label Text Classification with Keras](https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/)\n",
        "```\n",
        "\n",
        "Please replace `\"script_name.py\"` with the name of your Python script. Also, make sure to update the references with the correct URLs for the sources you used [Source 0](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks), [Source 1](https://markdown.land/markdown-code-block), [Source 3](https://readme.so/), [Source 4](https://code.visualstudio.com/Docs/languages/markdown)."
      ],
      "metadata": {
        "id": "avN8jUFz56Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the data\n",
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "   \"aclImdb/train\", batch_size=batch_size, validation_split=0.2, subset=\"training\", seed=1337\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "   \"aclImdb/train\", batch_size=batch_size, validation_split=0.2, subset=\"validation\", seed=1337\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "# Print number of batches in each dataset\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n",
        "\n",
        "# Print some examples from the training set\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "   for i in range(5):\n",
        "       print(text_batch.numpy()[i])\n",
        "       print(label_batch.numpy()[i])\n",
        "\n",
        "# Vectorize the data\n",
        "def vectorize_text(text, label):\n",
        "   text = tf.expand_dims(text, -1)\n",
        "   return vectorize_layer(text), label\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Cache and prefetch the data for best performance on GPU\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
        "\n",
        "# Build the model\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "indices = vectorize_layer(inputs)\n",
        "outputs = model(indices)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = end_to_end_model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = end_to_end_model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_accz}\")"
      ],
      "metadata": {
        "id": "HNGi3bAf6BT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.phind.com/search?cache=r5s7z4lpbntujj53iukrxdmm\n",
        "Based on the information from the [repository](https://github.com/kharrigian/mental-health-datasets), here's a README.md file that describes the project:\n",
        "\n",
        "```markdown\n",
        "# Mental Health Datasets\n",
        "\n",
        "This repository contains an evolving list of data sets primarily from electronic/social media that have been used to model mental-health phenomena. The raw data (with additional columns) can be found in `data_sources.xlsx`.\n",
        "\n",
        "For an overview of existing datasets, please consider reading our paper [On the State of Social Media Data for Mental Health Research](https://www.keithharrigian.com/pubs/harrigian2020state.pdf).\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{harrigian2020state,\n",
        " title={On the State of Social Media Data for Mental Health Research},\n",
        " author={Harrigian, Keith and Aguirre, Carlos and Dredze, Mark},\n",
        " booktitle={Proceedings of the 7th Workshop on Computational Linguistics and Clinical Psychology: Improving Access},\n",
        " year={2021}\n",
        "}\n",
        "```\n",
        "\n",
        "## Contributing\n",
        "\n",
        "We hope this repository becomes the central knowledge base for researchers working at the intersection of NLP and mental health. However, we cannot achieve this goal without the support of the community.\n",
        "\n",
        "You can view our backlog of literature that needs annotation [here](https://github.com/kharrigian/mental-health-datasets/issues). To annotate one of these papers, or to annotate a paper we haven't yet identified, please begin by updating the backlog to note that you are taking responsibility for a paper's annotation. After, you can use our [standardized annotation form](https://github.com/kharrigian/mental-health-datasets/blob/master/annotation_form.md) to make a submission that will be reviewed and published within the main directory.\n",
        "\n",
        "## About\n",
        "\n",
        "An evolving list of electronic media data sets used to model mental-health status.\n",
        "\n",
        "## Resources\n",
        "\n",
        "* [Readme](https://github.com/kharrigian/mental-health-datasets)\n",
        "* [Activity](https://github.com/kharrigian/mental-health-datasets/graphs)\n",
        "* [Stars](https://github.com/kharrigian/mental-health-datasets/stargazers)\n",
        "* [Watchers](https://github.com/kharrigian/mental-health-datasets/watchers)\n",
        "* [Forks](https://github.com/kharrigian/mental-health-datasets/network/members)\n",
        "```\n",
        "\n",
        "Please replace the URLs with the actual URLs if they are different."
      ],
      "metadata": {
        "id": "p9GMo3yT7o5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kharrigian/mental-health-datasets\n",
        "\n",
        "%cd mental*"
      ],
      "metadata": {
        "id": "0UvWhzxy7ywk",
        "outputId": "3235b180-ef0f-477e-890a-79f8b034b6fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mental-health-datasets'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 231 (delta 0), reused 0 (delta 0), pack-reused 218\u001b[K\n",
            "Receiving objects: 100% (231/231), 5.20 MiB | 21.30 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "[Errno 2] No such file or directory: 'mental*'\n",
            "/content\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}